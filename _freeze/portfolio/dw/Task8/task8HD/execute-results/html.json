{
  "hash": "c64ac166aa577e918b9e49dd33eb1c70",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Data Cleaning and Text Analysis\"\nformat:\n  html:\n    code-fold: true\n  pdf:\n    papersize: a4\n    geometry:\n      - top=20mm\n      - left=25mm\n      - right=25mm\n      - bottom=20mm\n    \njupyter: python3\n---\n\n::: {#278672a5 .cell execution_count=1}\n``` {.python .cell-code}\n# Versions of the libraries used are noted beside the import lines\n# Python version: 3.11.7\nimport os\nimport time\nimport re\nimport pandas as pd                     # 2.1.4\nimport numpy as np                      # 1.26.4\nimport seaborn as sns                   # 0.12.2\n\nimport geopandas                        # 1.01\nfrom geodatasets import get_path        # 2024.8.0\nfrom geopy.geocoders import Nominatim   # 2.4.1\nimport matplotlib.pyplot as plt         # 3.8.0\n\nfrom stop_words import get_stop_words   # 2018.7.23\nimport inflect                          # 7.5.0\nfrom wordcloud import WordCloud         # 1.9.4\nfrom IPython.display import Markdown    # 8.20.0\nfrom collections import Counter \n\nimport plotly.express as px             # 5.9.0\nimport textstat                         # 0.7.7\n\n```\n:::\n\n\n# Introduction\nThe Stack Exchange site chosen for this analysis is the Quantitative (Quant) Finance (<https://quant.stackexchange.com/>) site. The Quant site is approximately 14.5 years old and has 62,000 users (May 2025). 23,000 questions have been asked, and 27,000 answers have been provided, with 74\\% of the questions being answered. On average, the is site receives 2.5 questions per day.\n\nThis document analyses the geographic distribution of the site's users, the popular words used in posts, the popularity of tags used in posts, the type of programming language discussed and how this has change over the site's life and the response time to answering questions, and some of the factors that may contribute to longer response times.\n\n# Privacy and Ethical Considerations\nWhen conducting this analysis, several privacy and ethical concerns were given consideration. These were:\n\n* Privacy concerns:\n    - Unvalidated voluntary data: Users provide location information without verification, creating the potential for false data or privacy risks,\n    - Geographic mapping: Converting user locations to precise latitude/longitude coordinates could enable identification of individuals, especially in smaller communities,\n    - Data persistence: Location data from profiles may remain accessible long after users intend to share it.\n\n* Ethical concerns:\n    - Secondary use: Data was collected for Q&A purposes but is being used for research analysis,\n    - Lack of explicit consent: Did users consent to geographic distribution analysis or behavioural profiling when creating accounts?\n    - Data retention: Using archived data dumps raises questions about whether users can withdraw consent.\n\nWhen mapping users' locations, no user information (such as username, name, etc.) was stored with the calculated latitude and longitude coordinates, and the mapping was performed only to city, state, and country precision. Invalid locations were removed from the data. However, this doesn't guarantee the location data is accurate.\n\n# Data\nThe data used in the analysis was downloaded from the siteâ€™s most recent data dump (2 May 2024) from <https://archive.org/details/stackexchange/>. The site data is contained in eight XML files containing information on badges, comments, post history, post links, posts, tags, users, and votes.\n\n## Convert data files from XML to CSV\nThe site's data files are provided in XML format. The first step is to load each file separately and convert them to CSV. This is performed with the xml_to_csv function shown below. This step is only performed if the CSV file doesn't exists to prevent performing the conversion each time the code is executed.\n\n::: {#39a455c4 .cell execution_count=2}\n``` {.python .cell-code}\n# Function to convert XML files to CSV\ndef xml_to_csv(file_name, csv_name=None, drop_cols=None):\n    \"\"\"\n    Converts an XML file to CSV using Pandas.\n\n    :param file_name: Name of the XML file to convert\n    :param csv_name: Optional name of exported CSV file. If not provided, \n        the CSV file name will be the same as the XML file name.\n    :param drop_cols: Optional list of columns to drop from dataframe\n    \"\"\"\n    # Read XML file in dataframe\n    df = pd.read_xml(file_name)\n\n    # Check if the user wants to leave any columns out of the conversion\n    if drop_cols is not None:\n        for col in drop_cols:\n            del df[col]\n    # Set CSV name if not provided\n    if csv_name is None:\n        csv_name = file_name.split(\".\")[0]+ \".csv\"\n    # Write CSV file\n    df.to_csv(csv_name, index=False)\n\n    print(f\"Converted {file_name} to {csv_name}\")\n\n# Convert the files if required\nfiles = [\"Badges\", \"Comments\", \"PostHistory\", \"PostLinks\", \"Posts\", \"Tags\",\n        \"Users\", \"Votes\"]\n\ncsv_files = []\nfor file in files:\n    csv_file = os.path.join(\"data\", file+\".csv\")\n    xml_file = os.path.join(\"data\", file+\".xml\")\n    csv_files.append(csv_file)\n    \n    if os.path.exists(csv_file):\n        print(f\"File '{csv_file}' exists.\")\n    else:\n        print(f\"File '{csv_file}' does not exist.\")\n        xml_to_csv(xml_file,drop_cols=drops[file])\n```\n:::\n\n\n## Load CSV data files into pandas data frames\nEach CSV file is loaded into pandas data frames. For each data frame, the datatype (dtype) was checked (not shown in the code), and corrections were made if pandas incorrectly assigned the data type. Corrections were often needed for DateTime fields and some string fields. \n\n::: {#8a8b15e1 .cell execution_count=3}\n``` {.python .cell-code}\n# Load the data from CSV file into pandas data frames\nbadges = pd.read_csv(csv_files[0], comment=\"#\")\n\n# Correct data types in data frame\nbadges[\"Date\"] = pd.to_datetime(badges[\"Date\"])\nbadges = badges.astype({\"Name\": \"string\"})\n```\n:::\n\n\n::: {#a59537d1 .cell execution_count=4}\n``` {.python .cell-code}\n# Load the data from CSV file into pandas data frames\ncomments = pd.read_csv(csv_files[1], comment=\"#\")\n\n# Correct data types in data frame\ncomments[\"CreationDate\"] = pd.to_datetime(comments[\"CreationDate\"])\ncomments = comments.astype({\"Text\": \"string\", \"UserDisplayName\":\"string\"})\n```\n:::\n\n\n::: {#80f980d8 .cell execution_count=5}\n``` {.python .cell-code}\n# Load the data from CSV file into pandas data frames\npost_history = pd.read_csv(csv_files[2], comment=\"#\")\n\n# Correct data types in data frame\npost_history = post_history.convert_dtypes()\npost_history[\"CreationDate\"] = pd.to_datetime(post_history[\"CreationDate\"])\n```\n:::\n\n\n::: {#754a017b .cell execution_count=6}\n``` {.python .cell-code}\n# Load the data from CSV file into pandas data frames\npost_links = pd.read_csv(csv_files[3], comment=\"#\")\n\n# Correct data types in data frame\npost_links[\"CreationDate\"] = pd.to_datetime(post_links[\"CreationDate\"])\n```\n:::\n\n\n::: {#2405bbe3 .cell execution_count=7}\n``` {.python .cell-code}\n# Load the data from CSV file into pandas data frames\nposts = pd.read_csv(csv_files[4], comment=\"#\")\n\n# Correct data types in data frame\nposts = posts.convert_dtypes()\nposts[\"CreationDate\"] = pd.to_datetime(posts[\"CreationDate\"])\nposts[\"LastEditDate\"] = pd.to_datetime(posts[\"LastEditDate\"])\nposts[\"LastActivityDate\"] = pd.to_datetime(posts[\"LastActivityDate\"])\nposts[\"CommunityOwnedDate\"] = pd.to_datetime(posts[\"CommunityOwnedDate\"])\nposts[\"ClosedDate\"] = pd.to_datetime(posts[\"ClosedDate\"])\n```\n:::\n\n\n::: {#f8768df4 .cell execution_count=8}\n``` {.python .cell-code}\n# Load the data from CSV file into pandas data frames\ntags = pd.read_csv(csv_files[5], comment=\"#\")\n\n# Correct data types in dataframe\ntags = tags.convert_dtypes()\n```\n:::\n\n\n::: {#97f73843 .cell execution_count=9}\n``` {.python .cell-code}\n# Load the data from CSV file into pandas data frames\nusers = pd.read_csv(csv_files[6], comment=\"#\")\nusers.head()\n\n# Correct data types in dataframe\nusers = users.convert_dtypes()\nusers[\"CreationDate\"] = pd.to_datetime(users[\"CreationDate\"])\n```\n:::\n\n\n::: {#711255b5 .cell execution_count=10}\n``` {.python .cell-code}\n# Load the data from CSV file into pandas data frames\nvotes = pd.read_csv(csv_files[7], comment=\"#\")\nvotes.head()\n\n# Correct data types in dataframe\nvotes[\"CreationDate\"] = pd.to_datetime(votes[\"CreationDate\"])\n```\n:::\n\n\n# Analysis\n\n## Geographic distribution of forum users\nThis section investigates the distribution of forum users worldwide by using the location information provided by users on their account profiles. The location information users enter is optional, and no form of validation is performed. This results in many empty entries and free-form input of the data. The free-form input presents challenges when working with the data. These challenges are identified and addressed in this section, while others remain for future improvements.  \n\n### Data clean-up\nRemove any rows in the users data frame that do not contain location information.\n\n::: {#37066b38 .cell execution_count=11}\n``` {.python .cell-code}\nnum_users = users.shape[0]\n# Drop rows that have no location\nusers.dropna(subset=[\"Location\"], inplace=True)\nnum_users_loc = users.shape[0]\n```\n:::\n\n\nIn May 2024, 50776 users of the Quant site were registered, and 14636 of these had provided an entry in the location field of their profile.\n\nThe next step is to determine how many of the provided locations contain valid information that can be used to locate the user on a map. This is an issue of privacy, so Stack Exchange does not require users to provide a location. Privacy could also be the reason the location provided is not validated.\n\nA function was created to validate the location. To be a valid location, the string must contain either of the following patterns: \"city, state, country\" or \"city, country\". The functions start by checking if the string only contains numbers, URLs or non-ASCII characters. Then, it checks if any invalid character sequences have been found, such as double hyphens, double apostrophes, etc. A name pattern is used for each valid name component and accepts letters, spaces, hyphens, apostrophes and periods. This pattern is compiled into a full pattern that adds commas and spaces between each name.\n\nThe captured groups from the regex match are then checked to ensure that the city isn't empty and the state isn't empty if a country is provided. If only the city were state are provided, the state could also represent the country, e.g., Paris, France. If only a city and state are provided, this could also be for an American user as they often don't enter their country. It has been observed that some invalid locations are still allowed by the function, and therefore, the function could be improved.\n\n::: {#dc18d187 .cell execution_count=12}\n``` {.python .cell-code}\ndef is_valid_location(location_string):\n    \"\"\"\n    Uses regex to validate if a location string follows the pattern of:\n    - 'city, state, country' OR \n    - 'city, country' (valid for no USA countries or USA locations)\n    \n    Rejects strings with only numbers, URLs, non-ASCII characters.\n    Rejects invalid character sequences:\n        - Double hyphens\n        - Double apostrophes\n        - Hyphen followed by apostrophe\n        - Apostrophe followed by hyphen\n        - Double periods\n\n    A name pattern is used to identify valid name components \n    (city, state, country). This pattern now accepts letters, spaces,\n     hyphens, apostrophes, and periods.\n\n    The function requires at least two matches of the name pattern and marks\n    any entries were the state and 'usa'/'united states' are not separated\n     by a comma as invalid\n\n    This function allows some invalid locations, therefore could be improved.\n    \n    :param location_string: String containing the location information to check\n    :return: True or False depending on if the location_string is valid\n    \"\"\"\n    # Clean up input string\n    location_string = location_string.strip()\n    \n    # Reject if contains URLs, or non-ASCII characters\n    if re.search(r\"^[\\d\\s]+$|http|www|[^\\x00-\\x7F]\", location_string):\n        return False\n\n    # Check for invalid character patterns\n    invalid_patterns = {\n    \"double_hyphen\": re.compile(r\"--\"),      # Double hyphens\n    \"double_apostrophe\": re.compile(r\"''\"),  # Double apostrophes\n    \"hyphen_apostrophe\": re.compile(r\"-'\"),  # Hyphen followed by apostrophe\n    \"apostrophe_hyphen\": re.compile(r\"'-\"),  # Apostrophe followed by hyphen\n    \"double_period\": re.compile(r\"\\.\\.\")     # Double periods\n    }\n\n    # Check for invalid character sequences\n    for pattern_name, pattern in invalid_patterns.items():\n        if pattern.search(location_string):\n            return False\n\n   \n    # Define pattern for valid name components (city, state, country)\n    # This pattern now accepts letters, spaces, hyphens, apostrophes, and\n    #  periods.\n    name_pattern = r\"[A-Za-z\\s\\-'.]+\"\n\n    # Build the full pattern using the name_pattern\n    location_pattern = re.compile(\n        f\"^({name_pattern}),\\\\s*({name_pattern})(?:,\\\\s*({name_pattern}))?$\"\n    )\n\n    match = re.match(location_pattern, location_string)\n    if not match:\n        return False\n\n    # Extract the captured groups\n    groups = match.groups()\n    city = groups[0].strip()\n    state = groups[1].strip()\n    country = groups[2].strip() if groups[2] else None\n    \n    # Ensure city and state are not empty\n    if not city or not state:\n        return False\n        \n    # If we have a country part (3-part format), ensure it's not empty\n    if groups[2] is not None and not country:\n        return False\n\n    substrings = [\"usa\", \"us\", \"united states\"]\n    # Any entries that state == USA are dropped\n    if state.lower() in substrings:\n        return False\n\n    # Check the we don't have other strings with the substrings\n    for substring in substrings:\n        if substring in state.lower():\n            return False\n        \n    # Passed all checks\n    return True\n```\n:::\n\n\nThe is_valid_location function is applied to the Location column of the users data frame, with the function output captured in a new column, ValidLocation. Users without a valid location are removed from the analysis as a latitude and longitude cannot be found for invalid locations. \n\n::: {#15321489 .cell execution_count=13}\n``` {.python .cell-code}\n# Apply is_valid_location function to the dataSeries Location\nusers[\"ValidLocation\"] = users[\"Location\"].apply(is_valid_location)\n\n# Drop rows without a valid location\nusers = users[users[\"ValidLocation\"]]\nnum_users_valid_location = users.shape[0]\n```\n:::\n\n\nThe number of users with valid locations is 7598.\n\nNow that valid locations have been found, the next step is to take the location information and separate it into city, state, and country for usage later. For this purpose, a function was created to extract the location parts from the location string.\nThe function extract_location_parts takes a string and splits it into parts at commas. If the split produces three parts, they are assumed to represent the city, state, and country. If, after the split, only two parts are obtained, then it is assumed the first part represents the city. The second part is checked to see if it matches any American state or territory; if it does, then the second part is assigned to the state and the country is set to \"USA\". If the second part doesn't match any American state or territory, it is assumed to represent a country and the state is set to None. This function could be improved further by adding a check to the country part, making sure it is a valid country.\n\n::: {#4d559751 .cell execution_count=14}\n``` {.python .cell-code}\ndef extract_location_parts(location_string):\n    \"\"\"\n    Extract the parts of a location as city, state, country from a provided\n     string.\n\n    :param location_string: String to get location parts from\n    :return: pandas data series containing the parts of the location\n    \"\"\"\n    # Split the input string by commas\n    parts = [part.strip() for part in location_string.split(\",\")]\n\n    # Check if already in city, state, country format\n    if len(parts) == 3:\n        city, state, country = parts\n    \n    # Check if in city, country or city, state format\n    elif len(parts) == 2:\n        city, second_part = parts\n        \n        # Check if the second part might be a US state\n        #us_states = {\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n        #             \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n        #             \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n        #             \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n        #             \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\",\n        #             \"DC\"}  # Adding District of Columbia\n        us_states = {\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States#States.\n            \"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n            \"HI\", \"IA\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\", \"MD\",\n            \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\",\n            \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n            \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\", \"WY\",\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States#Federal_district.\n            \"DC\",\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States\n        # #Inhabited_territories.\n            \"AS\", \"GU\", \"MP\", \"PR\", \"VI\",\n        }\n        \n        # Also check for full state names\n        us_state_names = {\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States#States.\n            \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \n            \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \n            \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \n            \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \n            \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \n            \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \n            \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \n            \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \n            \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \n            \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\",\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States#Federal_district.        \n            \"District of Columbia\",\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States\n        # #Inhabited_territories.\n            \"American Samoa\", \"Guam GU\", \"Northern Mariana Islands\",\n            \"Puerto Rico PR\", \"U.S. Virgin Islands\"\n        }\n\n        # If the second part looks like a US state, treat it as city, state and\n        #  add USA\n        if second_part.upper() in us_states or second_part in us_state_names:\n            state = second_part\n            country = \"USA\"\n        else:\n            # Otherwise, it's city, country format\n            country = second_part\n            state = None\n    \n    return pd.Series([city, state, country])\n```\n:::\n\n\nThe extract_location_parts function is applied to the Location column of the user data frame and the result is returned as three new columns: city, state, and country. For ease of processing, a copy of the users data frame is produced that only contains unique entries for the city, state, and country.\n\n::: {#4b4cc195 .cell execution_count=15}\n``` {.python .cell-code}\nusers[[\"City\", \"State\", \"Country\"]] = users[\"Location\"].apply(\n    extract_location_parts)\n\nusers_geo = users.drop_duplicates(\n    subset=[\"City\",\"State\",\"Country\"])[[\"City\", \"State\", \"Country\"]].copy()\nnum_unique_loc = users_geo.shape[0]\n```\n:::\n\n\nAfter cleaning the user's location data 2000 unique locations required geo-locating.\n\n### Obtaining latitude and longitude\nThe geopy package is used to obtain the latitude and longitude of a location . Geopy provides a client interface for several popular geocoding web services. The Nominatim web service was utilised as it is free. Nominatim uses OpenStreetMap data to find locations on Earth by name and address. The only limitations found were that it required only one request per second and could time out. A time out value of 60 seconds was found to work.\n\n::: {#411decc7 .cell execution_count=16}\n``` {.python .cell-code}\ndef get_lat_lon(city, country, state=None):\n    \"\"\"\n    Function to return the latitude and longitude given a city, state,\n     country or city, country. If no latitude or longitude can be \n     found for the provided address then None is returned.\n\n    :param city: string containing the name of locations city\n    :param country: string containing the name of locations country\n    :param state: Option string containing the name of locations state\n    :return: Pandas data series containing the latitude and longitude\n    \"\"\"\n    geolocator = Nominatim(user_agent=\"geocoder\")\n\n    # Slow down the requests. The Nominatim usage policy says no more than one\n    #  request per second\n    time.sleep(1.1)\n    \n    if state:\n        location = geolocator.geocode(f\"{city}, {state}, {country}\", timeout=60)\n    else:\n        location = geolocator.geocode(f\"{city}, {country}\", timeout=60)\n        \n    if location:\n        return pd.Series([location.latitude, location.longitude])\n    else:\n        return pd.Series([None, None])\n```\n:::\n\n\nSince the requesting latitude and longitude for each location was slow, it was only performed once, and the result was stored on disk. If the user_geolocation.csv file is found on disk, then the data is loaded into a data frame. Otherwise, it is calculated and stored on disk. \n\n::: {#36c87504 .cell execution_count=17}\n``` {.python .cell-code}\ngeo_location_file = os.path.join(\"data\", \"user_geolocation.csv\")\nif os.path.exists(geo_location_file):\n    print(f\"File '{geo_location_file}' exists.\")\n    print(f\"Reading geo locations.\")\n    users_geo = pd.read_csv(geo_location_file, comment=\"#\")\nelse:\n    print(f\"File '{geo_location_file}' does not exist.\")\n    print(f\"Creating geo locations.\")\n    users_geo[[\"Latitude\",\"Longitude\"]] = users_geo.apply(\n        lambda x: get_lat_lon(x[\"City\"], x[\"Country\"],x[\"State\"]), axis=1)\n\n    # Drop and location where a latitude or longitude was not found\n    users_geo = users_geo.dropna(subset=[\"Latitude\", \"Longitude\"])\n    users_geo.to_csv(geo_location_file, index=False)\n\nnum_unique_loc_val = users_geo.shape[0]\nnum_invalid_latlong = num_unique_loc - num_unique_loc_val\n```\n:::\n\n\nDue to false positives in the cleaning function, there were still some (71) invalid locations were a latitude and longitude did not exist. Thus the final number of unique user locations was 1929.\n\n### Producing a map of locations\nThe unique user locations were plotted onto a world map using Geopandas.\n\n::: {#cell-fig-world-plot .cell execution_count=18}\n``` {.python .cell-code}\ngdf = geopandas.GeoDataFrame(\n    users_geo, geometry=geopandas.points_from_xy(\n        users_geo.Longitude, users_geo.Latitude), crs=\"EPSG:4326\"\n)\n\nworld = geopandas.read_file(get_path(\"naturalearth.land\"))\n\nfig = plt.figure(figsize=(12, 6))\nax = fig.add_subplot()\n\n# Remove Antarctica\nworld.clip([-180, -55, 180, 90]).plot(\n    ax=ax, color=\"lightgray\", edgecolor=\"black\")\n\n# We can now plot our GeoDataFrame.\ngdf.plot(ax=ax, color=\"red\", markersize=8)\nax.set_xticks([])\nax.set_yticks([])\nplt.title(\"Location of Quant Stack Exchange users\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Location of Quant Stack Exchange users](task8HD_files/figure-html/fig-world-plot-output-1.png){#fig-world-plot width=912 height=387}\n:::\n:::\n\n\n@fig-world-plot shows the unique locations of the Quant Stack Exchange site users worldwide. Many users are clustered in Europe, the east and west coasts of the United States and India. Africa, South America and South-East Asia have fewer user sites. In Australia, the users are located around capital cities, with Perth being an exception, with no users. \n\n## The top 100 words used in the titles of the top 20\\% of questions\nFor this analysis, the titles of questions are analysed for the top 20\\% of posts based on their score, and the top 100 words are extracted and visualised as a word cloud. The count of the top 20 words are shown in a table. A post's score is the number of UpVotes minus the number of DownVotes.  \n\n### Data preparation\nTo prepare the data for visualisation, a data frame consisting of only questions is created from the posts data frame using the PostTypeId column. Questions have a PostTypeId = 1. A function (text_clean) is created that will clean the title text for each post. The cleaning involves converting all text to lowercase, removing any numbers, removing common words (stop words), and converting plurals to the singular version. The text_clean function is applied to the Title column from the questions data frame, and the result is returned as a new column, CleanTitle. The questions data frame is then filtered only to have posts with a score higher than the top 80\\% of scores. Cleaned titles from the remaining posts are converted to a list. \n\n::: {#fcdd443f .cell execution_count=19}\n``` {.python .cell-code}\n# Filter for questions only (PostTypeId = 1)\n# Make a copy as we modify this data frame later\nquestions = posts[posts[\"PostTypeId\"] == 1].copy()\n\n# Generic English stop words to ignore\nstop_words = get_stop_words(\"en\")\n\n# Define custom stop words for the finance domain\ncustom_stopwords = [\"can\", \"question\", \"using\", \"use\", \"value\", \"values\",\n        \"calculate\", \"formula\", \"formulas\", \"quant\", \"quantitative\",\n        \"finance\", \"financial\"]\n\n# Combine generic and custom stop words\nstop_words.extend(custom_stopwords)\n\n# Function to clean text\ndef clean_text(text, convert_plurals=False):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove numbers\n    text = re.sub(r\"\\d+\", \"\", text)\n    \n    # Remove stop words\n    words = [word for word in text.split() \n            if word not in stop_words and len(word) > 2]\n\n    # Convert plurals of words to their singular form\n    if convert_plurals:\n        p = inflect.engine()\n        words = [p.singular_noun(word) \n                if p.singular_noun(word) else word  for word in words]\n    \n    return \" \".join(words)\n\n# Clean the titles\nquestions[\"CleanTitle\"] = questions[\"Title\"].apply(\n    clean_text,  convert_plurals=True)\n\n# Separate high posts\nhigh_posts = questions[questions[\"Score\"] >= questions[\"Score\"].quantile(0.8)]\n\n# Combine all titles for each group\nhigh_posts_text = \" \".join(high_posts[\"CleanTitle\"].tolist())\n```\n:::\n\n\n### Visualisation and Top 20 words\n@fig-wordcloud shows the words used in the titles of the 20\\% of the questions. Option, model and volatility are the words that stand out. It isn't surprising that \"option\" was the most common occurring word in the questions, given it is the most used type of derivative by financial markets. @tbl-top20words shows the top 20 words that appear in the high-scoring posts. It is observed that the words are those that you would expect from financial discussions. The words I would have expected to see but are missing are those relating to programming, programming languages and specific financial models such as the black-scholes.\n\n::: {#cell-fig-wordcloud .cell execution_count=20}\n``` {.python .cell-code}\n# Generate word cloud for the top 100 words\nwordcloud = WordCloud(width=1000, height=600, background_color=\"white\",\n        max_words=100, collocations=False, contour_width=3\n        ).generate(high_posts_text)\n    \nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.title(\"Words in titles of the top 20% of Questions\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Words in titles of the top 20% of Questions](task8HD_files/figure-html/fig-wordcloud-output-1.png){#fig-wordcloud width=771 height=490}\n:::\n:::\n\n\n::: {#tbl-top20words .cell tbl-cap='Top 20 words in the title of the high-scoring questions' execution_count=21}\n``` {.python .cell-code}\n# Analyse word frequencies\nhigh_freq = Counter(high_posts_text.split())\n\n# Get the most common words in each category\ndf = pd.DataFrame(high_freq.most_common(20), columns=[\"Word\",\"Count\"])\nMarkdown(df.to_markdown(index = False))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=21}\n| Word       |   Count |\n|:-----------|--------:|\n| option     |     419 |\n| volatility |     383 |\n| model      |     364 |\n| price      |     282 |\n| portfolio  |     234 |\n| market     |     223 |\n| rate       |     199 |\n| datum      |     190 |\n| pricing    |     183 |\n| risk       |     174 |\n| return     |     171 |\n| stock      |     158 |\n| implied    |     154 |\n| trading    |     154 |\n| time       |     135 |\n| stochastic |     118 |\n| future     |     107 |\n| bond       |     100 |\n| interest   |     100 |\n| factor     |      91 |\n:::\n:::\n\n\n## Evolution of tag popularity over time {#sec-tags}\nThis visualisation tracks how the popularity of different tags changes over time. Tags are used to categorise posts, making it easier for people to search for posts related to a particular topic.\n\n### Data preparation\nThe first step is to drop any rows that contain a NaN in the Tags column of the posts data frame. The return from this operation is a view into the data frame, which will prevent the addition of any new columns. Therefore, a copy (posts_cleaned) of the data frame is made to allow for alteration. The tags for each post are stored as |tag1|tag2|tag3| etc. A function, separate_tags, was created to separate the tags from a string to a list of individual tags with this format. The separate_tags function is applied to the posts_cleaned data frame, and the results are stored in a new column called SepTag. Having the tags in a list isn't helpful for visualisation, so the explode function creates duplicate rows for each tag in the list of a given row (tag_posts). The creation date for the posts is used to create a new column containing each post's month-year. The data is then grouped by month and tag to allow for counting occurrences of each tag per month. The top ten tags by count are extracted into a list, which is then used to filter the monthly tag counts so only the top ten tags remain. The last step is to create a column in the posts_cleaned data frame for the tag type. If the SepTags column is empty, then the 'No Tag' label is applied; if the tag lists contain any of the top 10 tags, the label' Popular Tag' is applied; otherwise, the label 'Unpopular Tag' is applied. These labels will be used in later visualisations.\n\n::: {#c04758ed .cell execution_count=22}\n``` {.python .cell-code}\n# Drop rows with NA values in the Tags column and make a copy of the resulting\n#  data frame\nposts_cleaned = posts.dropna(subset=[\"Tags\"]).copy()\n\n# Tags in StackExchange are stored as |tag1|tag2|tag3|\ndef separate_tags(x):\n    \"\"\"\n    Function to separate tags and return as a list\n\n    :param x: string containing tags that need separating\n    :return: A list of tags \n    \"\"\"\n    return x[1:-1].split(\"|\")\n\n# Apply the separate_tags function to the \"Tags\" column and store the result\n#  in a new column\nposts_cleaned[\"SepTag\"] = posts_cleaned[\"Tags\"].apply(separate_tags)\n\n# Use explode to ensure that there is only one tag per row. This duplicates \n# rows for all other column values\ntag_posts = posts_cleaned.explode(\"SepTag\")\n\n# Group by month and tag to count occurrences\ntag_posts[\"Month\"] = tag_posts[\"CreationDate\"].dt.to_period(\"M\")\nmonthly_tag_counts = tag_posts.groupby(\n    [\"Month\", \"SepTag\"]).size().reset_index(name=\"Count\")\n\n# Convert Period to datetime for plotting\nmonthly_tag_counts[\"MonthDate\"] = monthly_tag_counts[\"Month\"].dt.to_timestamp()\n\n# Get the top 10 tags\ntop_tags = tag_posts[\"SepTag\"].value_counts().nlargest(10).index.tolist()\n\n# Filter for only the top tags\ntop_tag_counts = monthly_tag_counts[monthly_tag_counts[\"SepTag\"].isin(top_tags)]\n\n# Define conditions and choices\nconditions = [\n    # Empty list check\n    posts_cleaned[\"SepTag\"].str.len() == 0,  \n    # Intersection check\n    posts_cleaned[\"SepTag\"].apply(lambda x: bool(set(x) & set(top_tags)))  \n]\n\n# Create a TagType column\nchoices = [\"No Tag\", \"Popular Tag\"]\nposts_cleaned[\"TagType\"] = np.select(\n    conditions, choices, default=\"Unpopular Tag\")\n```\n:::\n\n\n### Visualisation of the top 10 tags\nFor the visualisation, a stacked area chart is chosen as it allows to see the cumulative total of the top 10 tags as well as their distribution. @fig-tag-evolve shows the evolution of the tag's popularity over time.\n\n::: {#cell-fig-tag-evolve .cell execution_count=23}\n``` {.python .cell-code}\n# Create a pivot table for the stacked area chart\npivot_data = top_tag_counts.pivot(\n    index=\"MonthDate\", columns=\"SepTag\", values=\"Count\")\n\n# Create the visualization\nplt.rcParams[\"figure.figsize\"] = (8,6)\n\n# Stacked area chart\nax = pivot_data.plot.area(alpha=0.7)\nplt.xlabel(\"Date\", fontsize=14)\nplt.ylabel(\"Number of Posts per Month\", fontsize=14)\n\n# Add a legend\nax.legend(loc=\"upper left\", fontsize=12)\nplt.tight_layout()\nplt.title(\"Evolution of tag popularity over time in Quant Stack Exchange site\")\nplt.grid(True, alpha=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Evolution of tag popularity over time in Quant Stack Exchange site](task8HD_files/figure-html/fig-tag-evolve-output-1.png){#fig-tag-evolve width=758 height=586}\n:::\n:::\n\n\nSome observations from this graph:\n\n* From 2011 to 2021, the monthly posts increased, although there were some sudden drops between 2016 and 2021. This might be attributed to periods of global economic challenges, such as the financial challenges in 2018. It was a year marked by market volatility, economic concerns, and a slowdown in growth, making it a challenging period for investors and the global economy.\n* The number of monthly posts declined from 2021 to the end of 2022. This period was during the COVID-19 pandemic. Was this decline related to COVID-19 or some other reasons?\n* Monthly posts have increased since 2023 but not back to the level of 2021.\n* ChatGPT launched at the end of 2022. Does this affect why the number of posts hasn't recovered since 2023? Are people now asking Chatgpt for answers rather than posting on the Quant Stack Exchange site? \n* From 2011 to 2024, the most common tags were options, options pricing, and black scholes, which are the mathematical equations used to price options.\n* Programming was not popular in the early years, but has gained popularity since, especially during the COVID-19 period.\n* 'Volatility' appears more often in the word cloud than in the tag usage. 'Black-scholes' appears in the top 10 tags but not in the top 20 title words. This implies users are using tag words in post titles but not tagging the posts with an appropriate tag, or they are tagging posts but not using the the tag words in the title. Therefore, if title words and tags are isolated, they are not good indicators of what topic the post is about.\n\n## Cross-site reference analysis for the top 25 domains\n\n### Data preparation\nA regular expression pattern is used to find all occurrences of 'http' or 'https' followed by '://', followed by words, hyphens and periods. The regular expression pattern is used with the pandas findall command on the Body column of the posts data frame. The results from the findall matching are returned as a list and stored in a new column called Domains. Similar to the tags processing, the data frame is exploded using the list entries in the Domains column, and the unique domain names are counted and returned as a data series in descending order. The domains are then classified as an internal domain, a link to another stack exchange site or stackoverflow, or an external domain. The top 25 domains and counts are then copied into another data frame for further processing. The total counts of each type are calculated, and then for each domain its percentage of type count and percentage of the overall count is evaluated. \n\n::: {#2d45d000 .cell execution_count=24}\n``` {.python .cell-code}\n# Regular expression for finding URL's in posts r\"https?://([\\w\\-\\.]+)\"\n  # https? - matches http or https\n  # :// - matches the colon and two forward slashes\n  # ([\\w\\-\\.]+) - Group to match domain name.\n    # [] character class\n      # \\w match any word (alphanumeric + underscore)\n      # \\- match a hyphen\n      # \\. match a period/dot\n      # + - make the match greedy and allow one or match of the character class\npattern = r\"https?://([\\w\\-\\.]+)\"\n\n# Find all patterns and return as a list stored in new column Domains.\nposts[\"Domains\"] = posts[\"Body\"].str.findall(pattern)\n\n# Use explode to create new rows for each entry in the lists in the domains\n#  column\n# Select the domains column and count the unique entries. The data series will \n# be returned sorted in descending order\n# Reset the index\ndomain_counts = posts.explode(\n    \"Domains\")[\"Domains\"].value_counts().reset_index().rename(\n        columns={\"count\":\"Count\"})\n\n# Classify the domains as internal or external using a regular expression \n# search for domain names\n# Any stackexchange or stackoverflow domains are internal \n# (r\"stackexchange|stackoverflow\")\ndomain_counts[\"Type\"] = domain_counts[\"Domains\"].str.contains(\n    r\"stackexchange|stackoverflow\", case=False, regex=True).map(\n        {True: \"Internal\", False: \"External\"})\n\n# Get the top 25 domains\n# Make a copy so we can add extra columns later\ntop_25_domains = domain_counts.head(25).copy() \n\n# Calculate total counts by type\ndomain_type_totals = top_25_domains.groupby(\"Type\")[\"Count\"].transform(\"sum\")\n\n# Calculate the overall total\ntotal_links = top_25_domains[\"Count\"].sum()\n\n# Add percentage columns\ntop_25_domains[\"Percentage of Type\"]= (\n    top_25_domains[\"Count\"] / domain_type_totals * 100).round(2)\ntop_25_domains[\"Percentage Overall\"] = (\n    top_25_domains[\"Count\"] / total_links * 100).round(2)\n\n# Sort by type and count (descending)\ntop_25_domains = top_25_domains.sort_values(\n    [\"Type\", \"Count\"], ascending=[True, False])\n```\n:::\n\n\n### Summary table and tree map for the top 25 domains\n\n::: {#9d55cd43 .cell execution_count=25}\n``` {.python .cell-code}\nposts_total = posts[\"Body\"].shape[0]\nposts_total_links = domain_counts.shape[0]\navg_links_post = round(total_links/posts_total_links)\n```\n:::\n\n\nSummary information for the cross-site data analysis:\n\n* Total number of posts (questions and answers): 48764\n* Total number of posts with website links: 4726\n* Total number of website links: 21210\n* Average links per post with any links: 4\n\nApproximately 10% of posts have website links. When a post has a link, the average number of links is 4.\n\n::: {#tbl-domaintype .cell tbl-cap='Domain Count, Type  and Percentages coloured by type' execution_count=26}\n``` {.python .cell-code}\ndef colour_rows_by_type(row):\n    \"\"\"\n    Function to set the background colour for a row based upon its domain type\n\n    :param row: Row to return background colour for\n    :return: List of background colours for each entry in the row\n    \"\"\"\n    if row[\"Type\"] == \"Internal\":\n        # Light blue for Internal\n        return [\"background-color: #E6F2FF\"] * len(row)  \n    else:\n        # Light red for External\n        return [\"background-color: #FFECE6\"] * len(row)  \n\n# Apply the styling and hide the index\nstyled_table = top_25_domains.style.apply(\n    colour_rows_by_type, axis=1).hide(axis=\"index\")\n\n# Basic styling\nstyled_table = styled_table.format({\"Count\": \"{:,d}\",\n    \"Percentage of Type\": \"{:.2f}%\",\n    \"Percentage Overall\": \"{:.2f}%\"\n})\n\n# Add a table style with borders\nstyled_table = styled_table.set_table_styles([\n    {\"selector\": \"th\", \"props\": [(\"background-color\", \"#f5f5f5\"), \n                                (\"color\", \"#333\"), \n                                (\"font-weight\", \"bold\"),\n                                (\"border\", \"1px solid #ddd\"),\n                                (\"padding\", \"8px\")]},\n    {\"selector\": \"td\", \"props\": [(\"border\", \"1px solid #ddd\"),\n                                (\"padding\", \"8px\")]},\n    {\"selector\": \"caption\", \"props\": [(\"caption-side\", \"top\"), \n                                     (\"font-size\", \"16px\"),\n                                     (\"font-weight\", \"bold\"),\n                                     (\"color\", \"#333\")]}\n])\n\nstyled_table\n```\n\n::: {.cell-output .cell-output-display execution_count=26 html-table-processing=none}\n```{=html}\n<style type=\"text/css\">\n#T_c9cd6 th {\n  background-color: #f5f5f5;\n  color: #333;\n  font-weight: bold;\n  border: 1px solid #ddd;\n  padding: 8px;\n}\n#T_c9cd6 td {\n  border: 1px solid #ddd;\n  padding: 8px;\n}\n#T_c9cd6 caption {\n  caption-side: top;\n  font-size: 16px;\n  font-weight: bold;\n  color: #333;\n}\n#T_c9cd6_row0_col0, #T_c9cd6_row0_col1, #T_c9cd6_row0_col2, #T_c9cd6_row0_col3, #T_c9cd6_row0_col4, #T_c9cd6_row1_col0, #T_c9cd6_row1_col1, #T_c9cd6_row1_col2, #T_c9cd6_row1_col3, #T_c9cd6_row1_col4, #T_c9cd6_row2_col0, #T_c9cd6_row2_col1, #T_c9cd6_row2_col2, #T_c9cd6_row2_col3, #T_c9cd6_row2_col4, #T_c9cd6_row3_col0, #T_c9cd6_row3_col1, #T_c9cd6_row3_col2, #T_c9cd6_row3_col3, #T_c9cd6_row3_col4, #T_c9cd6_row4_col0, #T_c9cd6_row4_col1, #T_c9cd6_row4_col2, #T_c9cd6_row4_col3, #T_c9cd6_row4_col4, #T_c9cd6_row5_col0, #T_c9cd6_row5_col1, #T_c9cd6_row5_col2, #T_c9cd6_row5_col3, #T_c9cd6_row5_col4, #T_c9cd6_row6_col0, #T_c9cd6_row6_col1, #T_c9cd6_row6_col2, #T_c9cd6_row6_col3, #T_c9cd6_row6_col4, #T_c9cd6_row7_col0, #T_c9cd6_row7_col1, #T_c9cd6_row7_col2, #T_c9cd6_row7_col3, #T_c9cd6_row7_col4, #T_c9cd6_row8_col0, #T_c9cd6_row8_col1, #T_c9cd6_row8_col2, #T_c9cd6_row8_col3, #T_c9cd6_row8_col4, #T_c9cd6_row9_col0, #T_c9cd6_row9_col1, #T_c9cd6_row9_col2, #T_c9cd6_row9_col3, #T_c9cd6_row9_col4, #T_c9cd6_row10_col0, #T_c9cd6_row10_col1, #T_c9cd6_row10_col2, #T_c9cd6_row10_col3, #T_c9cd6_row10_col4, #T_c9cd6_row11_col0, #T_c9cd6_row11_col1, #T_c9cd6_row11_col2, #T_c9cd6_row11_col3, #T_c9cd6_row11_col4, #T_c9cd6_row12_col0, #T_c9cd6_row12_col1, #T_c9cd6_row12_col2, #T_c9cd6_row12_col3, #T_c9cd6_row12_col4, #T_c9cd6_row13_col0, #T_c9cd6_row13_col1, #T_c9cd6_row13_col2, #T_c9cd6_row13_col3, #T_c9cd6_row13_col4, #T_c9cd6_row14_col0, #T_c9cd6_row14_col1, #T_c9cd6_row14_col2, #T_c9cd6_row14_col3, #T_c9cd6_row14_col4, #T_c9cd6_row15_col0, #T_c9cd6_row15_col1, #T_c9cd6_row15_col2, #T_c9cd6_row15_col3, #T_c9cd6_row15_col4, #T_c9cd6_row16_col0, #T_c9cd6_row16_col1, #T_c9cd6_row16_col2, #T_c9cd6_row16_col3, #T_c9cd6_row16_col4, #T_c9cd6_row17_col0, #T_c9cd6_row17_col1, #T_c9cd6_row17_col2, #T_c9cd6_row17_col3, #T_c9cd6_row17_col4, #T_c9cd6_row18_col0, #T_c9cd6_row18_col1, #T_c9cd6_row18_col2, #T_c9cd6_row18_col3, #T_c9cd6_row18_col4, #T_c9cd6_row19_col0, #T_c9cd6_row19_col1, #T_c9cd6_row19_col2, #T_c9cd6_row19_col3, #T_c9cd6_row19_col4 {\n  background-color: #FFECE6;\n}\n#T_c9cd6_row20_col0, #T_c9cd6_row20_col1, #T_c9cd6_row20_col2, #T_c9cd6_row20_col3, #T_c9cd6_row20_col4, #T_c9cd6_row21_col0, #T_c9cd6_row21_col1, #T_c9cd6_row21_col2, #T_c9cd6_row21_col3, #T_c9cd6_row21_col4, #T_c9cd6_row22_col0, #T_c9cd6_row22_col1, #T_c9cd6_row22_col2, #T_c9cd6_row22_col3, #T_c9cd6_row22_col4, #T_c9cd6_row23_col0, #T_c9cd6_row23_col1, #T_c9cd6_row23_col2, #T_c9cd6_row23_col3, #T_c9cd6_row23_col4, #T_c9cd6_row24_col0, #T_c9cd6_row24_col1, #T_c9cd6_row24_col2, #T_c9cd6_row24_col3, #T_c9cd6_row24_col4 {\n  background-color: #E6F2FF;\n}\n</style>\n<table id=\"T_c9cd6\">\n  <thead>\n    <tr>\n      <th id=\"T_c9cd6_level0_col0\" class=\"col_heading level0 col0\" >Domains</th>\n      <th id=\"T_c9cd6_level0_col1\" class=\"col_heading level0 col1\" >Count</th>\n      <th id=\"T_c9cd6_level0_col2\" class=\"col_heading level0 col2\" >Type</th>\n      <th id=\"T_c9cd6_level0_col3\" class=\"col_heading level0 col3\" >Percentage of Type</th>\n      <th id=\"T_c9cd6_level0_col4\" class=\"col_heading level0 col4\" >Percentage Overall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_c9cd6_row0_col0\" class=\"data row0 col0\" >i.stack.imgur.com</td>\n      <td id=\"T_c9cd6_row0_col1\" class=\"data row0 col1\" >9,163</td>\n      <td id=\"T_c9cd6_row0_col2\" class=\"data row0 col2\" >External</td>\n      <td id=\"T_c9cd6_row0_col3\" class=\"data row0 col3\" >51.49%</td>\n      <td id=\"T_c9cd6_row0_col4\" class=\"data row0 col4\" >43.20%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row1_col0\" class=\"data row1 col0\" >en.wikipedia.org</td>\n      <td id=\"T_c9cd6_row1_col1\" class=\"data row1 col1\" >2,945</td>\n      <td id=\"T_c9cd6_row1_col2\" class=\"data row1 col2\" >External</td>\n      <td id=\"T_c9cd6_row1_col3\" class=\"data row1 col3\" >16.55%</td>\n      <td id=\"T_c9cd6_row1_col4\" class=\"data row1 col4\" >13.88%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row2_col0\" class=\"data row2 col0\" >papers.ssrn.com</td>\n      <td id=\"T_c9cd6_row2_col1\" class=\"data row2 col1\" >1,073</td>\n      <td id=\"T_c9cd6_row2_col2\" class=\"data row2 col2\" >External</td>\n      <td id=\"T_c9cd6_row2_col3\" class=\"data row2 col3\" >6.03%</td>\n      <td id=\"T_c9cd6_row2_col4\" class=\"data row2 col4\" >5.06%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row3_col0\" class=\"data row3 col0\" >github.com</td>\n      <td id=\"T_c9cd6_row3_col1\" class=\"data row3 col1\" >779</td>\n      <td id=\"T_c9cd6_row3_col2\" class=\"data row3 col2\" >External</td>\n      <td id=\"T_c9cd6_row3_col3\" class=\"data row3 col3\" >4.38%</td>\n      <td id=\"T_c9cd6_row3_col4\" class=\"data row3 col4\" >3.67%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row4_col0\" class=\"data row4 col0\" >arxiv.org</td>\n      <td id=\"T_c9cd6_row4_col1\" class=\"data row4 col1\" >589</td>\n      <td id=\"T_c9cd6_row4_col2\" class=\"data row4 col2\" >External</td>\n      <td id=\"T_c9cd6_row4_col3\" class=\"data row4 col3\" >3.31%</td>\n      <td id=\"T_c9cd6_row4_col4\" class=\"data row4 col4\" >2.78%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row5_col0\" class=\"data row5 col0\" >www.cmegroup.com</td>\n      <td id=\"T_c9cd6_row5_col1\" class=\"data row5 col1\" >294</td>\n      <td id=\"T_c9cd6_row5_col2\" class=\"data row5 col2\" >External</td>\n      <td id=\"T_c9cd6_row5_col3\" class=\"data row5 col3\" >1.65%</td>\n      <td id=\"T_c9cd6_row5_col4\" class=\"data row5 col4\" >1.39%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row6_col0\" class=\"data row6 col0\" >www.investopedia.com</td>\n      <td id=\"T_c9cd6_row6_col1\" class=\"data row6 col1\" >284</td>\n      <td id=\"T_c9cd6_row6_col2\" class=\"data row6 col2\" >External</td>\n      <td id=\"T_c9cd6_row6_col3\" class=\"data row6 col3\" >1.60%</td>\n      <td id=\"T_c9cd6_row6_col4\" class=\"data row6 col4\" >1.34%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row7_col0\" class=\"data row7 col0\" >cran.r-project.org</td>\n      <td id=\"T_c9cd6_row7_col1\" class=\"data row7 col1\" >281</td>\n      <td id=\"T_c9cd6_row7_col2\" class=\"data row7 col2\" >External</td>\n      <td id=\"T_c9cd6_row7_col3\" class=\"data row7 col3\" >1.58%</td>\n      <td id=\"T_c9cd6_row7_col4\" class=\"data row7 col4\" >1.32%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row8_col0\" class=\"data row8 col0\" >ssrn.com</td>\n      <td id=\"T_c9cd6_row8_col1\" class=\"data row8 col1\" >252</td>\n      <td id=\"T_c9cd6_row8_col2\" class=\"data row8 col2\" >External</td>\n      <td id=\"T_c9cd6_row8_col3\" class=\"data row8 col3\" >1.42%</td>\n      <td id=\"T_c9cd6_row8_col4\" class=\"data row8 col4\" >1.19%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row9_col0\" class=\"data row9 col0\" >www.sciencedirect.com</td>\n      <td id=\"T_c9cd6_row9_col1\" class=\"data row9 col1\" >245</td>\n      <td id=\"T_c9cd6_row9_col2\" class=\"data row9 col2\" >External</td>\n      <td id=\"T_c9cd6_row9_col3\" class=\"data row9 col3\" >1.38%</td>\n      <td id=\"T_c9cd6_row9_col4\" class=\"data row9 col4\" >1.16%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row10_col0\" class=\"data row10 col0\" >www.sec.gov</td>\n      <td id=\"T_c9cd6_row10_col1\" class=\"data row10 col1\" >241</td>\n      <td id=\"T_c9cd6_row10_col2\" class=\"data row10 col2\" >External</td>\n      <td id=\"T_c9cd6_row10_col3\" class=\"data row10 col3\" >1.35%</td>\n      <td id=\"T_c9cd6_row10_col4\" class=\"data row10 col4\" >1.14%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row11_col0\" class=\"data row11 col0\" >finance.yahoo.com</td>\n      <td id=\"T_c9cd6_row11_col1\" class=\"data row11 col1\" >220</td>\n      <td id=\"T_c9cd6_row11_col2\" class=\"data row11 col2\" >External</td>\n      <td id=\"T_c9cd6_row11_col3\" class=\"data row11 col3\" >1.24%</td>\n      <td id=\"T_c9cd6_row11_col4\" class=\"data row11 col4\" >1.04%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row12_col0\" class=\"data row12 col0\" >onlinelibrary.wiley.com</td>\n      <td id=\"T_c9cd6_row12_col1\" class=\"data row12 col1\" >210</td>\n      <td id=\"T_c9cd6_row12_col2\" class=\"data row12 col2\" >External</td>\n      <td id=\"T_c9cd6_row12_col3\" class=\"data row12 col3\" >1.18%</td>\n      <td id=\"T_c9cd6_row12_col4\" class=\"data row12 col4\" >0.99%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row13_col0\" class=\"data row13 col0\" >doi.org</td>\n      <td id=\"T_c9cd6_row13_col1\" class=\"data row13 col1\" >202</td>\n      <td id=\"T_c9cd6_row13_col2\" class=\"data row13 col2\" >External</td>\n      <td id=\"T_c9cd6_row13_col3\" class=\"data row13 col3\" >1.14%</td>\n      <td id=\"T_c9cd6_row13_col4\" class=\"data row13 col4\" >0.95%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row14_col0\" class=\"data row14 col0\" >www.jstor.org</td>\n      <td id=\"T_c9cd6_row14_col1\" class=\"data row14 col1\" >196</td>\n      <td id=\"T_c9cd6_row14_col2\" class=\"data row14 col2\" >External</td>\n      <td id=\"T_c9cd6_row14_col3\" class=\"data row14 col3\" >1.10%</td>\n      <td id=\"T_c9cd6_row14_col4\" class=\"data row14 col4\" >0.92%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row15_col0\" class=\"data row15 col0\" >www.youtube.com</td>\n      <td id=\"T_c9cd6_row15_col1\" class=\"data row15 col1\" >178</td>\n      <td id=\"T_c9cd6_row15_col2\" class=\"data row15 col2\" >External</td>\n      <td id=\"T_c9cd6_row15_col3\" class=\"data row15 col3\" >1.00%</td>\n      <td id=\"T_c9cd6_row15_col4\" class=\"data row15 col4\" >0.84%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row16_col0\" class=\"data row16 col0\" >www.google.com</td>\n      <td id=\"T_c9cd6_row16_col1\" class=\"data row16 col1\" >175</td>\n      <td id=\"T_c9cd6_row16_col2\" class=\"data row16 col2\" >External</td>\n      <td id=\"T_c9cd6_row16_col3\" class=\"data row16 col3\" >0.98%</td>\n      <td id=\"T_c9cd6_row16_col4\" class=\"data row16 col4\" >0.83%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row17_col0\" class=\"data row17 col0\" >www.researchgate.net</td>\n      <td id=\"T_c9cd6_row17_col1\" class=\"data row17 col1\" >169</td>\n      <td id=\"T_c9cd6_row17_col2\" class=\"data row17 col2\" >External</td>\n      <td id=\"T_c9cd6_row17_col3\" class=\"data row17 col3\" >0.95%</td>\n      <td id=\"T_c9cd6_row17_col4\" class=\"data row17 col4\" >0.80%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row18_col0\" class=\"data row18 col0\" >www.bloomberg.com</td>\n      <td id=\"T_c9cd6_row18_col1\" class=\"data row18 col1\" >151</td>\n      <td id=\"T_c9cd6_row18_col2\" class=\"data row18 col2\" >External</td>\n      <td id=\"T_c9cd6_row18_col3\" class=\"data row18 col3\" >0.85%</td>\n      <td id=\"T_c9cd6_row18_col4\" class=\"data row18 col4\" >0.71%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row19_col0\" class=\"data row19 col0\" >www.quandl.com</td>\n      <td id=\"T_c9cd6_row19_col1\" class=\"data row19 col1\" >149</td>\n      <td id=\"T_c9cd6_row19_col2\" class=\"data row19 col2\" >External</td>\n      <td id=\"T_c9cd6_row19_col3\" class=\"data row19 col3\" >0.84%</td>\n      <td id=\"T_c9cd6_row19_col4\" class=\"data row19 col4\" >0.70%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row20_col0\" class=\"data row20 col0\" >quant.stackexchange.com</td>\n      <td id=\"T_c9cd6_row20_col1\" class=\"data row20 col1\" >2,286</td>\n      <td id=\"T_c9cd6_row20_col2\" class=\"data row20 col2\" >Internal</td>\n      <td id=\"T_c9cd6_row20_col3\" class=\"data row20 col3\" >66.96%</td>\n      <td id=\"T_c9cd6_row20_col4\" class=\"data row20 col4\" >10.78%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row21_col0\" class=\"data row21 col0\" >rads.stackoverflow.com</td>\n      <td id=\"T_c9cd6_row21_col1\" class=\"data row21 col1\" >590</td>\n      <td id=\"T_c9cd6_row21_col2\" class=\"data row21 col2\" >Internal</td>\n      <td id=\"T_c9cd6_row21_col3\" class=\"data row21 col3\" >17.28%</td>\n      <td id=\"T_c9cd6_row21_col4\" class=\"data row21 col4\" >2.78%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row22_col0\" class=\"data row22 col0\" >stats.stackexchange.com</td>\n      <td id=\"T_c9cd6_row22_col1\" class=\"data row22 col1\" >210</td>\n      <td id=\"T_c9cd6_row22_col2\" class=\"data row22 col2\" >Internal</td>\n      <td id=\"T_c9cd6_row22_col3\" class=\"data row22 col3\" >6.15%</td>\n      <td id=\"T_c9cd6_row22_col4\" class=\"data row22 col4\" >0.99%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row23_col0\" class=\"data row23 col0\" >math.stackexchange.com</td>\n      <td id=\"T_c9cd6_row23_col1\" class=\"data row23 col1\" >191</td>\n      <td id=\"T_c9cd6_row23_col2\" class=\"data row23 col2\" >Internal</td>\n      <td id=\"T_c9cd6_row23_col3\" class=\"data row23 col3\" >5.59%</td>\n      <td id=\"T_c9cd6_row23_col4\" class=\"data row23 col4\" >0.90%</td>\n    </tr>\n    <tr>\n      <td id=\"T_c9cd6_row24_col0\" class=\"data row24 col0\" >stackoverflow.com</td>\n      <td id=\"T_c9cd6_row24_col1\" class=\"data row24 col1\" >137</td>\n      <td id=\"T_c9cd6_row24_col2\" class=\"data row24 col2\" >Internal</td>\n      <td id=\"T_c9cd6_row24_col3\" class=\"data row24 col3\" >4.01%</td>\n      <td id=\"T_c9cd6_row24_col4\" class=\"data row24 col4\" >0.65%</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n@tbl-domaintype shows the domain counts, type and percentages. Observations of this data:\n\n* 16 \\% of the links in posts are to other Stack Exchange or Stack Overflow sites, i.e., they are internal.\n* Of the internal links, 67 \\% are self-links to other posts within the Quant Stack Exchange site, <https://quant.stackexchange.com>.\n* The remaining 84% of links are to external websites, with a majority of these being sites that host academic publications such as <https://papers.ssrn.com>, <https://arxiv.com>, <https://www.jstor> and <https://www.researchgate.net> \n* All images in Stack Exchange posts are hosted from <https://i.stack.imgur.com>. That's why this domain makes up 43.2% of the post links.\n\nApproximately 10% of the posts website links, and 43% are images.\n\nThe tabular data can also be graphically visualised using a tree map as shown in @fig-domaintree. This makes it easier to see the split between internal and external domains and well as the relative proportions within them.\n\n::: {#cell-fig-domaintree .cell execution_count=27}\n``` {.python .cell-code}\nfig = px.treemap(top_25_domains, path=[px.Constant(\"Links\"), \"Type\",\"Domains\"],\n    values=\"Count\", color=\"Count\",color_continuous_scale=\"rdbu_r\",\n    title=\"Tree map of the top 25 linked domains\")\nfig.update_traces(root_color=\"lightgrey\")\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()\n```\n\n::: {#fig-domaintree .cell-output .cell-output-display}\n```{=html}\n<div>                            <div id=\"4c746523-bb52-473a-a9d8-2159ea3ace53\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4c746523-bb52-473a-a9d8-2159ea3ace53\")) {                    Plotly.newPlot(                        \"4c746523-bb52-473a-a9d8-2159ea3ace53\",                        [{\"branchvalues\":\"total\",\"customdata\":[[589.0],[281.0],[202.0],[2945.0],[220.0],[779.0],[9163.0],[191.0],[210.0],[1073.0],[2286.0],[590.0],[252.0],[137.0],[210.0],[151.0],[294.0],[175.0],[284.0],[196.0],[149.0],[169.0],[245.0],[241.0],[178.0],[5364.910092155541],[1661.7592267135326],[4768.8442244224425]],\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"hovertemplate\":\"labels=%{label}<br>Count_sum=%{value}<br>parent=%{parent}<br>id=%{id}<br>Count=%{color}<extra></extra>\",\"ids\":[\"Links/External/arxiv.org\",\"Links/External/cran.r-project.org\",\"Links/External/doi.org\",\"Links/External/en.wikipedia.org\",\"Links/External/finance.yahoo.com\",\"Links/External/github.com\",\"Links/External/i.stack.imgur.com\",\"Links/Internal/math.stackexchange.com\",\"Links/External/onlinelibrary.wiley.com\",\"Links/External/papers.ssrn.com\",\"Links/Internal/quant.stackexchange.com\",\"Links/Internal/rads.stackoverflow.com\",\"Links/External/ssrn.com\",\"Links/Internal/stackoverflow.com\",\"Links/Internal/stats.stackexchange.com\",\"Links/External/www.bloomberg.com\",\"Links/External/www.cmegroup.com\",\"Links/External/www.google.com\",\"Links/External/www.investopedia.com\",\"Links/External/www.jstor.org\",\"Links/External/www.quandl.com\",\"Links/External/www.researchgate.net\",\"Links/External/www.sciencedirect.com\",\"Links/External/www.sec.gov\",\"Links/External/www.youtube.com\",\"Links/External\",\"Links/Internal\",\"Links\"],\"labels\":[\"arxiv.org\",\"cran.r-project.org\",\"doi.org\",\"en.wikipedia.org\",\"finance.yahoo.com\",\"github.com\",\"i.stack.imgur.com\",\"math.stackexchange.com\",\"onlinelibrary.wiley.com\",\"papers.ssrn.com\",\"quant.stackexchange.com\",\"rads.stackoverflow.com\",\"ssrn.com\",\"stackoverflow.com\",\"stats.stackexchange.com\",\"www.bloomberg.com\",\"www.cmegroup.com\",\"www.google.com\",\"www.investopedia.com\",\"www.jstor.org\",\"www.quandl.com\",\"www.researchgate.net\",\"www.sciencedirect.com\",\"www.sec.gov\",\"www.youtube.com\",\"External\",\"Internal\",\"Links\"],\"marker\":{\"coloraxis\":\"coloraxis\",\"colors\":[589.0,281.0,202.0,2945.0,220.0,779.0,9163.0,191.0,210.0,1073.0,2286.0,590.0,252.0,137.0,210.0,151.0,294.0,175.0,284.0,196.0,149.0,169.0,245.0,241.0,178.0,5364.910092155541,1661.7592267135326,4768.8442244224425]},\"name\":\"\",\"parents\":[\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/Internal\",\"Links/External\",\"Links/External\",\"Links/Internal\",\"Links/Internal\",\"Links/External\",\"Links/Internal\",\"Links/Internal\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links/External\",\"Links\",\"Links\",\"\"],\"values\":[589,281,202,2945,220,779,9163,191,210,1073,2286,590,252,137,210,151,294,175,284,196,149,169,245,241,178,17796,3414,21210],\"type\":\"treemap\",\"root\":{\"color\":\"lightgrey\"}}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"},\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":30}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Count\"}},\"colorscale\":[[0.0,\"rgb(5,48,97)\"],[0.1,\"rgb(33,102,172)\"],[0.2,\"rgb(67,147,195)\"],[0.3,\"rgb(146,197,222)\"],[0.4,\"rgb(209,229,240)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(253,219,199)\"],[0.7,\"rgb(244,165,130)\"],[0.8,\"rgb(214,96,77)\"],[0.9,\"rgb(178,24,43)\"],[1.0,\"rgb(103,0,31)\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Tree map of the top 25 linked domains\"},\"margin\":{\"t\":50,\"l\":25,\"r\":25,\"b\":25}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('4c746523-bb52-473a-a9d8-2159ea3ace53');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>\n```\n\nTree map of the top 25 linked domains\n:::\n:::\n\n\n## Programming language usage evolution\nThe evolution of tags, @sec-tags showed programming as a popular tag. In this section, posts containing code blocks are analysed for the programming language used to investigate any trends in the language usage over time.\n\n### Data preparation\nThe procedure used for preparing that data is:\n\n1. Extract code blocks from the body of posts using a regex pattern. This step is performed with the function extract_code_block.\n2. Using regex patterns, identify the programming languages used for each extracted code block. The function identify_programming_language performs this.\n3. Create a new data frame containing data gathered about the code block, such as the post ID, post type, post score, post length, language, and code length.\n\nAlong with full code functions, the code blocks in the Quant Stack Exchange also contain fragments of code, markdown tables, program outputs, and other miscellaneous text. Some programming languages are difficult to distinguish when they are only code fragments.\n\n::: {#0080cf27 .cell execution_count=28}\n``` {.python .cell-code}\n# Filter for questions and answers that likely contain code\ndef extract_code_blocks(body):\n    \"\"\"\n    Extract code blocks from post body using regex.\n\n    :param body: Body from posts as string\n    :return: List of strings containing the the text from each code block\n    \"\"\"\n    # If the body is empty, return an empty list\n    if pd.isna(body):\n        return []\n    \n    # Find code blocks (text between <code> tags)\n    # (.*?) \n        # () makes a capture group\n        # . matches any character except newlines\n        # * zero or more characters\n        # ? non-greedy, stops at first </code> tag after <code>\n    code_pattern = re.compile(r\"<code>(.*?)</code>\", re.DOTALL)\n    code_blocks = code_pattern.findall(body)\n    \n    return code_blocks\n```\n:::\n\n\n::: {#39d6d64b .cell execution_count=29}\n``` {.python .cell-code}\n# Function to identify programming languages in code blocks using regular\n#  expressions\ndef identify_programming_language(code_block):\n    \"\"\"\n    Identify the likely programming language of a code_block.\n    \n    This function uses regex patterns to try and identify the programming \n    language within the code block. However, code block in the Quant stack\n    exchange site can also contain markdown code for tables and other text\n    etc.\n    \n    :param code_block: String of the text between code tags in a posts\n    :return: String containing the language identified\n    \"\"\"\n    # Define regex patterns for different languages\n    patterns = {\n    #   Python patterns\n    #   1.  import\\s+ matches the word 'import' followed by one or more \n    #       whitespace characters\n    #   2.  def\\s+ matches the word 'def' followed by one or more \n    #       whitespace characters\n    #   3.  class\\s+ matches the word 'class' followed by one or more\n    #       white space characters\n    #   4.  \\s*for\\s+.*\\s+in\\s+ matches a Python for loop pattern,\n    #       \\s* zero or more white space characters, for the word 'for',\n    #       \\s+ one or more whitespace character after the for,\n    #       .* matches any characters, \\s+in\\s+ matches the word in with\n    #       whitespace on both sides\n    #   5.  numpy, pandas, matplotlib, scipy, sklearn, tensorflow, pytorch,\n    #       matches common python libraries\n    #   6.  np\\. matches np followed by a period\n    #   7.  pd.\\ matches pd followed by a period \n    #   8.  print\\( matches print(\n    #   9.  \\.plot\\( matches .plot(\n    #   10. datetime\\. matches, datetime followed by a period\n    #   11. \\[\\s*(\\d+)\\s*rows\\s*x\\s*(\\d+)\\s*columns\\s*\\]\\dt\\. matches the \n    #       dimension information about a data table \n    #       e.g., [ 12 rows x 12 columns]\n    #   12. \\dt. matches dt followed by a period\n        \"Python\": r\"\"\"import\\s+|def\\s+|class\\s+|\\s*for\\s+.*\\s+in\\s+|numpy|\n                    np\\.|pandas|pd\\.|matplotlib|scipy|sklearn|tensorflow|\n                    pytorch|print\\(f|\\.plot\\(|datetime\\.|\n                    \\[\\s*(\\d+)\\s*rows\\s*x\\s*(\\d+)\\s*columns\\s*\\]|\\dt\\.\"\"\",\n\n    # R patterns\n    #   1.  library\\( matches library\n    #   2.  <- Matches <-, which is used as an assignment operator in R\n    #   3.  (?<=.)\\$(?=.) matches the dollar sign when it is between two other\n    #       characters\n    #   4.  ddply matches the ddply function name\n    #   5.  rnorm matches the rnorm function name\n    #   6.  ggplot matches the ggplot function name\n    #   7.  data\\.frame matches data.frame\n    #   8.  function\\s*\\(.*\\)\\s*\\{ matches R function definition pattern, \n    #       function keyword followed by optional whitespace, matching \n    #       parentheses with any character between them, optional white \n    #       space followed by opening curly brace\n    #   9.  rbind matches the rbind function name\n    #   10. require\\( matches the require function\n    #   11. tidyr matches the package tidyr\n    #   12. caret matches the package caret\n    #   13. xts matches the package xts\n    #   14. quantmode matches the package quantmode\n        \"R\": r\"\"\"library\\(|<-|(?<=.)\\$(?=.)|ddply|rnorm|ggplot|data\\.frame|\n                function\\s*\\(.*\\)\\s*\\{|rbind|require\\(|tidyr|caret|xts|\n                quantmode\"\"\",\n        \n        # SQL patterns\n        # Matches either of the keywords SELECT, FROM, WHERE, JOIN, GROUP BY,\n        #  ORDER BY, INSERT, UPDATE, DELETE\n        \"SQL\": r\"\"\"SELECT|FROM|WHERE|JOIN|GROUP BY|ORDER BY|INSERT|UPDATE|\n                DELETE\"\"\",\n\n    # MATLAB patterns\n    #   1.  function\\s+.*\\s*= matches the MATLAB/Octave function declarations,\n    #       function followed by whitespace, any character, whitespace followed\n    #       by equals sign\n    #   2.  matlab, octave match either matlab or octave keywords\n    #   3.  \\.\\* Matches element-wise multiplication.*\n    #   4.  \\.\\^ matches element-wise power operation .^\n    #   5.  zeros\\( matches the zeros function zeros(\n    #   6.  ones\\( matches the ones function ones(\n    #   7.  figure\\s*\\( matches the figure function with option whitespace\n    #       between figure and the bracket\n    #   8.  linspace matches the keyword linspace\n    #   9.  matrix matches the keyword matrix\n        \"MATLAB\": r\"\"\"function\\s+.*\\s*=|matlab|octave|\\.\\*|\\.\\^|zeros\\(|ones\\(|\n                    figure\\s*\\(|linspace|matrix\"\"\",\n\n    # C/C++ patterns\n    #   1.  \\#include match #include\n    #   2.  int\\s+main matches the start of a main function declaration\n    #   3.  void\\s+\\w+\\s*\\(  matches a void funtion declaration void followed \n    #       by one or more spaces, oen or more words followed by zero or more\n    #       spaces and an open bracket\n    #   4.  std:: matches standard librart namespace\n    #   5.  printf matches printf statement\n    #   6.  cout match the c++ output stream\n    #   7.  template matches the keyword template\n    #   8.  boost:: matches the boost namespace\n    #   9.  eigen matches the eigen function\n        \"C/C++\": r\"\"\"\\#include|int\\s+main|void\\s+\\w+\\s*\\(|std::|printf|cout|\n                    template|boost::|eigen\"\"\",\n        \n    #   1.  Javascript patterns\n    #   2.  function\\s+\\w+\\s*\\( matches a javascript function declaration. \n    #       function followed by one or more whitespaces, one or more word\n    #       matches, zero or more whitespaces followed by an opening braket\n    #   3.  var\\s+ mactches the use of the var keyword\n    #   4.  let\\s+ matches the use of the let keyword\n    #   5.  const\\s+ matches the use of the constant keyword\n    #   6.  document\\. matches the DOM document object\n    #   7.  window\\. matches the browser window object\n    #   8.  Math\\. matches the Math object\n        \"Javascript\": r\"\"\"function\\s+\\w+\\s*\\(|var\\s+|let\\s+|const\\s+|\n                        document\\.|window\\.|Math\\.\"\"\",\n\n    #   1. VBA patterns\n    #   2. Sub\\s+ matches subroutn declaration\n    #   3. Function\\s+ matches a function declaration\n    #   4. Dim\\s+ Matches variable declarations in VBA\n    #   5. Worksheets matches the keyword worksheets in Excel VBA\n    #   6. Range\\( matches usage of the Range object\n    #   7. Cells\\( matches usage of the Cells object\n        \"Excel/VBA\": r\"\"\"Sub\\s+|Function\\s+|Dim\\s+|Worksheets|Range\\(|Cells\\(\"\"\",\n\n    # Mathematica patterns\n    #   1.  Plot\\[ matches command Plot followed by [\n    #   2.  Integrate\\[ matches command Integrate followed by [\n    #   3.  Solve\\[ matches command Solve followed by [\n    #   4.  Module\\[ matches command Module followed by [\n    #   5.  \\\\\\[Sigma\\] matches command \\[Sigma] \n        \"Mathematica\": r\"\"\"Plot\\[|Integrate\\[|Solve\\[|Module\\[|\\\\\\[Sigma\\[\"\"\",\n\n    # Latex patterns\n    #   1.  \\\\begin match \\begin\n    #   2.  \\\\end match \\end\n    #   3.  \\\\frac match \\frac\n    #   4.  \\\\sum match \\sum\n    #   5.  \\\\int match \\int\n    #   6.  mathbb match \\mathbb\n        \"Latex\": r\"\"\"\\\\begin|\\\\end|\\\\frac|\\\\sum|\\\\int|\\\\mathbb\"\"\",\n\n    # Markdown patterns\n    #   1.  \\| Any pipe\n    #   2.  \\|[\\s\\-\\|:]+ Match a table delimiter row\n    #        | --- | --- | or | :---:| :---:|\n    #   3.  -{2,} two or more hypens\n    #   4.  ={3,} three or more equals\n    #   5.  &quo t\n    #   6. \\*\\s astrix foollowed by white space\n        \"Markdown/HTML\":r\"\"\"\\||\\|[\\s\\-\\|:]+|-{2,}|={3,}|\\&quot;|\\*\\s\"\"\"\n    }\n    \n    # Check for language indicators\n    for lang, pattern in patterns.items():\n        if re.search(pattern, code_block, re.IGNORECASE):\n            return lang\n    \n    # Check for specific math symbols common in quant posts\n    if re.search(r\"\\\\sigma|\\\\mu|\\\\alpha|\\\\beta|\\\\Delta\", code_block):\n        return \"Mathematical Notation\"\n    \n    # If code block is very short check for specific features\n    #   1.  Simple function calls e.g. print(), sum(), calulcate123()\n    #   2.  Functions with arguments e.g. add(1,2), get(first, last)\n    #   3.  Method calls e.g. object.method()\n    #   4.  Nested calls e.g. print(name())\n    if len(code_block) < 50:\n        if re.search(r\"[a-zA-Z0-9]+\\([a-zA-Z0-9,\\s\\.]*\\)\", code_block):\n            return \"Formula\"\n\n    # No match then return Unknown\n    return \"Unknown\"\n```\n:::\n\n\n::: {#e218519f .cell execution_count=30}\n``` {.python .cell-code}\n# Extract code from posts\nposts[\"CodeBlocks\"] = posts[\"Body\"].apply(extract_code_blocks)\nposts[\"CodeCount\"] = posts[\"CodeBlocks\"].apply(len)\n\n# Filter posts with code\nposts_with_code = posts[posts[\"CodeCount\"] > 0].copy()\nnum_post_code_blocks = len(posts_with_code)\n\n# Identify the language for each code block\nlanguage_data = []\nfor _, row in posts_with_code.iterrows():\n    post_id = row[\"Id\"]\n    post_type = \"Question\" if row[\"PostTypeId\"] == 1 else \"Answer\"\n    score = row[\"Score\"]\n    post_length = len(row[\"Body\"])\n    \n    for i, code_block in enumerate(row[\"CodeBlocks\"]):\n        language = identify_programming_language(code_block)\n        code_length = len(code_block)\n        \n        language_data.append({\n            \"PostId\": post_id,\n            \"PostType\": post_type,\n            \"Score\": score,\n            \"PostLength\": post_length,\n            \"Language\": language,\n            \"CodeLength\": code_length\n        })\n\ncode_df = pd.DataFrame(language_data)\nlanguage_counts = code_df[\"Language\"].value_counts()\n\n# Gather some statistics\nnum_posts = posts.shape[0]\nper_of_posts = round((num_post_code_blocks/num_posts)*100, 1)\nnum_code_blocks = sum(posts[\"CodeCount\"])\navg_blocks_post = round(num_code_blocks/num_post_code_blocks)\n```\n:::\n\n\n4789 posts containing code blocks were found.\n\n\n### Programming language statistics and distribution \nAfter the data has been prepared, some statisitics about the code blocks found can be reported.\n\nCode block statistics:\n\n* Total posts analysed: 48764 \n* Posts containing code:  4789 (9.8)%\n* Total code blocks found: 14225\n* Average code blocks per post with code: 3\n\n@tbl-codelang shows the distribution of programming languages detected in the code blocks found in posts. You will notice that the 'Unknown' is significant, this is because of the large number of program outputs and code fragments contained in the code blocks. Further refinement of the regex patterns is recommended to reduce the number of 'unknown' detections.\n\n::: {#tbl-codelang .cell tbl-cap='Distribution of programming languages detected' execution_count=31}\n``` {.python .cell-code}\ndf_lang_dist = language_counts.to_frame(name=\"Count\")\n\ndf_lang_dist[\"Percent of code\"] = language_counts.apply(\n    lambda x: f\"{x*100/num_code_blocks:.2f}\")\nMarkdown(df_lang_dist.to_markdown(index = True))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=31}\n| Language              |   Count |   Percent of code |\n|:----------------------|--------:|------------------:|\n| Unknown               |    9503 |             66.8  |\n| Python                |    1634 |             11.49 |\n| R                     |     806 |              5.67 |\n| Markdown/HTML         |     802 |              5.64 |\n| Formula               |     668 |              4.7  |\n| SQL                   |     252 |              1.77 |\n| MATLAB                |     237 |              1.67 |\n| Javascript            |     151 |              1.06 |\n| C/C++                 |     104 |              0.73 |\n| Excel/VBA             |      50 |              0.35 |\n| Mathematica           |      12 |              0.08 |\n| Latex                 |       3 |              0.02 |\n| Mathematical Notation |       3 |              0.02 |\n:::\n:::\n\n\n@fig-langdist shows a bar chart of the distribution of programming languages in posts on the Quant Stack Exchange site. For plot, the 'Unknow', 'Markdown/HTML', 'Formula', 'Latex', and 'Mathemtical Notation' categories were removed to focus on actual languages used in software programming. From the figure, it is observed that Python is twice as popular as R, eight times more popular than SQL or MATLAB, and approximately sixteen times more popular than Javascript or C/C++.\n\n::: {#cell-fig-langdist .cell execution_count=32}\n``` {.python .cell-code}\n# Programming Language Distribution\nplt.figure(figsize=(8, 6))\n\n# Filter for languages with >10 occurrences and not the unknown, markdown,\n#  formula, notation or latex categories\nlanguage_counts = language_counts.loc[(~language_counts.index.str.contains(\n    \"Unk|Mark|Form|not|latex\")) & (language_counts > 10)] \n\nsns.barplot(x=language_counts.index, y=language_counts.values,\n palette=\"viridis\")\nplt.xlabel(\"Language\", fontsize=14)\nplt.ylabel(\"Number of Code Blocks\", fontsize=14)\nplt.xticks(rotation=45, ha=\"right\")\nplt.grid(axis=\"y\", alpha=0.3)\nplt.title(\"Programming Languages Used in Quant Stack Exchange Posts\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Programming Languages Used in Quant Stack Exchange Posts](task8HD_files/figure-html/fig-langdist-output-1.png){#fig-langdist width=758 height=565}\n:::\n:::\n\n\n### Language Usage Over Time\nBased on all the posts, Python may be the most popular programming language, but has it always been the most popular? A heatmap of language usage over time was created to answer this question.\nThe creation data of posts was used to create a column of the year of the posts in the posts_with_code data frame.  The language data was joined with the post dates using an inner join on 'PostId' from the language data frame and the 'Id' from the post dates data frame. The languages were then filtered to remove the 'Unknow', 'Markdown/HTML', 'Formula', 'Latex', and 'Mathemtical Notation' categories. A cross-table was created using the 'Year' and 'Language', and the number of code blocks per year was calculated by performing a sum on the table columns. The data in cross-table were then normalised using the count per year. Leaving the data as a percentage of code blocks for that year. This was done because the number of code blocks per year increased with time, and we are only interested in the change in the percentage of a language used for a given year. \n\n::: {#7a77481c .cell execution_count=33}\n``` {.python .cell-code}\n# Add year column using creationDate\nposts_with_code[\"Year\"] = posts_with_code[\"CreationDate\"].dt.year\n    \n# Join language data with post dates\nlanguage_dates = pd.merge(code_df, posts_with_code[[\"Id\", \"Year\", \"CodeCount\"]],\n  left_on=\"PostId\", right_on=\"Id\")\nlanguage_dates = language_dates[~language_dates[\"Language\"].str.contains(\n    \"Unk|Mark|Form|Not|Latex\")]\n\n# Count languages by year\nlanguage_by_year = pd.crosstab(\n    language_dates[\"Year\"], language_dates[\"Language\"])\n\n# Count the number of code blocks per year\ncode_blocks_by_year = language_by_year.sum(axis=1)\n\nlanguage_by_year_norm = language_by_year.div(code_blocks_by_year, axis=\"index\")\n```\n:::\n\n\n@fig-langtrends shows the evolution of the usage of each programming language between 2011 and 2024. Most notable in the figure is the rise of Python and the demise of R. R was the most popular language from 2011 until 2015. In 2015 Python and R had similar popularity; however, in 2020, Python became the dominant language used in the code blocks of the site, wih approximately 80\\% of the detected languages usage being Python. Another interesting feature is that SQL usage has declined since 2014. It is unlikely that people have stopped using databases, but more likely that people are now using Python to interact with databases rather than SQL. All other programming languages follow a similar trend to SQL; their usage was highest around 2011 but has dropped since 2017. \n\n::: {#cell-fig-langtrends .cell execution_count=34}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 8))\n\nax = sns.heatmap(language_by_year_norm, vmin=0, vmax=1, annot=False,\n linewidth=0.5, cmap=\"viridis\", fmt=\".2f\")\nplt.ylabel(\"Year\", fontsize=14)\nplt.xlabel(\"Language\", fontsize=14)\nax.invert_yaxis()\nplt.tight_layout()\nplt.title(\"Programming language usage trends over time (% of code blocks)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Programming language usage trends over time (\\% of code blocks)](task8HD_files/figure-html/fig-langtrends-output-1.png){#fig-langtrends width=715 height=772}\n:::\n:::\n\n\n## Time taken for a question to be answered\nIf I were to post a question today, how long would I need to wait until someone answered it? This question is the focus of this section.\n\n### Data preparation\nThe response time of answering a post needs to be calculated; this is done by creating two data frames from the post data frame, one containing the questions (PostTypeId=1) and the other containing the answers (PostTypeId=2). These two data frames are then merged using an inner joint on 'Id' from the questions and 'ParentId' from the answers, with the result being stored in a data frame called answered_df. Columns with the same name in each data frame are kept using the suffixes _question and _answer. The time in minutes to answer a question is calculated from the difference in creation dates. Answers that are faster than one minute are not likely to be correct and, therefore, are excluded from the data. Since there can be multiple answers to a question, we take the fastest answer response using group by and the min() function.\n\n::: {#f5db79a8 .cell execution_count=35}\n``` {.python .cell-code}\n# Response time calculation\nquestions_df = posts[posts[\"PostTypeId\"]==1].copy()\nanswers_df = posts[posts[\"PostTypeId\"]==2].copy()\n\n# Merge questions with their answers\naswered_df = pd.merge(\n    questions_df[[\"Id\", \"CreationDate\"]],\n    answers_df[[\"ParentId\", \"CreationDate\"]],\n    left_on=\"Id\",\n    right_on=\"ParentId\",\n    suffixes=(\"_question\", \"_answer\")\n)\n    \n# Calculate time difference in minutes\naswered_df[\"TimeInterval\"] = (\n    (aswered_df[\"CreationDate_answer\"] - aswered_df[\"CreationDate_question\"])\n    .dt.total_seconds() / 60\n)\n\n# Drop times less than 1 minute\naswered_df = aswered_df[~(aswered_df[\"TimeInterval\"] < 1) ]\n\n# Group by question Id and find the minimum time interval\nresponse_df = aswered_df.groupby(\"Id\")[\"TimeInterval\"].min().reset_index()\n    \n# Sort by question Id\nresponse_df = response_df.sort_values(\"Id\")\n```\n:::\n\n\n### Distribution of answer response time\n@fig-reponsehist shows the distribution of the log of the time taken in minutes to answer a question. Several vertical lines have been added to the figure to show the first quartile, the median, the third quartile and a 30-day response time. The distribution is bimodal, with the peak of the primary mode around 6.8 hours and the secondary peak around 307 days. What factors would influence the second mode in the distribution?\n\nThe Quant Stack Exchange site is a community-driven Q&A website governed by a reputation system. It rewards the users by giving repuation points and badges for the usefulness of their posts. The response time to answer a question depends on a number of factors, such as the question's quality and complexity, the availability of experts and the experts' interest in the question topic. For this study, only the relationship between the complexity of the question  and the response time to answer is considered. \n\n::: {#cell-fig-reponsehist .cell execution_count=36}\n``` {.python .cell-code}\n# Create histogram\nplt.hist(np.log(response_df[\"TimeInterval\"]), bins=50, color=\"skyblue\",\n edgecolor=\"black\")\n\n# Add a vertical line at Q1\nplt.axvline(x=4.6, color=\"tab:orange\", linestyle=\"--\", label=\"Q1 = 1.7 hours\")\n\n# Add a vertical line at 1 days\nplt.axvline(x=6, color=\"tab:green\", linestyle=\"--\", label=\"Median = 6.8 hours\")\n\n# Add a vertical line at Q1\nplt.axvline(x=7.51, color=\"tab:purple\", linestyle=\"--\", label=\"Q3 = 30.7 hours\")\n\n# Add a vertical line at 30 days\nplt.axvline(x=10.67, color=\"tab:red\", linestyle=\"--\", label=\"30 days\")\n\nplt.xlabel(\"Log of Time to Answer a question (minutes)\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.title(\"Distribution of Log(Response time (min))\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of the log of the time in minutes  to answer a question](task8HD_files/figure-html/fig-reponsehist-output-1.png){#fig-reponsehist width=667 height=523}\n:::\n:::\n\n\n### Relationship between response time and question complexity\nTo investigate the association between question complexity and the time taken to answer the question, the following parameters were studied:\n\n* The density of the mathematics in the question\n* The length of the question\n* Repuation of the asker\n* Readability of the question\n\nI define the density of mathematics in a question as the number of Latex equation patterns per word in the question after code blocks have been removed. A function was created to calculate the density of the mathematics used in a question. The function takes the body of a post,\nfinds code blocks using a regex, and replaces them with an empty string; then, Latex math patterns are found using regex, and the number is counted. The post's word count is calculated by finding all the words in the posts using a regular expression. The density is the number of Latex math expressions divided by the word count.\n\n::: {#3c1dbf86 .cell execution_count=37}\n``` {.python .cell-code}\n# Function to calculate math density using regex\ndef calculate_math_density(body):\n    \"\"\"\n    Calculate math density from post body using various regex patterns.\n\n    :param body: string of the body text\n    :return: Float containg the density of math in the post body\n    \"\"\"\n    if pd.isna(body):\n        return []\n    \n    body = body.lower()\n    \n    # Remove code blocks\n    code_pattern = re.compile(r\"<code>(.*?)</code>\", re.DOTALL)\n    body = re.sub(code_pattern, \"\", body)\n\n    math = []\n    \n    # Find inline LaTeX math (between $ signs)\n    inline_pattern = re.compile(r\"\\$([^$]+)\\$\")\n    inline_math = inline_pattern.findall(body)\n    math.extend(inline_math)\n    \n    # Find display LaTeX formulas (between $$ signs)\n    display_pattern = re.compile(r\"\\$\\$([^$]+)\\$\\$\")\n    display_math = display_pattern.findall(body)\n    math.extend(display_math)\n    \n    # Count words (simple split)\n    num_words = len(re.findall(r\"\\w+\", body))\n\n    # Avoid division by zero\n    if num_words == 0:\n        return 0\n\n    # Density = math matches per word\n    return len(math) / num_words\n```\n:::\n\n\nAfter the mathematics density is calculated for each question, a left join is performed between the response time and the question data frames to get the response time for each question with an answer. This is stored in a new questions_response_df data frame. The cleaned posts data frame created in @sec-tags is also merged with the question data to leave only rows in the clean posts data frame that are in the questions data. The tag type information in the cleaned posts data frame will be used to colour the points in the following scatter plots.  \n\n::: {#0fe0a443 .cell execution_count=38}\n``` {.python .cell-code}\n# Apply math density calculation to posts\nquestions_df[\"MathDensity\"] = questions_df[\"Body\"].apply(calculate_math_density)\n\n# Merge response_df with questions to get response time for questions with\n#  an answer\nquestion_response_df = pd.merge(\n    response_df,\n    questions_df,\n    on=\"Id\",\n    how=\"left\"\n)\n\n# Merge to ensure the rows are the same. \nposts_cleaned_filtered = posts_cleaned.merge(\n    question_response_df[[\"Id\"]], on=\"Id\", how=\"inner\")\n```\n:::\n\n\n::: {#cell-fig-spmathdensity .cell execution_count=39}\n``` {.python .cell-code}\n# Plot Response time versus mathematical density\npalette = [\"tab:red\", \"tab:green\"]\n\nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(x=question_response_df[\"MathDensity\"],\n    y=np.log(question_response_df[\"TimeInterval\"]),\n    alpha=0.6,hue=posts_cleaned_filtered[\"TagType\"], palette=palette)\n\nplt.xlabel(\"Mathematical density\")\nplt.ylabel(\"Log(Response time (min))\")\nplt.title(\"Scatter Plot of Log(Response time (min)) vs. Mathematical density\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Scatter plot of Log(Response time (min)) vs. Mathematical density](task8HD_files/figure-html/fig-spmathdensity-output-1.png){#fig-spmathdensity width=808 height=523}\n:::\n:::\n\n\n::: {#41fb8a27 .cell execution_count=40}\n``` {.python .cell-code}\nsp_correlation_md = round(question_response_df[\"MathDensity\"].corr(\n    np.log(question_response_df[\"TimeInterval\"]), method=\"spearman\"),3)\nprint(f\"\"\"The Spearman correlation between Mathematical density and \"\"\"\n\"\"\"Log(Response time) is {sp_correlation_md:.3f}\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Spearman correlation between Mathematical density and Log(Response time) is {sp_correlation_md:.3f}\n```\n:::\n:::\n\n\n@fig-spmathdensity shows a scatter plot between Log(Response time) and Mathematical density with the points coloured by tag type. The first notable feature of this plot is that there are no tags of type 'No Tag'. This shows that all answered questions had at least one tag set. The distribution of 'Unpopular' and 'Popular' tags is random and scattered throughout the plots with no visible clustering. The second notable feature is that there are a lot of answered questions that have no mathematics in them, i.e., a math density of zero. Lastly, the high response time is visible for all mathematical density values. The Spearman correlation coefficient (-0.004) shows no consistent monotonic relationship between the Log(Response time) and the Mathematical density. \n\n::: {#cell-fig-splength .cell execution_count=41}\n``` {.python .cell-code}\n# Plot Log response versus Log post length\nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(x=np.log(question_response_df[\"Body\"].apply(len)),\n    y= np.log(question_response_df[\"TimeInterval\"]),\n    alpha=0.6,hue=posts_cleaned_filtered[\"TagType\"], palette=palette)\n\nplt.xlabel(\"Log(Post length (characters))\")\nplt.ylabel(\"Log(Response time (min))\")\nplt.title(\n\"Scatter plot of Log(Response time (min)) vs. Log(Post length (characters))\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Scatter plot of Log(Response time (min)) vs. Log(Post length (characters))](task8HD_files/figure-html/fig-splength-output-1.png){#fig-splength width=808 height=523}\n:::\n:::\n\n\n::: {#2e4d83c5 .cell execution_count=42}\n``` {.python .cell-code}\nsp_correlation_pl = round(np.log(question_response_df['Body'].apply(len)).corr(\n    np.log(question_response_df['TimeInterval']), method='spearman'), 3)\nprint(f\"\"\"The Spearman correlation between Log(Post length) and \"\"\"\n\"\"\"Log(Response time) is {sp_correlation_pl:.3f}\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Spearman correlation between Log(Post length) and Log(Response time) is {sp_correlation_pl:.3f}\n```\n:::\n:::\n\n\n@fig-splength shows a scatter plot between Log(Response time) and Log(Post length). The majority of answered questions had a length between 150 and 3000 characters. Again, the distribution of the tag type is random and spreads throughout the entire domain of the plot. Two clusters are visible, one with a high response time and the other with a lower response time. The Spearman correlation coefficient (0.143) shows no consistent monotonic relationship between the Log(Response time) and Log(Post length). \n\n::: {#cell-fig-sprepuation .cell execution_count=43}\n``` {.python .cell-code}\nquestions_reputation_df = pd.merge(question_response_df, \n    users[['Id', 'Reputation']], left_on='OwnerUserId', right_on='Id',\n    how='left')\nquestions_reputation_df['Reputation'].fillna(1, inplace=True)\n\nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(\n    x=np.log(questions_reputation_df['Reputation']),\n    y=np.log(questions_reputation_df['TimeInterval']),\n    alpha=0.6,hue=posts_cleaned_filtered['TagType'], palette=palette)\n\nplt.xlabel('Log(Asker reputation)')\nplt.ylabel('Log(Response time)')\nplt.title('Scatter plot of Log(Response time (min)) vs. Log(Asker reputation)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\darrin\\AppData\\Local\\Temp\\ipykernel_74288\\578517144.py:4: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Scatter plot of Log(Response time (min)) vs. Log(Asker reputation)](task8HD_files/figure-html/fig-sprepuation-output-2.png){#fig-sprepuation width=808 height=523}\n:::\n:::\n\n\n::: {#a1cba355 .cell execution_count=44}\n``` {.python .cell-code}\nsp_correlation_ar = round(np.log(questions_reputation_df['Reputation']).corr(\n    np.log(question_response_df['TimeInterval']), method='spearman'), 3)\nprint(f\"\"\"The Spearman correlation between Log(Askers reputation) and \"\"\"\n\"\"\"Log(Response time) is {sp_correlation_ar:.3f}\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Spearman correlation between Log(Askers reputation) and Log(Response time) is {sp_correlation_ar:.3f}\n```\n:::\n:::\n\n\n@fig-sprepuation shows a scatter plot between Log(Response time) and Log(Asker reputation). Many users have a low reputation of one, and a scattering of users with a high reputation ask questions. The tag type again shows a random distribution throughout the plot. Questions from users with high reputations can also take a long time to answer. Again, the Spearman correlation coefficient (0.143) shows no consistent monotonic relationship between the Log(Response time) and Log(Askers reputation). \n\n::: {#ba0ea9cf .cell execution_count=45}\n``` {.python .cell-code}\n# Function to clean text\ndef clean_body(text):\n    \"\"\"\n    Removes number, html blocks and latex code from a given string.\n\n    :param text: String to clean\n    :return: clened text\n    \"\"\"\n    if pd.isna(text):\n        return \"\"\n    \n    # Remove numbers\n    text = re.sub(r\"\\d+\", \"\", text)\n\n    # Remove inline math $...$\n    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n    \n    # Remove display math $$...$$\n    text = re.sub(r\"\\$\\$.*?\\$\\$\", \"\", text)\n\n    # Remove <code> </code> blocks\n    code_pattern = re.compile(r\"<code>(.*?)</code>\", re.DOTALL)\n    text = re.sub(code_pattern, \"\", text)\n    \n    return text\n```\n:::\n\n\n::: {#cell-fig-spread .cell execution_count=46}\n``` {.python .cell-code}\n# Clean html and Latex from body\nquestion_response_df[\"CleanedBody\"] =  question_response_df[\"Body\"].apply(\n    clean_body)\n\n#Calculating readability score using Flesch Kincaid Grade\nquestion_response_df[\"FleschKincaidGrade\"] =  question_response_df[\"CleanedBody\"\n    ].apply(textstat.flesch_kincaid_grade)\n\nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(\n    x=question_response_df[\"FleschKincaidGrade\"],\n    y=np.log(question_response_df[\"TimeInterval\"]),\n    alpha=0.6,hue=posts_cleaned_filtered[\"TagType\"], palette=palette)\n\nplt.xlabel(\"Readability Score\")\nplt.ylabel(\"Log(Response time)\")\nplt.title(\"Scatter plot of Log(Response time (min)) vs. Readability Score\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Scatter plot of Log(Response time (min)) vs. Log(Asker reputation)](task8HD_files/figure-html/fig-spread-output-1.png){#fig-spread width=808 height=523}\n:::\n:::\n\n\n::: {#185ff6fc .cell execution_count=47}\n``` {.python .cell-code}\nsp_correlation_read = round(question_response_df[\"FleschKincaidGrade\"].corr(\n    question_response_df[\"TimeInterval\"], method=\"spearman\"), 3)\nprint(f\"\"\"The Spearman correlation between asker's reputation and response \"\"\"\n\"\"\"time is {sp_correlation_read:.3f}\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Spearman correlation between asker's reputation and response time is {sp_correlation_read:.3f}\n```\n:::\n:::\n\n\nThe readability score of a question was calculated using the Flesch Kincaid Grade. Higher numbers indicate harder-to-read posts. The readability score was calculated on the posts's body text after the mathmatical formula and code blocks were removed. @fig-spread shows a scatter plot between the Log(Response time) and Readability score. Most questions have a readability score of around 13, with some scoring above 20. There is no visible trend between response time and readability score. The Spearman correlation coefficient (0.074) shows no consistent monotonic relationship between the Log(Response time) and Readability score.\n\nIt has been seen that there is no consistent monotonic relationship between Log(Response time) and either of Mathematical density, Log(Post length), Log(Asker reputation) or Readability score.  The non-existence of a relationship between the Log(Response time) and the other variables does not mean we can rule out any relationship between these variables as there could still be:\n\n* Multivariate relationships where variables might only show associations in combination\n* Interaction effects among the independent variables\n\nFurther analysis that is beyond the scope of this work would be needed to draw a conclusion about the impact of question complexity on the time it takes to be answered. \n\n## Summary\n\nAnalysis of the Quant Stack Exchange site has been performed for the geographic distribution of the site's users, the popular words used in posts, the popularity of tags used in posts, the type of programming language discussed, and how this has changed over the site's life, and the response time to answering questions, and the relationship to question complexity.\n\n### User Geographic Distribution\n- Users are heavily concentrated in Europe, US coasts, and India\n- Lower representation in Africa, South America, and Southeast Asia\n- Australian users mainly in capital cities (except Perth)\n\n### Popular Content Analysis\n- The most common words in the top questions: \"option,\" \"volatility,\" and \"model\".\n- Financial terminology dominates high-scoring questions\n- Top tags evolved from 2011 to 2024: options, options pricing, and Black-Scholes consistently popular\n- Programming tags gained popularity, especially during COVID-19\n\n### Cross-Site References\n- ~10% of posts contain website links (average four links per post)\n- 84% of links are to external sites, with academic resources being common\n- 16% link to other Stack Exchange/Overflow sites\n- Images account for 43% of all post links\n\n### Programming Language Trends\n- Python overtook R as the dominant language (80% of code blocks by 2020)\n- R was most popular from 2011-2015\n- SQL and other languages have declined since 2017\n- About 10% of posts contain code blocks\n\n### Question Response Time Analysis\n- Distribution is bimodal: peaks at ~6.8 hours and ~307 days\n- No strong correlation was found between response time and:\n  - Mathematical content density\n  - Post length\n  - Asker reputation\n  - Text readability\n\nThe analysis suggests that while the site has evolved, particularly in programming language preferences, the factors that determine how quickly questions receive answers remain complex and possibly involve multivariate relationships beyond the scope of this analysis.\n\n",
    "supporting": [
      "task8HD_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script type=\"text/javascript\">\nwindow.PlotlyConfig = {MathJaxConfig: 'local'};\nif (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\nif (typeof require !== 'undefined') {\nrequire.undef(\"plotly\");\nrequirejs.config({\n    paths: {\n        'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n    }\n});\nrequire(['plotly'], function(Plotly) {\n    window._Plotly = Plotly;\n});\n}\n</script>\n\n"
      ]
    }
  }
}