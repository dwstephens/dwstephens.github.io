[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "ToDo\n\n\n\n Back to top"
  },
  {
    "objectID": "portfolio/ml/empty.html",
    "href": "portfolio/ml/empty.html",
    "title": "Empty",
    "section": "",
    "text": "This is a placeholder file.\n\n\n\n Back to top",
    "crumbs": [
      "Portfolio",
      "Machine Learning",
      "Empty"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html",
    "href": "portfolio/dw/Task7/Task_7HD.html",
    "title": "Data Mining",
    "section": "",
    "text": "# Versions of the libraries used are noted beside the import lines\n# Python version: 3.11.7\nimport numpy as np                              # 1.26.4\nimport pandas as pd                             # 2.1.4\nimport jupyter_bokeh                            # 4.0.5                              \nfrom itertools import combinations\nfrom bokeh.plotting import figure               # bokeh 3.3.4\nfrom bokeh.plotting import show\nfrom bokeh.layouts import column\nfrom bokeh.layouts import row\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import FactorRange\nfrom bokeh.models import RadioButtonGroup\nfrom bokeh.models import Div\nfrom bokeh.models import CustomJS\nfrom bokeh.models import Legend\nfrom bokeh.models import MultiChoice\nfrom bokeh.models import CheckboxGroup\nfrom bokeh.models import ColorBar\nfrom bokeh.models import BasicTicker\nfrom bokeh.models import PrintfTickFormatter\nfrom bokeh.models import Label, Select\nfrom bokeh.models import DataRange1d\nfrom bokeh.models import FixedTicker\nfrom bokeh.models import LabelSet\nfrom bokeh.models import LinearColorMapper\nfrom bokeh.models import NumberFormatter\nfrom bokeh.models import DataTable\nfrom bokeh.models import TableColumn\nfrom bokeh.models import HoverTool\nfrom bokeh.models import Whisker\nfrom bokeh.transform import linear_cmap\nfrom bokeh.palettes import Blues256\nfrom bokeh.palettes import Category10\nfrom bokeh.io import output_notebook\nimport sklearn.preprocessing                    # scikit-learn 1.2.2\nimport sklearn.model_selection\nimport sklearn.neighbors \nimport sklearn.metrics\n\n# Required to generate static images for pdf export\nfrom bokeh.io import export_png\nfrom IPython.display import Image\nfrom IPython.display import display\n\n# Enable notebook output\noutput_notebook()\n\ncreate_pdf = False\n\n    \n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#load-data-and-clean",
    "href": "portfolio/dw/Task7/Task_7HD.html#load-data-and-clean",
    "title": "Data Mining",
    "section": "Load data and clean",
    "text": "Load data and clean\nEach CSV file is loaded into pandas data frames. For each data frame, only the required variables were kept, with others being dropped. The remaining variable names were adjusted to be more meaningful. The data type (dtype) was checked (not shown in the code), and the existence of null values was checked. For some datasets, zero values were also removed.\n\nDemographics variables\nThe variables retained were:\n\nSEQN: Respondent sequence number\nRIAGENDR: Gender (male=1, female=2)\nRIDAGEYR: Age in years at screening\n\nI adjusted the gender values so that male = 0 and female = 2.\n\n# Load demographic data file in data frame\ndemo = pd.read_csv(\"Demographics.csv\", comment=\"#\")\n\n# Dropping the unnecessary columns\ncolumns_to_keep = [\"SEQN\", \"RIAGENDR\", \"RIDAGEYR\"]\ndemo = demo[columns_to_keep]\n\n# Adjust gender so male = 0 and female = 1\ndemo[\"RIAGENDR\"] -= 1\n\n# Rename columns\ndemo.rename(columns={\"RIAGENDR\": \"SEX\", \"RIDAGEYR\": \"AGE\"}, inplace=True)\n\nprint(f\"{demo.shape[0]} data points\")\n\n# Check how many null values in the data frame\nprint(\"Number of null values per variable\")\nprint(demo.isnull().sum())\n\n15600 data points\nNumber of null values per variable\nSEQN    0\nSEX     0\nAGE     0\ndtype: int64\n\n\n\n\nBody measurement variables\nThe variables retained were:\n\nSEQN Respondent sequence number\nBMXBMI Body Mass Index (kg/m^2)\n\n\n# Load body data file in a data frame\nbody = pd.read_csv(\"Body_Measurements.csv\", comment=\"#\")\n\n# Dropping the unnecessary columns\ncolumns_to_keep = [\"SEQN\", \"BMXBMI\"]\nbody = body[columns_to_keep]\n\n# Drop rows where the measurement is zero\nbody = body.loc[(body[\"BMXBMI\"] != 0)]\n\n# Rename columns\nbody.rename(columns={\"BMXBMI\": \"BMI\"}, inplace=True)\n\nprint(f\"{body.shape[0]} data points\")\n\n# Check how many null values in the data frame\nprint(\"Number of null values per variable\")\nprint(body.isnull().sum())\n\n13137 data points\nNumber of null values per variable\nSEQN    0\nBMI     0\ndtype: int64\n\n\n\n\nBlood pressure measurement variables\nThe variables retained were:\n\nSEQN Respondent sequence number\nBPXOSY1 Systolic - 1st oscillometric reading\nBPXODI1 Diastolic - 1st oscillometric reading\nBPXOSY2 Systolic - 2nd oscillometric reading\nBPXODI2 Diastolic - 2nd oscillometric reading\nBPXOSY3 Systolic - 3rd oscillometric reading\nBPXODI3 Diastolic - 3rd oscillometric reading\n\nThe three systolic and diastolic pressure readings were averaged, and only the average was retained. Blood pressure is measured in mmHg.\n\n# Load the blood pressure data file in data frame\nblood = pd.read_csv(\"Blood_Pressure_Measurement.csv\", comment=\"#\")\n\n# Dropping the unnecessary columns\nblood.drop(columns=[\"BPAOARM\", \"BPAOCSZ\", \"BPXOPLS1\", \"BPXOPLS2\", \"BPXOPLS3\"],\n            inplace=True)\n\n# Calcualte the averaage of the three measurements, retuning as an integer\nblood[\"BPXOSYX\"] = blood[[\"BPXOSY1\", \"BPXOSY2\", \"BPXOSY3\"]].mean(axis=1).astype(int)\nblood[\"BPXODIX\"] = blood[[\"BPXODI1\", \"BPXODI2\", \"BPXODI3\"]].mean(axis=1).astype(int)\n\n# Now we have the average of blood pressure, we don't need the original measurements\nblood.drop(columns=[\"BPXOSY1\", \"BPXOSY2\", \"BPXOSY3\",\"BPXODI1\", \"BPXODI2\", \"BPXODI3\"],\n            inplace=True)\n\n# Drop rows where the measurement is zero\nblood = blood.loc[(blood[\"BPXOSYX\"] != 0) & (blood[\"BPXODIX\"] != 0)]\n\n# Rename columns\nblood.rename(columns={\"BPXOSYX\": \"BPSY\", \"BPXODIX\": \"BPDI\"}, inplace=True)\n\nprint(f\"{blood.shape[0]} data points\")\n\n# Check how many null values in the data frame\nprint(\"Number of null values per variable\")\nprint(blood.isnull().sum())\n\n10353 data points\nNumber of null values per variable\nSEQN    0\nBPSY    0\nBPDI    0\ndtype: int64\n\n\n\n\nCholesterol HDL variables\nThe variables retained were:\n\nSEQN Respondent sequence number\nLBDHDD Direct HDL-Cholesterol (mg/dL)\n\n\n# Load  Cholesterol - HDL data file in data frame\ncol_hdl = pd.read_csv(\"Cholesterol_HDL.csv\", comment=\"#\")\n\n# Dropping the unnecessary columns\ncol_hdl.drop(columns=[\"LBDHDDSI\"], inplace=True)\n\n# Drop rows where the measurement is zero\ncol_hdl = col_hdl.loc[(col_hdl[\"LBDHDD\"] != 0)]\n\n# Rename columns\ncol_hdl.rename(columns={\"LBDHDD\": \"CHDL\"}, inplace=True)\n\nprint(f\"{col_hdl.shape[0]} data points\")\n\n# Check how many null values in the data frame\nprint(\"Number of null values per variable\")\nprint(col_hdl.isnull().sum())\n\n10828 data points\nNumber of null values per variable\nSEQN    0\nCHDL    0\ndtype: int64\n\n\n\n\nCholesterol Total\nThe variables retained were:\n\nSEQN Respondent sequence number\nLBXTC Total Cholesterol (mg/dL)\n\n\n# Load  Cholesterol - Total data file in data frame\ncol_total = pd.read_csv(\"Cholesterol_Total.csv\", comment=\"#\")\n\n# Dropping the unnecessary columns\ncol_total.drop(columns=[\"LBDTCSI\"], inplace=True)\n\n# Rename columns\ncol_total.rename(columns={\"LBXTC\": \"CTOT\"}, inplace=True)\n\nprint(f\"{col_total.shape[0]} data points\")\n\n# Check how many null values in the data frame\nprint(\"Number of null values per variable\")\nprint(col_total.isnull().sum())\n\n12198 data points\nNumber of null values per variable\nSEQN    0\nCTOT    0\ndtype: int64\n\n\n\n\nDiabetes\nThe variables retained were:\n\nSEQN Respondent sequence number\nDIQ010 Doctors has told you have diabetes (1 = Yes, 2 = No, 3 = Borderline, 7 = Refused, 9 = Don’t know)\n\nI am only interested in Yes/No, so I dropped any row with a value above 2 and adjusted the values so that No = 0 and Yes = 1.\n\n# Load  Diabetes questions file in data frame\ndiab = pd.read_csv(\"Diabetes.csv\", comment=\"#\")\n\n# Dropping the unnecessary columns\ncolumns_to_keep = [\"SEQN\", \"DIQ010\"]\ndiab = diab[columns_to_keep]\n\n# Keep only those who answer 1 or 2.\ndiab = diab[diab[\"DIQ010\"] &lt;= 2]\n\n# Adjust No value to be equal to 0\ndiab[\"DIQ010\"] = np.where(diab[\"DIQ010\"] == 2, diab[\"DIQ010\"] - 2, diab[\"DIQ010\"])\n\n# Rename columns\ndiab.rename(columns={\"DIQ010\": \"DIABETES\"}, inplace=True)\n\nprint(f\"{diab.shape[0]} data points\")\n\n# Check how many null values in the data frame\nprint(\"Number of null values per variable\")\nprint(diab.isnull().sum())\n\n14694 data points\nNumber of null values per variable\nSEQN        0\nDIABETES    0\ndtype: int64",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#smoking",
    "href": "portfolio/dw/Task7/Task_7HD.html#smoking",
    "title": "Data Mining",
    "section": "Smoking",
    "text": "Smoking\nThe variables retained were:\n\nSEQN Respondent sequence number\nSMQ020 Smoked at least 100 cigarettes in life (1 = Yes, 2 = No, 7 = Refused, 9 = Don’t know)\n\nI am only interested in Yes/No, so I dropped any row with a value greater than 2 and adjusted the values so that No = 0 and Yes = 1.\n\n# Load  smoking questions file in data frame\nsmoke = pd.read_csv(\"Smoking.csv\", comment=\"#\")\n\n# Dropping the unnecessary columns\ncolumns_to_keep = [\"SEQN\", \"SMQ020\"]\nsmoke = smoke[columns_to_keep]\n\n# Keep only those who answer 1 or 2.\nsmoke = smoke[smoke[\"SMQ020\"] &lt;= 2]\n\n# Adjust No value to be equal to 0\nsmoke[\"SMQ020\"] = np.where(smoke[\"SMQ020\"] == 2, smoke[\"SMQ020\"] - 2,\n                            smoke[\"SMQ020\"])\n\n# Rename columns\nsmoke.rename(columns={\"SMQ020\": \"SMOKES\"}, inplace=True)\n            \nprint(f\"{smoke.shape[0]} data points\")\n\n# Check how many null values in the data frame\nprint(\"Number of null values per variable\")\nprint(smoke.isnull().sum())\n\n11132 data points\nNumber of null values per variable\nSEQN      0\nSMOKES    0\ndtype: int64",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#coronary-heart-disease",
    "href": "portfolio/dw/Task7/Task_7HD.html#coronary-heart-disease",
    "title": "Data Mining",
    "section": "Coronary heart disease",
    "text": "Coronary heart disease\nThis data is for Coronary heart disease, a specific type of Cardiovascular disease. Cardiovascular disease is a broad term encompassing all diseases of the heart and blood vessels. The NHANES dataset didn’t have data specifically for cardiovascular disease, so I’m using coronary heart disease data as a substitute.\nThe variables retained were:\n\nSEQN Respondent sequence number\nMCQ160c Ever told you had coronary heart disease (1 = Yes, 2 = No, 7 = Refused, 9 = Don’t know)\n\nI am only interested in Yes/No, so I dropped any row with a value greater than 2 and adjusted the values so that No = 0 and Yes = 1.\n\n# Load  smoking questions file in data frame\nmedical = pd.read_csv(\"Medical_Conditions.csv\", comment=\"#\")\n\n# Dropping the unnecessary columns\ncolumns_to_keep = [\"SEQN\", \"MCQ160C\"]\nmedical = medical[columns_to_keep]\n\n# Keep only those who answer 1 or 2.\nmedical = medical[medical[\"MCQ160C\"] &lt;= 2]\n\n# Adjust No value to be equal to 0\nmedical[\"MCQ160C\"] = np.where(medical[\"MCQ160C\"] == 2, medical[\"MCQ160C\"] - 2,\n                                medical[\"MCQ160C\"])\n\n# Rename columns\nmedical.rename(columns={\"MCQ160C\": \"CD\"}, inplace=True)\n            \nprint(f\"{medical.shape[0]} data points\")\n\n# Check how many null values in the data frame\nprint(\"Number of null values per variable\")\nprint(medical.isnull().sum())\n\n14958 data points\nNumber of null values per variable\nSEQN    0\nCD      0\ndtype: int64",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#join-data-frames",
    "href": "portfolio/dw/Task7/Task_7HD.html#join-data-frames",
    "title": "Data Mining",
    "section": "Join data frames",
    "text": "Join data frames\nEach participant is uniquely identified in the datasets by their sequence number (SQEN). The demographics data frame contains an entry for each participant in the survey. In contrast, the other data frames contain a subset of participants based on different criteria for the data being collected. If I do a left merge between the other data frames and the demo data frames, I will keep entries for each participant and fill in missing data from the other data frames with NaNs.\n\n# Data frames to merge\ndfs = [body, blood, col_hdl, col_total, diab, smoke, medical]\n\n# Primary date frame\nnhanes_df = demo \n\n# Loop through all data frames, doing a left merge\nfor df in dfs:\n    nhanes_df = pd.merge(nhanes_df, df, on=\"SEQN\", how=\"left\")\n\n# Drop the sequence number column as it is no longer needed\nnhanes_df = nhanes_df.drop(\"SEQN\", axis=1)\n\n# Check how many null values in the data frame\nprint(nhanes_df.isnull().sum())\n\nprint(f\"{nhanes_df.shape[0]} number of points before dropping null values.\")\n\n# Drop all rows containing null values\nnhanes_df = nhanes_df.dropna()\n\n# Only interested in those over 20 and under 75\nnhanes_df = nhanes_df[(nhanes_df[\"AGE\"] &gt;= 20) & (nhanes_df[\"AGE\"] &lt;= 75)]\n\n\nprint(f\"{nhanes_df.shape[0]} data points in the final data frame.\")\n\n# Final check for null values\nprint(nhanes_df.isnull().sum())\n\nSEX            0\nAGE            0\nBMI         2423\nBPSY        5207\nBPDI        5207\nCHDL        4732\nCTOT        3362\nDIABETES     866\nSMOKES      4428\nCD           602\ndtype: int64\n15600 number of points before dropping null values.\n6298 data points in the final data frame.\nSEX         0\nAGE         0\nBMI         0\nBPSY        0\nBPDI        0\nCHDL        0\nCTOT        0\nDIABETES    0\nSMOKES      0\nCD          0\ndtype: int64",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#data-distributions",
    "href": "portfolio/dw/Task7/Task_7HD.html#data-distributions",
    "title": "Data Mining",
    "section": "Data Distributions",
    "text": "Data Distributions\nTo visualise the distributions of each variable using bokeh, a function was created that groups the data by sex and allows the user to visualise each sex separately or together on the same histogram plot. The distributions of the categorical variables, smoking, diabetes, and coronary heart disease, were not plotted using histograms as they were uninteresting. For these variables, a stacked bar graph was prepared instead.\n\n# Function to create an interactive histogram by sex for a given data frame\n#  and variable\ndef plot_histogram_by_sex(df, variable, type, nbins=24, units=\"\", add_categories=None):\n    \"\"\"\n    Create a histogram plot that allows the user to select to show \n    either, male, female or both male and female data.\n\n    :param df: Data frame to use\n    :param variable: Name of the column to plot\n    :param type: The label for the name for the plots \n    \"\"\"\n    sex_group = df.groupby(\"SEX\")\n    # 0: 'Male', 1: 'Female'\n    male_df = sex_group.get_group(0)[variable]\n    female_df = sex_group.get_group(1)[variable]\n    \n    # Set up histogram bins\n    max_value = df[variable].max()\n    min_value = df[variable].min()\n    bins = np.linspace(min_value, max_value, nbins+1)\n    hist_male, edges_male = np.histogram(male_df, bins=bins)\n    hist_female, edges_female = np.histogram(female_df, bins=bins)\n\n    # Create data sources\n    source_male = ColumnDataSource(data=dict(\n        top=hist_male,\n        bottom=np.zeros_like(hist_male),\n        left=edges_male[:-1],\n        right=edges_male[1:]))\n\n    source_female = ColumnDataSource(data=dict(\n        top=hist_female,\n        bottom=np.zeros_like(hist_female),\n        left=edges_female[:-1],\n        right=edges_female[1:]))\n\n    # Create the main figure for showing individual or overlaid histograms\n    p = figure(\n        title=type+\" Distribution\",\n        x_axis_label=type+units,\n        y_axis_label=\"Count\",\n        height=500,\n        width=900,\n        tools=\"pan,wheel_zoom,box_zoom,reset,save\",\n        active_drag=\"box_zoom\")\n\n    # Add the histograms as quad glyphs with initially male visible,\n    # female invisible\n    male_quad = p.quad(\n        top=\"top\", bottom=\"bottom\", left=\"left\", right=\"right\",\n        source=source_male,\n        fill_color=\"#1E90FF\", #Doger blue for males \n        line_color=\"white\",\n        alpha=0.7,\n        hover_fill_color=\"#4169E1\",\n        hover_line_color=\"white\",\n        legend_label=\"Male\",\n        name=\"male_hist\")\n\n    female_quad = p.quad(\n        top=\"top\", bottom=\"bottom\", left=\"left\", right=\"right\",\n        source=source_female,\n        fill_color=\"#FF1493\",  # Deeppink for females\n        line_color=\"white\",\n        alpha=0.5,  # Lower alpha for better overlay visibility\n        hover_fill_color=\"#FF1493\",\n        hover_line_color=\"white\",\n        legend_label=\"Female\",\n        visible=False,  # Initially hidden\n        name=\"female_hist\")\n\n    if add_categories:\n        # Add vertical lines to the plot\n        max_height = max(max(hist_male), max(hist_female)) * 1.1\n\n        # Add annotations for BMI categories\n        y_pos = max_height * 0.97\n\n        i = 0\n        for val, label, colour in add_categories:\n            line = p.line([val, val], [0, max_height], line_color=colour,\n                            line_width=2, line_dash=\"dashed\")\n            text = p.text(val + 1, y_pos - 16*i, [label], text_color=colour,\n                            text_font_size=\"8pt\")\n            i += 1\n            \n    # Create a header\n    header = Div(\n        text=f\"\"\"\n        &lt;h2 style='text-align: center; color: #444444;'&gt;{type} Distribution by Gender&lt;/h2&gt;\n        &lt;p style='text-align: center;'&gt;Select an option to view male, female, or overlaid {type} histograms&lt;/p&gt;\n        \"\"\",\n        width=700)\n\n    # Configure the legend\n    p.legend.location = \"top_right\"\n    p.legend.click_policy = \"hide\"  # Allow toggling by clicking the legend\n\n    # Set up the radio button group for selection with the third \"Both\" option\n    radio_button_group = RadioButtonGroup(labels=[\"Male\", \"Female\", \"Both\"],\n                                            active=0)\n\n    # Add JavaScript callback for interactivity\n    callback = CustomJS(args=dict(\n            male_quad=male_quad,\n            female_quad=female_quad,\n            plot=p, type=type\n        ), code=\"\"\"\n        if (cb_obj.active == 0) {\n            // Male only\n            male_quad.visible = true;\n            female_quad.visible = false;\n            let sex_text = \"Male \"\n            let title = sex_text.concat(type, \" Distribution\");\n            plot.title.text = title;\n        } else if (cb_obj.active == 1) {\n            // Female only\n            male_quad.visible = false;\n            female_quad.visible = true;\n            let sex_text = \"Female \"\n            let title = sex_text.concat(type, \" Distribution\");\n            plot.title.text = title;\n        } else {\n            // Both overlaid\n            male_quad.visible = true;\n            female_quad.visible = true;\n            let sex_text = \"Male & Female \"\n            let title = sex_text.concat(type, \" Distribution Comparison\");\n            plot.title.text = title;\n        }\n    \"\"\")\n\n    radio_button_group.js_on_change(\"active\", callback)\n\n    # Update the title based on initial selection\n    p.title.text = f\"Male {type} Distribution\"\n\n    # Create the layout\n    layout = column(header, radio_button_group, p)\n\n    if create_pdf:\n        # Create static images for pdf\n        export_png(layout, filename=f\"{variable}_plot.png\")\n        display(Image(filename=f\"{variable}_plot.png\"))\n    else:\n        # Display the plot in the notebook\n        show(layout, notebook_handle=False)\n\n\nDistribution of Age\nThe distribution of participants’ ages is shown in the figure below. The distribution for males and females is uniform, with approximately similar counts between the sexes. There are a couple of age groups with slightly more data, with the 60-year-old group having almost twice that of other age groups. The NHANES survey tries to balance participation across age groups and sexes, and this plot shows that this has been achieved.\n\nplot_histogram_by_sex(nhanes_df, \"AGE\", \"AGE\".title(), units=\" (years)\")\n\n\n  \n\n\n\n\n\n\n\nDistribution of BMI\nThe BMI distribution, shown below, is unimodal and skewed to the right for both sexes, although the skewness appears to be higher for females than males. Also superimposed on the plot are the health classifications based on BMI. To the left of the green line is underweight, and above the red line is severely obese. More females are severely obese than males.\n\nbmi_categories = [\n    (18.5, \"Normals\", \"#32CD32\"),  # Green\n    (25, \"Overweight\", \"#FFD700\"), # Gold\n    (30, \"Obese\", \"#FFA500\"),      # Orange\n    (35, \"Severely Obese\", \"#FF4500\")   # OrangeRed\n]\nplot_histogram_by_sex(nhanes_df, \"BMI\", \"BMI\", add_categories=bmi_categories,\n                        units=\" (kg/m^2)\")\n\n\n  \n\n\n\n\n\n\n\nDistribution of Systolic Blood Pressure\nThe distribution of systolic blood pressure, shown below, is unimodal and skewed to the right for both sexes. More females have a systolic blood pressure that is either in Hypertensive crisis or below normal. Both sexes contain very low values below 50 mmHg. These are potential measurement issues and should be investigated further.\n\nbpsy_categories = [\n    (120, \"Normal\", \"#32CD32\"),  # Green\n    (130, \"St 1 Hypertension\", \"#FFD700\"),     # Gold\n    (140, \"St 2 Hypertension\", \"#FFA500\"),      # Orange\n    (180, \"Hypertensive crisis\", \"#FF4500\")   # OrangeRed\n]\nplot_histogram_by_sex(nhanes_df, \"BPSY\", \"Systolic Blood Pressure\".title(),\n                        add_categories=bpsy_categories, units=\" (mmHg)\")\n\n\n  \n\n\n\n\n\n\n\nDistribution of Diastolic Blood Pressure\nThe distribution of diastolic blood pressure, shown below, is unimodal and symmetric for both sexes. Both males and females have potential outliers on either side of the distribution. Both sexes have a median diastolic blood pressure below what is considered normal, with some very low values below 40 mmHg. These are potential measurement issues and should be investigated further.\n\nbpdi_categories = [\n    (80, \"Normal\", \"#32CD32\"),  # Green\n    (90, \"St 1 Hypertension\", \"#FFD700\"),     # Gold\n    (120, \"Hypertensive crisis\", \"#FF4500\")   # OrangeRed\n]\nplot_histogram_by_sex(nhanes_df, \"BPDI\", \"Distolic Blood Pressure\".title(),\n                        add_categories=bpdi_categories, units=\" (mmHg)\")\n\n\n  \n\n\n\n\n\n\n\nDistribution of HDL Cholesterol\nThe distribution of HDL cholesterol, shown below, is unimodal and skewed to the right for both sexes. Both males and females have potential outliers on the right side of the distribution, with a few values larger than 150 mg/dL. Males tend to have lower HDL levels than females, and lower HDL levels are undesirable.\n\nplot_histogram_by_sex(nhanes_df, \"CHDL\", \"HDL Cholesterol\")\n\n\n  \n\n\n\n\n\n\n\nDistribution of Total Cholesterol\nThe distribution of total cholesterol, shown below, is unimodal and nearly symmetric for both sexes. The median value is similar for both males and females and is less than the desirable value of 200 mg/dL. Both sexes have potential outliers on the right side of the distribution, with a few values larger than 350 mg/dL.\n\nplot_histogram_by_sex(nhanes_df, \"CTOT\", \"Total Cholesterol\")\n\n\n  \n\n\n\n\n\n\n\nSmoking, Diabetes and Coronary heart disease\nThese variables are categorical, and a stack bar chart was prepared to visualise their distribution. Each variable had a possible value of either No or Yes. The stacked bar chart below provides a visual breakdown of the Yes and No relative proportions for each variable and sex.\n\n# Function for producing the stack bar chart of the categorical variables\ndef plot_categorical():\n    \"\"\"\n    Creates a stack bar chart for categorical variables\n\n    This function uses ideas found at\n    https://docs.bokeh.org/en/2.4.1/docs/user_guide/categorical.html\n    \"\"\"\n    # Group by SEX and get the counts of the other categorical variables\n    smoke_counts = nhanes_df.groupby(\"SEX\")[\"SMOKES\"].value_counts().unstack(\n                                        fill_value=0)\n    diabetes_counts = nhanes_df.groupby(\"SEX\")[\"DIABETES\"].value_counts().unstack(\n                                        fill_value=0)\n    cd_counts = nhanes_df.groupby(\"SEX\")[\"CD\"].value_counts().unstack(\n                                        fill_value=0)\n\n    factors = [(\"Smoke\", \"No\"), (\"Smoke\", \"Yes\"),\n        (\"Diabetes\", \"No\"), (\"Diabetes\", \"Yes\"),\n        (\"Coronary Heart Disease\", \"No\"), (\"Coronary Heart Disease\", \"Yes\"),\n    ]\n\n    regions = ['Male', 'Female']\n\n    source = ColumnDataSource(data=dict(\n        x = factors,\n        Male = [smoke_counts.iloc[0, 0], smoke_counts.iloc[0, 1],\n                diabetes_counts.iloc[0, 0], diabetes_counts.iloc[0, 1],\n                cd_counts.iloc[0, 0], cd_counts.iloc[0, 1]\n               ],\n        Female = [smoke_counts.iloc[1, 0], smoke_counts.iloc[1, 1],\n                  diabetes_counts.iloc[1, 0], diabetes_counts.iloc[1, 1],\n                  cd_counts.iloc[1, 0], diabetes_counts.iloc[1,1]\n                 ]))\n\n    p = figure(x_range=FactorRange(*factors), width=900, height=500,\n                y_axis_label=\"Count\", toolbar_location=None, tools=\"\")\n\n    p.vbar_stack(regions, x='x', width=0.9, alpha=0.5,\n                    color=[\"#1E90FF\", \"#FF1493\"], source=source,\n                    legend_label=regions)\n\n    p.y_range.start = 0\n    p.x_range.range_padding = 0.1\n    p.xaxis.major_label_orientation = 1\n    p.xgrid.grid_line_color = None\n    p.legend.location = \"top_center\"\n    p.legend.orientation = \"horizontal\"\n\n    if create_pdf:\n        # Create static images for pdf\n        export_png(p, filename=\"categorical_plot.png\")\n        display(Image(filename=\"categorical_plot.png\"))        \n    else:\n        show(p)\n\n\n# Produce the plot\nplot_categorical()\n\n\n  \n\n\n\n\n\nSome observations regarding smoking:\n\nThere are more non-smokers than smokers.\nApproximately the same number of males are smokers and non-smokers.\nThere are more non-smoking females than smoking females.\nThere are more non-smoking females than non-smoking males.\n\nSome observations regarding diabetes:\n\nThere are many more people without diabetes than with diabetes.\nOf the people with diabetes, approximately 50% are male and 50% are female.\nThere are more men with diabetes than females.\n\nSome observations regarding Coronary heart disease\n\nMany more people haven’t been diagnosed than those who have.\nOf the people without coronary heart disease, approximately 50% are male and 50% are female.\nThere are more females diagnosed with coronary heart disease than males.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#correlation",
    "href": "portfolio/dw/Task7/Task_7HD.html#correlation",
    "title": "Data Mining",
    "section": "Correlation",
    "text": "Correlation\nA correlogram plot was used to compare the linear association between all pairs of quantitative variables. A function was created to create this plot and provide interactivity for the user. The user can select the sex and other factors such as smoking or diabetes. The plot will then be updated to show the correlation between the chosen options, e.g., female, non-smoker with diabetes.\n\n# Function to create correlation matrix\ndef create_correlation_plot():\n    \"\"\"\n    Creates a correlogram to be used as a correlation matrix.\n\n    The ideas for this function came from the correlogram example in the\n    bokeh documentation.\n    https://docs.bokeh.org/en/3.3.4/docs/examples/topics/categorical/correlogram.html\n    \"\"\"\n    # Plot correlation matrix\n    groups = nhanes_df.groupby([\"SEX\", \"SMOKES\", \"DIABETES\"])\n    df = groups.get_group((0,0,0)) # Male, non-smoker, non-diabetic\n    df = df[df.columns[~df.columns.isin([\"SEX\", \"SMOKES\", \"DIABETES\"])]]\n\n    all_vars = list(df.columns)\n    pairs = list(combinations(all_vars, 2))\n     \n    x, y = list(zip(*pairs))\n\n    # Create a dictionary to store our data with all combinations\n    data = {}\n    \n    # Generate data for all combinations\n    for gender in [\"male\", \"female\"]:\n        # male=0, female =1\n        g_factor = 0 if gender == \"male\" else 1\n    \n        for smoker in [False, True]:\n            # no = 0, yes =1\n            s_factor = 0 if not smoker else 1\n        \n            for diabetic in [False, True]:\n                # no = 0, yes = 1\n                d_factor = 0 if not diabetic else 1\n            \n                # Create a key for this combination\n                key = f'{gender}_{\"smoker\" if smoker else \"nonsmoker\"}_{\"diabetic\" if diabetic else \"nondiabetic\"}'\n\n                df = groups.get_group((g_factor,s_factor,d_factor))\n                df = df[df.columns[~df.columns.isin([\"SEX\", \"SMOKES\", \"DIABETES\"])]]\n\n                correlations = []\n                for a, b in pairs:\n                    matrix = np.corrcoef(df[a], df[b])\n                    correlations.append(matrix[0, 1])\n\n                data[key] = {\"var_1\": x, \"var_2\": y,\n                    \"correlation\": correlations,\n                    \"dot_size\": [(1+ 10 * abs(corr)) * 10 for corr in correlations],\n                    \"texts\": [f\"{x:.2f}\" for x in correlations]}\n\n    new_df= pd.DataFrame(data[\"male_nonsmoker_nondiabetic\"])\n    x_range = new_df[\"var_1\"].unique()\n    y_range = list(new_df[\"var_2\"].unique())\n    \n    data_sources= {}    \n    # Add all combinations to each data source\n    for key in data:\n        data_sources[key] = ColumnDataSource(pd.DataFrame(data[key]))\n        \n    p = figure(width=800, height=800,\n        x_range= x_range, y_range= y_range,\n        x_axis_location=\"above\", \n        toolbar_location=None,\n        tools=\"hover\",\n        tooltips=[(\"correlation\", \"@texts\")],\n        background_fill_color=\"#fafafa\")\n\n    # Create a renderers dictionary to store all line renderers\n    renderers = {}\n\n    # Create a line for each combination of factors for each metric\n    for gender in [\"male\", \"female\"]:\n        for smoker in [False, True]:\n            for diabetic in [False, True]:\n                renderer_key = f'{gender}_{\"smoker\" if smoker else \"nonsmoker\"}_{\"diabetic\" if diabetic else \"nondiabetic\"}'\n                renderers[renderer_key] = p.scatter(x=\"var_1\", y=\"var_2\",\n                    size=\"dot_size\",\n                    source=data_sources[renderer_key],\n                    fill_color=linear_cmap(\"correlation\", \"RdYlGn9\", -1, 1),\n                    line_color=\"#202020\",\n                    visible=False)\n\n    # Default is \"male_nonsmoker_nondiabetic\"\n    c = renderers[\"male_nonsmoker_nondiabetic\"]\n    c.visible= True\n    color_bar = c.construct_color_bar(\n        location=(300, 0),\n        ticker=FixedTicker(ticks=[-1, 0.0, 1]),\n        title=\"correlation\",\n        major_tick_line_color=None,\n        width=400,\n        height=30)\n\n    p.add_layout(color_bar, \"below\")\n    p.axis.major_tick_line_color = None\n    p.axis.major_tick_out = 0\n    p.axis.axis_line_color = None\n    p.grid.grid_line_color = None\n    p.outline_line_color = None\n\n    # Create MultiChoice widget with only demographic options\n    options = [\n        (\"female\", 'Female'),  # Default is male\n        (\"smoker\", \"Smoker\"),  # Default is non-smoker\n        (\"diabetic\", \"Diabetic\")  # Default is non-diabetic\n    ]\n\n    # Initially select no modifiers (default to male, non-smoker, non-diabetic)\n    multi_choice = MultiChoice(value=[], options=options, \n                    title=\"Select sex and other factors:\", width=300)\n\n    # Create info text\n    info_div = Div(text=\"\"\"&lt;p&gt;&lt;b&gt;Current Selection:&lt;/b&gt; Male, Non-smoker, Non-diabetic&lt;/p&gt;\"\"\", \n               width=400, height=30)\n\n    # CustomJS callback to control visibility\n    callback = CustomJS(args=dict(\n            renderers=renderers, info_div=info_div\n        ), code=\"\"\"\n        // Get the selected values from the MultiChoice widget\n        const selected = cb_obj.value;\n    \n        // Check selections with defaults\n        const isFemale = selected.includes('female');\n        const gender = isFemale ? 'female' : 'male';\n    \n        const isSmoker = selected.includes('smoker');\n        const smokerStatus = isSmoker ? 'smoker' : 'nonsmoker';\n    \n        const isDiabetic = selected.includes('diabetic');\n        const diabeticStatus = isDiabetic ? 'diabetic' : 'nondiabetic';\n    \n        // Build the key\n        const key = gender + '_' + smokerStatus + '_' + diabeticStatus;\n    \n        // Update info display\n        let infoText = \"&lt;p&gt;&lt;b&gt;Current Selection:&lt;/b&gt; \" + \n                  (gender === 'male' ? 'Male' : 'Female') + \", \" +\n                  (smokerStatus === 'smoker' ? 'Smoker' : 'Non-smoker') + \", \" +\n                  (diabeticStatus === 'diabetic' ? 'Diabetic' : 'Non-diabetic') + \"&lt;/p&gt;\";\n        info_div.text = infoText;\n    \n        // Default all renderers to invisible first\n        for (let name in renderers) {\n            renderers[name].visible = false;\n        }\n    \n        // Show the selected profile\n        if (key in renderers) {\n            renderers[key].visible = true;\n        }\n    \"\"\")\n\n    # Attach the callback to the widget\n    multi_choice.js_on_change(\"value\", callback)\n\n    # Create the layout and show the result\n    layout = column(row(multi_choice, info_div), p)\n\n    if create_pdf:\n        # Create static images for pdf\n        export_png(layout, filename=\"correlation_plot.png\")\n        display(Image(filename=\"correlation_plot.png\"))\n    else:\n        show(layout)\n    \n    return data\n\n\ncorrelation_data = create_correlation_plot()\n\n\n  \n\n\n\n\n\n\n# Create data table for the correlstion information\ndef create_correlation_table(corr_data):\n    \"\"\"\n    Creates a bokeh data table for the correlation information. The\n    table has the same interactivty as the correlogram plot.\n\n    :param corr_data: Dictionary containg the correlation data\n    \"\"\"\n    # Store all datasets in a JS-accessible args dict\n    all_data = corr_data\n\n    # The source attached to the DataTable\n    source =  ColumnDataSource(all_data[\"male_nonsmoker_nondiabetic\"])\n\n    columns = [TableColumn(field=\"var_1\", title=\"Variable 1\"),\n                TableColumn(field=\"var_2\", title=\"Variable 2\"),\n                TableColumn(field=\"correlation\", title=\"Correlation\",\n                            formatter=NumberFormatter(format=\"0.000\",\n                            text_align=\"center\"))\n            ]\n    data_table = DataTable(source=source, columns=columns, width=500, height=600)\n\n    # Create MultiChoice widget with only demographic options\n    options = [\n        (\"female\", 'Female'),  # Default is male\n        (\"smoker\", \"Smoker\"),  # Default is non-smoker\n        (\"diabetic\", \"Diabetic\")  # Default is non-diabetic\n        ]\n\n    # Initially select no modifiers (default to male, non-smoker, non-diabetic)\n    multi_choice = MultiChoice(value=[], options=options, \n                    title=\"Select sex and behaviour factors:\", width=300)\n\n    # Create info text\n    info_div = Div(text=\"\"\"&lt;p&gt;&lt;b&gt;Current Selection:&lt;/b&gt; Male, Non-smoker, Non-diabetic&lt;/p&gt;\"\"\", \n               width=400, height=30)\n\n    # Pass all datasets as an argument to CustomJS\n    callback = CustomJS(args=dict(\n            source=source, all_data=all_data, info_div=info_div\n        ), code=\"\"\"\n        // Get the selected values from the MultiChoice widget\n        const selected = cb_obj.value;\n    \n        // Check selections with defaults\n        const isFemale = selected.includes('female');\n        const gender = isFemale ? 'female' : 'male';\n    \n        const isSmoker = selected.includes('smoker');\n        const smokerStatus = isSmoker ? 'smoker' : 'nonsmoker';\n    \n        const isDiabetic = selected.includes('diabetic');\n        const diabeticStatus = isDiabetic ? 'diabetic' : 'nondiabetic';\n    \n        // Build the key\n        const key = gender + '_' + smokerStatus + '_' + diabeticStatus;\n    \n        // Update info display\n        let infoText = \"&lt;p&gt;&lt;b&gt;Current Selection:&lt;/b&gt; \" + \n                  (gender === 'male' ? 'Male' : 'Female') + \", \" +\n                  (smokerStatus === 'smoker' ? 'Smoker' : 'Non-smoker') + \", \" +\n                  (diabeticStatus === 'diabetic' ? 'Diabetic' : 'Non-diabetic') + \"&lt;/p&gt;\";\n        info_div.text = infoText;\n\n        // Overwrite the source's data\n        source.data = {...all_data[key]};\n\n        source.change.emit();\n    \"\"\")\n\n    # Attach the callback to the widget\n    multi_choice.js_on_change(\"value\", callback)\n\n    layout = column(row(multi_choice, info_div), data_table)\n\n    if create_pdf:\n        # Create static images for pdf\n        export_png(layout, filename=\"correlation_table.png\")\n        display(Image(filename=\"correlation_table.png\"))\n    else:\n        show(layout)\n\n\ncreate_correlation_table(correlation_data)\n\n\n  \n\n\n\n\n\nThe correlogram plot and data table shown above contain lots of information. There is a strong positive linear association between the systolic and diastolic blood pressures for both sexes. Other specific observations by sex are:\nMale\n\nApart from blood pressure, all other linear associations are weak (0.15 &lt; |correlation| &lt; 0.5) or non-existent.\nFor a non-smoking non-diabetic, there are weak linear associations between BMI and HDL cholesterol (negative), BMI and diastolic blood pressure (positive), age and systolic blood pressure (positive), diastolic blood pressure and total cholesterol (positive), age and total cholesterol (positive) and age and coronary heart disease (positive).\nFor a smoking non-diabetic, the following weak linear associations get slightly stronger: age and coronary heart disease, age and systolic blood pressure and BMI and diastolic blood pressure.\nSome of the weak linear associations for diabetic non-smokers are similar to those for non-diabetics, such as BMI and HDL cholesterol (negative). Still, new associations appear for age and diastolic blood pressure (negative), age and total cholesterol (negative), and age and coronary heart disease (positive).\nThe weak linear association for a smoking diabetic is similar to that for a non-smoking diabetic, with the majority of changes being to the strength of the association, which mainly increases.\n\nFemale\n\nLike the males, the only strong linear association is between the systolic and diastolic blood pressures. All other linear associations are weak or non-existent.\nCompared to males, females have a stronger linear association between age and total cholesterol (positive), age and systolic blood pressure (positive), total cholesterol and systolic blood pressure (positive), and total cholesterol and HDL cholesterol (positive).\nFor a non-diabetic, smoking reduces the strength of the weak linear associations.\nFor a diabetic non-smoker, the strength of the weak linear association is lower than for a non-diabetic non-smoker.\nFor a diabetic smoker, the weak associations involving age change from positive to negative.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#scatter-plot-matrix",
    "href": "portfolio/dw/Task7/Task_7HD.html#scatter-plot-matrix",
    "title": "Data Mining",
    "section": "Scatter plot matrix",
    "text": "Scatter plot matrix\nThe scatter plot matrix is a helpful visualisation for understanding the pairwise relationships between the quantitative variables in the dataset. It can help identify any linear or non-linear correlations between the variables and potential outliers or unusual patterns in the data. A function was created that generates a scatter plot matrix for a given data frame and group by variable. The group by feature allows the data to be grouped by sex, smoking status, and diabetes status.\n\ndef create_scatter_plot_matrix(df, sel_var):\n    \"\"\"\n    Creates a scatter plot matrix for the provided data frame\n\n    This function was created with ideas from:\n     * https://docs.bokeh.org/en/3.1.1/docs/examples/topics/stats/splom.html\n     * https://datawranglingpy.gagolewski.com/chapter/310-matrix.html#\n     \n    :param df: The data frame to use for plotting\n    :param sel_var: String with the name of the selector variable\n    \"\"\"\n    # Categorical variables\n    cat_vars = [\"SEX\", \"SMOKES\", \"DIABETES\", \"CD\"]\n    cat_vars.remove(sel_var)\n    \n    # Drop the categorical columns\n    df = df[df.columns[~df.columns.isin(cat_vars)]]\n\n    all_vars = list(df.columns)\n\n    # Remove the selector variable from all_vars\n    all_vars.remove(sel_var)\n    n_var = len(all_vars)\n\n    colours = { \"SEX\": (\"#1E90FF\", \"#FF1493\"),\n        \"SMOKES\": (\"#008080\",\"#ffa500\"),\n        \"DIABETES\": (\"#000000\",\"#ff0000\"),\n        \"CD\": (\"#5e4fa2\",\"#9e0142\")}\n    units = [\"(years)\",\"(kg/m^2)\",\"(mmHg)\", \"(mmHg)\", \"(mg/dL)\", \"(mg/dL)\"]\n    tools =\"pan,wheel_zoom,box_zoom,reset\"\n\n    # Compute min/max for each variable and create DataRange1d with padding\n    x_ranges = []\n    y_ranges = []\n    for var in all_vars:\n        vmin = df[var].min()\n        vmax = df[var].max()\n        padding = 0.1 * (vmax - vmin)\n        x_ranges.append(DataRange1d(start=vmin - padding, end=vmax + padding))\n        y_ranges.append(DataRange1d(start=vmin - padding, end=vmax + padding))\n    \n    sources_1 = []\n    sources_2 = []\n    plots = []\n    pwidth = 150\n    for row_idx, y in enumerate(all_vars):\n        row = []\n        for col_idx, x in enumerate(all_vars):\n            if x == y:\n                # Dummy plot for diagonal: only label, no glyphs\n                p = figure(width=pwidth, height=pwidth, tools=tools)\n                # Hide axes and grid\n                p.xaxis.visible = False\n                p.yaxis.visible = False\n                p.grid.visible = False\n                # Add a centred text label\n                label = Label(x=50, y=60, x_units=\"screen\", y_units=\"screen\",\n                          text=y+\" \"+units[col_idx], text_align=\"center\",\n                          text_baseline=\"middle\",\n                          text_font_size=\"8pt\")\n                p.add_layout(label)\n                row.append(p)\n                sources_1.append(None)\n                sources_2.append(None)\n            else:\n                first_mask = (df[sel_var] == 0)\n                second_mask = (df[sel_var] == 1)\n                sm = ColumnDataSource(data=dict(\n                    x=df.loc[first_mask, x],\n                    y=df.loc[first_mask, y]))\n                sf = ColumnDataSource(data=dict(\n                    x=df.loc[second_mask, x],\n                    y=df.loc[second_mask, y]))\n                p = figure(\n                    width=pwidth, height=pwidth,\n                    x_axis_label=x, y_axis_label=y,\n                    tools=tools,\n                    x_range=x_ranges[col_idx], y_range=y_ranges[row_idx])\n                # Only show x-axis label on the bottom row\n                if row_idx == n_var - 1:\n                    p.xaxis.axis_label = \"\"#x\n                else:\n                    p.xaxis.axis_label = \"\"\n                # Only show y-axis label on the leftmost column\n                if col_idx == 0:\n                    p.yaxis.axis_label = \"\"#y\n                else:\n                    p.yaxis.axis_label = \"\" \n                \n                p.circle(\"x\", \"y\", source=sm, size=6, alpha=0.2,\n                         color=colours[sel_var][0])\n                p.circle(\"x\", \"y\", source=sf, size=6, alpha=0.2,\n                        color=colours[sel_var][1])\n                p.xaxis.major_label_text_font_size = \"6pt\"\n                p.yaxis.major_label_text_font_size = \"6pt\"\n                row.append(p)\n                sources_1.append(sm)\n                sources_2.append(sf)\n        plots.append(row)\n\n    # Prepare data for JS\n    data_dict = {v: df[v].values for v in all_vars}\n    cat_arr = df[sel_var].values\n\n    if sel_var == \"SMOKES\":\n        sel_opts = [\"Both\", \"Non-Smoker\", \"Smoker\"]\n    elif sel_var == \"DIABETES\":\n        sel_opts = [\"Both\", \"Non-Diabetic\", \"Diabetic\"]\n    elif sel_var == \"CD\":\n        sel_opts = [\"Both\", \"Not-Diagnosed\", \"Diagnosed\"]        \n    else:\n        sel_opts = [\"Both\", \"Male\", \"Female\"]\n\n    cat_select = Select(title=\"Category\", value=\"Both\", options=sel_opts)\n\n    callback = CustomJS(\n        args=dict(\n            sources_f=sources_1,\n            sources_s=sources_2,\n            variables=all_vars,\n            data_dict=data_dict,\n            cat_arr=cat_arr,\n            cat_select=cat_select,\n            sel_opts=sel_opts\n        ),\n        code=\"\"\"\n        const cat = cat_select.value;\n        const n = variables.length;\n        let source_idx = 0;\n        for (let row = 0; row &lt; n; row++) {\n            for (let col = 0; col &lt; n; col++) {\n                if (row === col) {\n                    source_idx += 1;\n                    continue;\n                }\n                let sm = sources_f[source_idx];\n                let sf = sources_s[source_idx];\n                if (cat === sel_opts[1]) {\n                    sm.data = {\n                        x: data_dict[variables[col]].filter((v, i) =&gt; cat_arr[i] === 0),\n                        y: data_dict[variables[row]].filter((v, i) =&gt; cat_arr[i] === 0)\n                    };\n                    sf.data = {x: [], y: []};\n                } else if (cat === sel_opts[2]) {\n                    sm.data = {x: [], y: []};\n                    sf.data = {\n                        x: data_dict[variables[col]].filter((v, i) =&gt; cat_arr[i] === 1),\n                        y: data_dict[variables[row]].filter((v, i) =&gt; cat_arr[i] === 1)\n                    };\n                } else { // Both\n                    sm.data = {\n                        x: data_dict[variables[col]].filter((v, i) =&gt; cat_arr[i] === 0),\n                        y: data_dict[variables[row]].filter((v, i) =&gt; cat_arr[i] === 0)\n                    };\n                    sf.data = {\n                        x: data_dict[variables[col]].filter((v, i) =&gt; cat_arr[i] === 1),\n                        y: data_dict[variables[row]].filter((v, i) =&gt; cat_arr[i] === 1)\n                    };\n                }\n                sm.change.emit();\n                sf.change.emit();\n                source_idx += 1;\n            }\n        }\n        \"\"\"\n    )\n    cat_select.js_on_change(\"value\", callback)\n\n    layout = column(cat_select, gridplot(plots)) \n\n    if create_pdf:\n        # Create static images for pdf\n        export_png(layout, filename=f\"{sel_var}_spm_plot.png\")\n        display(Image(filename=f\"{sel_var}_spm_plot.png\"))\n    else:\n        show(layout)\n\n\nScatter plot matrix grouped by sex\n\ncreate_scatter_plot_matrix(nhanes_df, \"SEX\")\n\n\n  \n\n\n\n\n\n\n\nScatter plot matrix grouped by smoking\n\ncreate_scatter_plot_matrix(nhanes_df, \"SMOKES\")\n\n\n  \n\n\n\n\n\n\n\nScatter plot matrix grouped by smoking\n\ncreate_scatter_plot_matrix(nhanes_df, \"DIABETES\")\n\n\n  \n\n\n\n\n\n\n\nScatter plot matrix grouped by smoking\n\ncreate_scatter_plot_matrix(nhanes_df, \"CD\")\n\n\n  \n\n\n\n\n\nThe scatter plot matrices show positive correlations between certain variables, such as age and systolic blood pressure. Systolic blood pressure and diastolic blood pressure are linearly correlated. Some variables show non-linear correlation, such as age and diastolic blood pressure (concave), age and HDL cholesterol (convex) and BMI and HDL cholesterol (L-shaped). Some of these patterns change when grouped with smoking and or diabetes. However, there does not appear to be a clear linear relationship between any of the independent variables and the target variable, coronary heart disease.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#scatter-plots",
    "href": "portfolio/dw/Task7/Task_7HD.html#scatter-plots",
    "title": "Data Mining",
    "section": "Scatter plots",
    "text": "Scatter plots\nWhilst the scatter plot matrix allows for the comparison of all quantitative variables, the plots tend to be small, and individual plots can be used to inspect specific pairs of variables. A function was created that creates scatter plots for the request pairs of variables. The plots are interactive and allow the users to filter the data based on sex and other factors (smoking or diabetes). Choosing a sex removes all data from the other sex. The plots are usually coloured by sex; however, if the user selects the colour by coronary heart disease, then the points in the plot will be coloured red for diagnosed and black for not diagnosed.\nBelow are some example plots; these were chosen to match some of the types of shapes discussed in the previous section.\n\ndef plot_scatter_by_cat(x, y):\n    \"\"\"\n    Creates an individual scatter plot using bokeh. The plot is interactive\n    and allow users to filter the data based on sex and other factors. If a\n    particular sex is selected, only data for that sex is shown.\n\n    :param x: Name of first variable as a string\n    :param y: Name of second variable as a string\n    \"\"\"\n    groups = nhanes_df.groupby([\"SEX\", \"SMOKES\", \"DIABETES\"])\n\n    good_names = {\"BPSY\": \"Systolic blood pressure\",\n                  \"BPDI\": \"Diastolic blood pressure\",\n                  \"BMI\": \"Body Mass Index\",\n                  \"CTOT\": \"Total Cholesterol\", \"CHDL\": \"HDL Cholesterol\",\n                  \"AGE\": \"Age\"}\n    \n    units = {\"BPSY\": \" (mmHg)\", \"BPDI\": \" (mmHg)\", \"BMI\": \" (kg/m^2)\",\n             \"CTOT\": \" (mg/dL)\", \"CHDL\": \" (mg/dL)\", \"AGE\": \" (years)\"}\n\n    # Dictionary to hold all data sources\n    data_sources = {}\n    \n    # Generate data for all combinations\n    for gender in [\"male\", \"female\"]:\n        # male=0, female =1\n        g_factor = 0 if gender == \"male\" else 1\n    \n        for smoker in [False, True]:\n            # no = 0, yes =1\n            s_factor = 0 if not smoker else 1\n        \n            for diabetic in [False, True]:\n                # no = 0, yes = 1\n                d_factor = 0 if not diabetic else 1\n            \n                # Create a key for this combination\n                key =f'{gender}_{\"smoker\" if smoker else \"nonsmoker\"}_{\"diabetic\" if diabetic else \"nondiabetic\"}'\n\n                df = groups.get_group((g_factor, s_factor, d_factor))\n                df = df[df.columns[~df.columns.isin([\"SMOKES\", \"DIABETES\"])]]\n\n                data_sources[key] = ColumnDataSource(\n                    pd.DataFrame({\n                        \"var_1\": df[x],\n                        \"var_2\": df[y],\n                        \"group\": df[\"SEX\"],\n                        \"colour\": [\"#FF1493\" if gender else \"#1E90FF\" for gender in df[\"SEX\"]],\n                        \"cd_colour\": [\"red\" if x else \"black\" for x in df[\"CD\"]]\n                        }))\n    # Create the figure\n    p = figure(title=f\"Scatter plot of {good_names[y]} versus {good_names[x]}\",\n            x_axis_label=good_names[x] + units[x],\n            y_axis_label=good_names[y] + units[y],\n            tools=\"box_zoom,wheel_zoom,pan,reset\", width=800, height=800)\n\n    # Create a renderers dictionary to store all line renderers\n    renderers = []\n\n    # Create a plot each category\n    for gender in [\"male\", \"female\"]:\n        colour = \"#1E90FF\" if gender==\"male\" else \"#FF1493\"\n        for smoker in [False, True]:\n            for diabetic in [False, True]:\n                if smoker:\n                    if diabetic:\n                        marker=\"triangle\"\n                    else:\n                        marker=\"square\"\n                else:\n                    if diabetic:\n                        marker=\"inverted_triangle\"\n                    else:\n                        marker=\"circle\"\n                    \n                renderer_key = f'{gender}_{\"smoker\" if smoker else \"nonsmoker\"}_{\"diabetic\" if diabetic else \"nondiabetic\"}'\n                \n                renderers.append(p.scatter(\n                    x=\"var_1\", y=\"var_2\",\n                    source=data_sources[renderer_key],\n                    color=\"colour\",\n                    marker=marker,\n                    visible=True))\n\n    # Define tooltips\n    tooltips = [(\"(x, y)\", \"(@var_1, @var_2)\")]\n\n    # Add HoverTool\n    hover = HoverTool(tooltips=tooltips)\n    p.add_tools(hover)\n\n    # Checkbox for data sources (applies to both sexes)\n    checkbox_sources = CheckboxGroup(\n        labels=[\"Non-Smoker/Non-Diabetic\", \"Non-Smoker/Diabetic\", \"Smoker/Non-Diabetic\", \"Smoker/Diabetic\"],\n        active=[0,1,2,3])\n    \n    # Checkbox for sexes\n    checkbox_sexes = CheckboxGroup(labels=[\"Male\", \"Female\"], active=[0,1])\n\n    # Check box for colour by coronary heart disease\n    checkbox_colourby = CheckboxGroup(labels=[\"Color by Coronary heart disease\"],\n                                      active=[])\n\n    # Add label under check box to tell user what the colour red means\n    label = Div(text='Diagnosed &lt;span style=\"color:red;\"&gt;red&lt;/span&gt; symbols.',\n                styles={'font-size': '12px', 'color': 'black'})\n    \n    # CustomJS callback\n    callback = CustomJS(args=dict(\n        renderers=renderers,\n        sources_cb=checkbox_sources,\n        sexes_cb=checkbox_sexes,\n        colourby_cb=checkbox_colourby,\n        data_sources=data_sources\n    ), code=\"\"\"\n        // 0-3: male sources, 4-7: female sources\n        for (let i = 0; i &lt; 4; i++) {\n            // Male\n            renderers[i].visible = sources_cb.active.includes(i) && sexes_cb.active.includes(0);\n            // Female\n            renderers[i+4].visible = sources_cb.active.includes(i) && sexes_cb.active.includes(1);\n        }\n\n        // Update colours in all data sources\n        for (const key in data_sources) {\n            let source = data_sources[key];\n            let data = source.data;\n            let group = data['group'];\n            let colour = data['colour'];\n            let cd_colour = data['cd_colour'];\n            if (colourby_cb.active.includes(0)) {\n                for (let i = 0; i &lt; group.length; i++) {\n                    colour[i] = cd_colour[i];\n                }\n            } else {\n            for (let i = 0; i &lt; group.length; i++) {\n                    colour[i] = (group[i] === 0) ? \"#1E90FF\" : \"#FF1493\";                    \n                }\n            }\n            source.change.emit();\n        }\n    \"\"\")\n    checkbox_sources.js_on_change(\"active\", callback)\n    checkbox_sexes.js_on_change(\"active\", callback)\n    checkbox_colourby.js_on_change(\"active\", callback)\n\n    layout = row(p, column(\n                    checkbox_sexes,\n                    checkbox_sources,\n                    checkbox_colourby,\n                    label\n                )\n            )\n\n    if create_pdf:\n        # Create static images for pdf\n        export_png(layout, filename=f\"{x}_v_{y}_s_plot.png\")\n        display(Image(filename=f\"{x}_v_{y}_s_plot.png\"))\n    else:\n        show(layout)\n\nThe figure below shows the scatter plot of diastolic versus systolic blood pressure. While there is some scatter, the plot does exhibit a positive relationship between the variables.\n\nplot_scatter_by_cat(x = \"BPSY\", y = \"BPDI\")\n\n\n  \n\n\n\n\n\nThe plot below shows the scatter plot of systolic blood pressure versus age, an example of a weak linear association, especially for female data. The scatter in systolic blood pressure measurement increases with age.\n\nplot_scatter_by_cat(x = \"AGE\", y = \"BPSY\")\n\n\n  \n\n\n\n\n\nThe plot below shows the scatter plot of HDL cholesterol versus age. If you isolate the male data, you will see the data has a convex trend, i.e. HDL levels are higher at low and high ages, with a minimum in the middle age.\n\nplot_scatter_by_cat(x = \"AGE\", y = \"CHDL\")\n\n\n  \n\n\n\n\n\nThe plot below shows the scatter plot of diastolic blood pressure versus age. Again, isolating the male data shows a concave trend, i.e., diastolic blood pressure is lower in young and old people and higher in the middle age.\n\nplot_scatter_by_cat(x = \"AGE\", y = \"BPDI\")\n\n\n  \n\n\n\n\n\nThe last plot below shows the scatter plot of BMI versus HDL cholesterol, which is an example of what I describe as an L-shaped trend. If you colour this plot to coronary heart disease, you will notice that many of the points for diagnosed coronary heart disease occur at the lower values of HDL cholesterol.\n\nplot_scatter_by_cat(x = \"CHDL\", y = \"BMI\")",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#outliers",
    "href": "portfolio/dw/Task7/Task_7HD.html#outliers",
    "title": "Data Mining",
    "section": "Outliers",
    "text": "Outliers\nOutliers have already been mentioned when discussing the distributions of the quantitative variables. Here, I produce a boxplot using bokeh to allow visualisation of the outliers.\n\ndef create_boxplot(df):\n    \"\"\"\n    Creates a boxplot using bokeh for the quantitative variables in the\n    provided data frame.\n\n    Ideas for this function came from the boxplot example in the bokeh\n    documentation\n    https://docs.bokeh.org/en/3.0.2/docs/examples/topics/stats/boxplot.html\n\n    :param df: Input data frame\n    \"\"\"\n    # Categorical variables\n    cat_vars = [\"SEX\", \"SMOKES\", \"DIABETES\", \"CD\"]\n    \n    # Drop the categorical columns\n    df = df[df.columns[~df.columns.isin(cat_vars)]]\n\n    # Get list of remaining columns\n    cols = list(df.columns)\n\n    colours = Category10[len(cols)]\n\n    # Calcualte the statistics\n    q1 = df[cols].quantile(0.25)\n    q2 = df[cols].quantile(0.50)\n    q3 = df[cols].quantile(0.75)\n    iqr = q3 - q1\n    upper = q3 + 1.5 * iqr\n    lower = q1 - 1.5 * iqr\n\n    # Cap lower whisker at zero\n    lower = lower.clip(lower=0)\n\n    # Create source\n    source = ColumnDataSource(data=dict(\n        variables=cols,\n        q1=q1.values,\n        q2=q2.values,\n        q3=q3.values,\n        upper=upper.values,\n        lower=lower.values,\n        colour=colours\n    ))\n\n    # Prepare outliers\n    outx = []\n    outy = []\n    for i, col in enumerate(cols):\n        col_outliers = df[(df[col] &gt; upper[col]) | (df[col] &lt; lower[col])][col]\n        outx.extend([col] * col_outliers.count())\n        outy.extend(col_outliers.values)\n\n    outlier_source = ColumnDataSource(data={\n        'variable': outx,\n        'value': outy\n    })\n    \n    p = figure(x_range=cols, background_fill_color=\"#efefef\",\n           width=800, height=800, \n           title=\"Boxplot of the quantitative variables in the data frame\",\n           tools=\"box_zoom,wheel_zoom,pan,reset\"\n          )\n\n    # Add boxes\n    box_1 = p.vbar(x='variables', width=0.7, bottom='q2', top='q3',\n                    source=source, fill_color='colour', line_color=\"black\")\n    box_2 = p.vbar(x='variables', width=0.7, bottom='q1', top='q2',\n                    source=source, fill_color='colour', line_color=\"black\")\n\n    # Add whiskers\n    p.add_layout(Whisker(source=source, base=\"variables\", upper=\"upper\",\n                        lower=\"lower\"))\n    # Add outliers\n    outlier_renderer = p.circle(x='variable', y='value', source=outlier_source,\n                                size=6, color=\"black\", fill_alpha=0.6)\n\n    hover_box = HoverTool(\n        tooltips=[\n            (\"Variable\", \"@variables\"),\n            (\"Q1\", \"@q1\"),\n            (\"Median\", \"@q2\"),\n            (\"Q3\", \"@q3\"),\n            (\"Lower Whisker\", \"@lower\"),\n            (\"Upper Whisker\", \"@upper\"),\n        ],\n        renderers=[box_1, box_2]\n    )\n    p.add_tools(hover_box)\n\n    hover_outlier = HoverTool(\n        tooltips=[\n            (\"Variable\", \"@variable\"),\n            (\"Outlier Value\", \"@value\"),\n        ],\n        renderers=[outlier_renderer]\n    )\n    p.add_tools(hover_outlier)\n\n    \n    if create_pdf:\n        # Create static images for pdf\n        export_png(p, filename=\"box_plot.png\")\n        display(Image(filename=\"box_plot.png\"))\n    else:\n        show(p)\n\nThe boxplot below shows age has no outliers, BMI has outliers above the top whisker, and the systolic and diastolic blood pressures have outliers above and below the whiskers. It has already been noted that the lower outliers had values that seemed too low for a person and, therefore, need further investigation. Total and HDL cholesterol have lower and higher outliers. A significant number of higher outliers is observed for these variables. Usually, the next step would be to consider treatments of the outliers, either trimming the data or transforming the data to remove outliers. In this analysis, we acknowledge the outlier’s existence but apply no treatment to remove or minimise them.\n\ncreate_boxplot(nhanes_df)",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#predicting-the-sex-of-individuals-based-on-health-data",
    "href": "portfolio/dw/Task7/Task_7HD.html#predicting-the-sex-of-individuals-based-on-health-data",
    "title": "Data Mining",
    "section": "Predicting the sex of individuals based on health data",
    "text": "Predicting the sex of individuals based on health data\nThis section uses the health data to build a classifying model to see if the person’s sex can be predicted from that data.\n\nsex_mapping = {0: 'Male', 1: 'Female'}\nsex_value_counts = nhanes_df[\"SEX\"].value_counts(normalize=True)\nsex_value_counts.index = sex_value_counts.index.map(sex_mapping)\npercentage_counts_sex = sex_value_counts.mul(100).round(2).astype(str) + \"%\"\nprint(\"\\nValue counts as percentages for the column:\")\nprint(percentage_counts_sex)\n\n\nValue counts as percentages for the column:\nSEX\nFemale    51.08%\nMale      48.92%\nName: proportion, dtype: object\n\n\nBefore building the model, I checked the proportion of females in the sample. From the above information, you can see females make up 51% of the sample, so this is a balanced classification problem.\n\nknn_1 = KNClassifier(nhanes_df, pred_var=\"SEX\", labels=[\"Male\", \"Female\"])\n\nPrediction  variable: SEX\nNumber of training points: 5038\nNumber of testing points: 1260\nAccuracy            66.27 %\nPrecision           67.50 %\nRecall              63.56 %\nF1 measure          65.48 %\n\n\n\n  \n\n\n\n\n\nThe model performance on the test data is shown above; 66% of the time, it accurately predicts the sex of the individual is female. This is higher than 51%, which is the outcome if it labelled everyone as female. The precision shows that 67.5% of people labelled as female are indeed female. The recall indicates that 63.5% of females will be correctly labelled as female. The harmonic average of precision and recall is 65.5%. My conclusion is that the classifier model is average.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#predicting-if-someone-is-diabetic",
    "href": "portfolio/dw/Task7/Task_7HD.html#predicting-if-someone-is-diabetic",
    "title": "Data Mining",
    "section": "Predicting if someone is diabetic",
    "text": "Predicting if someone is diabetic\n\ndb_mapping = {0: 'No', 1: 'Yes'}\ndb_value_counts = nhanes_df[\"DIABETES\"].value_counts(normalize=True)\ndb_value_counts.index = db_value_counts.index.map(db_mapping)\npercentage_counts_db = db_value_counts.mul(100).round(2).astype(str) + \"%\"\nprint(\"\\nValue counts as percentages for the column:\")\nprint(percentage_counts_db)\n\n\nValue counts as percentages for the column:\nDIABETES\nNo     85.5%\nYes    14.5%\nName: proportion, dtype: object\n\n\nOnly 14.5% of the sample is diabetic, which is a very imbalanced classification problem.\n\nknn_2 = KNClassifier(nhanes_df, pred_var=\"DIABETES\")\n\nPrediction  variable: DIABETES\nNumber of training points: 5038\nNumber of testing points: 1260\nAccuracy            85.24 %\nPrecision           47.47 %\nRecall              25.97 %\nF1 measure          33.57 %\n\n\n\n  \n\n\n\n\n\nThe model performance on the test data is shown above; 85% of the time, it accurately predicts the individual’s diabetic status (non-diabetic or diabetic). Given that 85.5% of the people in the sample are non-diabetic, the model performs no better than assuming everyone is non-diabetic. The precision shows that 47.5% of people labelled as diabetic indeed have diabetes. Whilst the recall indicates that 26% of diabetics will be correctly labelled as diabetic. My conclusion is that the classifier model for diabetes prediction is terrible. The underrepresentation of diabetic people in the data set is probably the cause of the poor model performance.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#predicting-if-someone-has-coronary-heart-disease",
    "href": "portfolio/dw/Task7/Task_7HD.html#predicting-if-someone-has-coronary-heart-disease",
    "title": "Data Mining",
    "section": "Predicting if someone has coronary heart disease",
    "text": "Predicting if someone has coronary heart disease\n\ncd_mapping = {0: 'No', 1: 'Yes'}\ncd_value_counts = nhanes_df[\"CD\"].value_counts(normalize=True)\ncd_value_counts.index = cd_value_counts.index.map(db_mapping)\npercentage_counts_cd = cd_value_counts.mul(100).round(2).astype(str) + \"%\"\nprint(\"\\nValue counts as percentages for the column:\")\nprint(percentage_counts_cd)\n\n\nValue counts as percentages for the column:\nCD\nNo     96.71%\nYes     3.29%\nName: proportion, dtype: object\n\n\nOnly 3.3% of the sample has been diagnosed with coronary heart disease. Based on experience with diabetes prediction, the model is unlikely to be useful.\n\nknn_3 = KNClassifier(nhanes_df, pred_var=\"CD\")\n\nPrediction  variable: CD\nNumber of training points: 5038\nNumber of testing points: 1260\nAccuracy            96.03 %\nPrecision           14.29 %\nRecall              2.22 %\nF1 measure          3.85 %\n\n\n\n  \n\n\n\n\n\nAs expected, the model’s performance is very poor, with a precision of 14.3% and a recall of only 2.2%. From the confusion matrix, you can see that 44 people who had coronary heart disease were predicted not to have the disease and, therefore, would go untreated. It is hypothesised that the very large underrepresentation of people with coronary heart disease in the sample is the cause for such a poor-performing model.\nTo test this hypothesis, I filtered the data to only include people over the age of 45 (the scatter lots showed that very few young people had coronary heart disease) and those who were smokers. This increased the proportion in the sample with a diagnosis of coronary heart disease to 7.4%.\n\n# Only people 45 and over\nnhanes_df_filt = nhanes_df[(nhanes_df[\"AGE\"] &gt;= 45)]\n\n# Group by smoking\ngroups = nhanes_df_filt.groupby([\"SMOKES\"])\n\n# Choose only the smokers\ndf_2 = groups.get_group((1,))\ndf_2 = df_2[df_2.columns[~df_2.columns.isin([\"SMOKES\"])]]\n\ncd_mapping = {0: 'No', 1: 'Yes'}\ncd_value_counts = df_2[\"CD\"].value_counts(normalize=True)\ncd_value_counts.index = cd_value_counts.index.map(db_mapping)\npercentage_counts_cd = cd_value_counts.mul(100).round(2).astype(str) + \"%\"\n\nprint(\"\\nValue counts as percentages for the column:\")\nprint(percentage_counts_cd)\n\nknn_4 = KNClassifier(df_2, pred_var=\"CD\")\n\n\nValue counts as percentages for the column:\nCD\nNo     92.85%\nYes     7.15%\nName: proportion, dtype: object\nPrediction  variable: CD\nNumber of training points: 1297\nNumber of testing points: 325\nAccuracy            92.31 %\nPrecision           16.67 %\nRecall              4.76 %\nF1 measure          7.41 %\n\n\n\n  \n\n\n\n\n\nFiltering the data to only include people who are more likely to have been diagnosed with coronary heart disease improved the model’s precision (16.7%) and recall (4.8%). This confirms that having a more even representation of people with coronary heart disease in the sample would help improve the performance, and therefore, model’s usefullness.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#key-findings-from-exploratory-data-analysis",
    "href": "portfolio/dw/Task7/Task_7HD.html#key-findings-from-exploratory-data-analysis",
    "title": "Data Mining",
    "section": "Key Findings from Exploratory Data Analysis:",
    "text": "Key Findings from Exploratory Data Analysis:\n\nAge distribution is uniform across sexes.\nBMI shows a right-skewed distribution with more severely obese females than males.\nBlood pressure measurements reveal some potential outliers requiring investigation.\nA strong positive correlation exists between systolic and diastolic blood pressure.\nMost other variable correlations are weak.\nClear patterns emerge in smoking (more male smokers), diabetes (balanced by sex), and coronary heart disease (more female diagnoses).",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#machine-learning-results",
    "href": "portfolio/dw/Task7/Task_7HD.html#machine-learning-results",
    "title": "Data Mining",
    "section": "Machine Learning Results:",
    "text": "Machine Learning Results:\nThree K-Nearest Neighbors classification models were tested:\n\nPredicting Sex: 66.27% accuracy - considered average performance.\nPredicting Diabetes: 85.24% accuracy but poor precision (47.5%) and recall (26%) due to class imbalance (only 14.5% diabetic).\nPredicting Coronary Heart Disease: Abysmal performance (96% accuracy but only 2.2% recall) due to severe class imbalance (only 3.3% with disease).",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#main-conclusions",
    "href": "portfolio/dw/Task7/Task_7HD.html#main-conclusions",
    "title": "Data Mining",
    "section": "Main Conclusions:",
    "text": "Main Conclusions:\nThe study demonstrates that predicting cardiovascular disease from basic health metrics is challenging, primarily due to the dataset’s class imbalance. The severe underrepresentation of people with coronary heart disease makes reliable prediction difficult with standard machine-learning approaches.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task7/Task_7HD.html#future-improvements",
    "href": "portfolio/dw/Task7/Task_7HD.html#future-improvements",
    "title": "Data Mining",
    "section": "Future Improvements:",
    "text": "Future Improvements:\nThe analysis suggests using more balanced datasets, investigating outlier treatments, exploring additional risk factors like physical activity, and testing other machine learning models to improve prediction accuracy.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Mining"
    ]
  },
  {
    "objectID": "portfolio/dw/Task4/Task_4P.html",
    "href": "portfolio/dw/Task4/Task_4P.html",
    "title": "Working with pandas Data Frames (Heterogeneous Data)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with pandas Data Frames (Heterogeneous Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task4/Task_4P.html#introduction",
    "href": "portfolio/dw/Task4/Task_4P.html#introduction",
    "title": "Working with pandas Data Frames (Heterogeneous Data)",
    "section": "Introduction",
    "text": "Introduction\nThis task requires working with pandas data frames and Python to conduct a data analysis exercise involving meteorological data for three airports in New York. The meteorological data is loaded into a pandas data frame from a text file. Pandas operations are used to filter and aggregate the data to enable plotting of the mean monthly and daily wind speeds for different airports. Additional analysis is performed for JFK airport to handle missing data and plot the daily average temperature throughout 2013.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with pandas Data Frames (Heterogeneous Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task4/Task_4P.html#data-input",
    "href": "portfolio/dw/Task4/Task_4P.html#data-input",
    "title": "Working with pandas Data Frames (Heterogeneous Data)",
    "section": "Data input",
    "text": "Data input\nA provided data file nycflights13_weather.csv.gz that contained hourly meteorological data for three airports in New York: EWR, JFK and LGA was used for this analysis. The data contained in the file is:\n\norigin – weather station: LGA, JFK, or EWR,\nyear, month, day, hour – time of recording,\ntemp, dewp – temperature and dew point in degrees Fahrenheit,\nhumid – relative humidity,\nwind_dir, wind_speed, wind_gust – wind direction (in degrees), speed and gust speed (in mph),\nprecip – precipitation, in inches,\npressure – sea level pressure in millibars,\nvisib – visibility in miles,\ntime_hour – date and hour (based on the year, month, day, hour fields)\n\nThis file is loaded into a pandas data frame using the pd.read_csv function. Any lines containing ‘#’ are ignored when loading. it.\n\n# Load data file in dataframe\nnyc_ap_weather = pd.read_csv(\"nycflights13_weather.csv.gz\",comment=\"#\")\n\n\nCovert all columns so they use SI units\nFor this step we use the pandas apply method a helper function fahrenheit_to_celsius and lambda functions to do the appropriate conversion for each variable.\n\n# Helper function for temperature conversion\ndef fahrenheit_to_celsius(fahrenheit):\n    \"\"\"\n    This function takes a Fahrenheit value and returns the Celsius equivalent.\n\n    The formula is C = (F-32)* 5/9\n\n    :param fahrenheit: Input temperature in Fahrenheit.\n    :return: Temperature in Celsius.\n    \"\"\"\n    return (fahrenheit - 32) * 5/9\n\n\n# Convert \"temp\" and \"dewp\" from Fahrenheit to Celsius\nnyc_ap_weather[\"temp\"] = nyc_ap_weather[\"temp\"].apply(fahrenheit_to_celsius)\nnyc_ap_weather[\"dewp\"] = nyc_ap_weather[\"dewp\"].apply(fahrenheit_to_celsius)\n\n# Convert \"precip\" to millimetres\nnyc_ap_weather[\"precip\"] = nyc_ap_weather[\"precip\"].apply(lambda x:x*25.4)\n\n# Convert \"visib\" to metres\nnyc_ap_weather[\"visib\"] = nyc_ap_weather[\"visib\"].apply(lambda x:x*1609.34)\n\n# Convert \"wind_speed\" and \"wind_gust\" to m/s\nnyc_ap_weather[\"wind_speed\"] = nyc_ap_weather[\"wind_speed\"].apply(lambda x:x*0.44704)\nnyc_ap_weather[\"wind_gust\"] = nyc_ap_weather[\"wind_gust\"].apply(lambda x:x*0.44704)\n\n\n\nCompute monthly mean wind speeds for all three airports\nBefore calculating the mean for each airport, we check that the recorded hourly wind speed is not above the highest recorded Hurricane wind speed (96 m/s). Values larger than this are likely to be erroneous and are replaced with NaN so they don’t impact the calculation of the mean.\n\n# Check for any wind speeds above the highest recorded Hurricane (345 km/h | 96 m/s) and replace any found with NaN \nnyc_ap_weather.loc[(nyc_ap_weather[\"wind_speed\"] &gt;= 96), \"wind_speed\"] = np.nan\n\n# Calculate the monthly average wind_speed at all three airports\nmonthly_ave_wind_speed = nyc_ap_weather.groupby([\"origin\", \"year\", \"month\"])[[\"wind_speed\"]].mean(numeric_only=True).reset_index()\n\n\n\nPlot of the monthly mean wind speeds for EWR, JFK and LGA\nNow that the monthly mean speed has been calculated for each airport, we can plot these on the same figure for visual comparison.\n\ndef plot_monthly_average(months, data, origin, colour, variable=\"wind_speed\"):\n    # Create plot of average wind_speed versus date\n    plt.plot(months, data[data.origin == origin][variable], label=origin, color=colour)\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Monthly average wind speed [m/s]\")\n\n# Create an array of dates, required for the x-axis\nmonth_dates = np.arange(\"2013-01-01\", \"2014-01-01\", dtype=\"datetime64[M]\")\n\n# Set figure size\nplt.figure(figsize=(11, 6))\n\n# Plot LGA\nplot_monthly_average(month_dates, monthly_ave_wind_speed, colour=\"tab:blue\", origin=\"LGA\")\n# Plot EWR\nplot_monthly_average(month_dates, monthly_ave_wind_speed, colour=\"tab:orange\", origin=\"EWR\")\n# Plot JFK\nplot_monthly_average(month_dates, monthly_ave_wind_speed, colour=\"tab:green\", origin=\"JFK\")\n\nplt.title(\"Monthly mean wind speeds for NYC airports during 2013\")\n# Add legend\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe monthly average wind speed for each airport follows a seasonal pattern with higher average wind speeds in the cooler months and lower monthly average wind speeds in the warmer months. JFK has the highest monthly wind speeds throughout the year, and EWR has the lowest.\n\n\nCompute daily mean wind speed for LGA airport\nWe filter the data frame on origin == “LGA”, then group by “year”, “month”, and “day”, select only the “wind_speed” column and call the mean function.\n\n# Calculate the daily average wind_speed at LGA\ndaily_ave_wind_speed = nyc_ap_weather[nyc_ap_weather.origin==\"LGA\"].groupby([\"year\",\"month\",\"day\"])[[\"wind_speed\"]].mean(numeric_only=True).reset_index()\n\n\n\nPlot of daily mean wind speeds at LGA\n\n# Create an array of dates, required for the x-axis\ndates = np.arange(\"2013-01-01\", \"2013-12-31\", dtype=\"datetime64[D]\")\n\n# Set figure size\nplt.figure(figsize=(11, 6))\n\n# Create line plot of daily average wind_speed\nplt.plot(dates, daily_ave_wind_speed[\"wind_speed\"], color=\"black\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Daily average wind speed [m/s] at LGA\")\nplt.title(\"Daily mean wind speed for LGA during 2013\")\nplt.show()\n\n\n\n\n\n\n\n\nThe daily average wind speed at LGA shows a higher daily variation during the cooler months and a lower variation during the warmer months.\n\n\nThe ten windiest days at LGA\nFinding the 10 windiest days can easily be done using the pandas nlargest function.\n\n# Use the pandas nlargest function to get the 10 windiest days at LGA\nwindiest_days = daily_ave_wind_speed.nlargest(10, \"wind_speed\").reset_index(drop=True)\nprint(\"##             wind_speed (m/s)\")\nprint(\"## date\")\nfor index, row in windiest_days.iterrows():\n    print(f'## {row[\"year\"]:4g}-{row[\"month\"]:02g}-{row[\"day\"]:02g}       {round(row[\"wind_speed\"], 2)}')\n\n##             wind_speed (m/s)\n## date\n## 2013-11-24       11.32\n## 2013-01-31       10.72\n## 2013-02-17       10.01\n## 2013-02-21       9.19\n## 2013-02-18       9.17\n## 2013-03-14       9.11\n## 2013-11-28       8.94\n## 2013-05-26       8.85\n## 2013-05-25       8.77\n## 2013-02-20       8.66\n\n\n\n\nAll the missing temperature readings for JFK\nWe are interested in finding all the missing temperature readings at JFK. These include those where the reading is NaN and those where the data is omitted from the data.\n\n# Get only JFK data\njfk_weather = nyc_ap_weather[nyc_ap_weather.origin==\"JFK\"]\n\n# Get all the rows where the temperature is NaN\nnull_rows = jfk_weather.loc[jfk_weather[\"temp\"].isna()]\n\n# Create a fixed frequency DatetimeIndex\ndate_range = pd.date_range(\"2013-01-01 00:00\", \"2013-12-31 23:00\", freq=\"H\")\n\n# Create dummy data frame to apply the DateTimeIndex to\ndf = pd.DataFrame(np.ones((date_range.shape[0], 1)))\ndf.index = date_range  # set index\n\n# Check for missing datetime index values based on reference index (with all values)\n# The to_datetime converts the year, month, day, hour columns into a datetime object\nmissing_dates = df.index[~df.index.isin(pd.to_datetime(jfk_weather.loc[:,[\"year\", \"month\", \"day\", \"hour\"]]))]\n\n# Print the list of dates with missing data\nprint(\"# Dates with missing date for JFK\\n\")\n\n# Dates where the temperature is Nan\nprint(\"## Dates were the temperature is NaN\")\nprint('## Year  Month  Day  Hour')\n\nif len(null_rows):\n    for idx, row in null_rows.iterrows():\n        print(f'## {row[\"year\"]:04g}  {row[\"month\"]:02g}     {row[\"day\"]:02g}   {row[\"hour\"]:02g}')\nelse:\n    print(\"## No rows with temp = NaN\")\n\nprint()    \nprint(\"## Dates were the temperature is NaN\")\nprint(\"## Year  Month  Day  Hour\")\nfor row in missing_dates:\n    print(f'## {row.year:04g}  {row.month:02g}     {row.day:02g}   {row.hour:02g}')\n\n# Dates with missing date for JFK\n\n## Dates were the temperature is NaN\n## Year  Month  Day  Hour\n## No rows with temp = NaN\n\n## Dates were the temperature is NaN\n## Year  Month  Day  Hour\n## 2013  01     01   05\n## 2013  02     21   05\n## 2013  03     05   06\n## 2013  03     31   01\n## 2013  04     03   00\n## 2013  08     13   04\n## 2013  08     16   04\n## 2013  08     19   21\n## 2013  08     22   22\n## 2013  08     23   00\n## 2013  08     23   01\n## 2013  10     26   00\n## 2013  10     26   01\n## 2013  10     26   02\n## 2013  10     26   03\n## 2013  10     26   04\n## 2013  10     27   01\n## 2013  11     01   07\n## 2013  11     01   08\n## 2013  11     03   00\n## 2013  11     03   01\n## 2013  11     03   02\n## 2013  11     03   03\n## 2013  11     03   04\n## 2013  11     04   15\n## 2013  12     31   00\n## 2013  12     31   01\n## 2013  12     31   02\n## 2013  12     31   03\n## 2013  12     31   04\n## 2013  12     31   05\n## 2013  12     31   06\n## 2013  12     31   07\n## 2013  12     31   08\n## 2013  12     31   09\n## 2013  12     31   10\n## 2013  12     31   11\n## 2013  12     31   12\n## 2013  12     31   13\n## 2013  12     31   14\n## 2013  12     31   15\n## 2013  12     31   16\n## 2013  12     31   17\n## 2013  12     31   18\n## 2013  12     31   19\n## 2013  12     31   20\n## 2013  12     31   21\n## 2013  12     31   22\n## 2013  12     31   23\n\n\nC:\\Users\\darrin\\AppData\\Local\\Temp\\ipykernel_74728\\315129500.py:8: FutureWarning:\n\n'H' is deprecated and will be removed in a future version, please use 'h' instead.\n\n\n\nAll the data for December 31st 2013 was omitted from the JFK data.\n\n\nAdd the missing temperature records for JFK to the JFK dataset\nNow that we have identified which months, days, and hours are missing data, we can insert NaNs into the dataset so that data is available for each month, day, and hour within 2013. This will allow us to consider replacing or imputing the missing value.\n\n# Data frame for missing dates\ndf_missing = pd.DataFrame(missing_dates, columns=[\"date\"])\n\n# Extract the year, month, day, and hour components\ndf_missing[\"origin\"]= \"JFK\"\ndf_missing[\"year\"] = df_missing[\"date\"].dt.year\ndf_missing[\"month\"] = df_missing[\"date\"].dt.month\ndf_missing[\"day\"] = df_missing[\"date\"].dt.day\ndf_missing[\"hour\"] = df_missing[\"date\"].dt.hour\n\n# Drop the date column\ndf_missing = df_missing.drop(columns=[\"date\"], axis=1)\n\n# Add the missing data dataframe to the JFK weather dataframe\njfk_weather = pd.concat([jfk_weather, df_missing], ignore_index=True)\n\n# Sort the data in ascending date and time order\njfk_weather.sort_values([\"year\", \"month\", \"day\", \"hour\"], ascending=[True, True, True, True], inplace=True)\n\n# Reset the index after sort\njfk_weather.reset_index(inplace=True,drop=True)\n\n\n\nCompute daily average temperatures, linearly interpolating for missing data\nTo allow comparison between the missing value-omitted and linearly interpolated cases for the daily average temperature at JFK we need to create linearly interpolated temperatures from the raw data. The linear interpolation is performed using the pandas function interpolate with the method=linear option. The resulting Series is then inserted as a column back into the original data frame. The daily average temperature is calculated by applying the mean aggregation function to the grouped by data.\n\n# Perform linear interpolation for the missing data and insert the resulting Series as a column into the jfk_weather data frame\njfk_weather.insert(6, \"temp_interpolate\", jfk_weather.loc[:, \"temp\"].interpolate(method=\"linear\"))\n\n# Create an array of dates, required for the x-axis\ndates = np.arange(\"2013-01-01\", \"2014-01-01\", dtype=\"datetime64[D]\")\n\n# Calculate the daily average wind_speed at LGA\ndaily_ave_temp = jfk_weather.groupby([\"year\", \"month\", \"day\"])[[\"temp\"]].mean(numeric_only=True).reset_index()\ndaily_ave_temp_interp = jfk_weather.groupby([\"year\", \"month\", \"day\"])[[\"temp_interpolate\"]].mean(numeric_only=True).reset_index()\n\n\n\nPlot the daily average temperatures comparing the missing value-omitted versus linearly interpolated cases.\nNow that we have the daily average temperatures for the value-omitted and linearly interpolated cases, we can create a plot to allow visual comparison.\n\n# Set figure size\nplt.figure(figsize=(11, 6))\n\n# Create a plot of daily average temperature\nplt.plot(dates, daily_ave_temp[\"temp\"], color=\"red\",label=\"value-omitted\")\nplt.plot(dates, daily_ave_temp_interp[\"temp_interpolate\"], linestyle=\"--\", color=\"green\",label=\"linearly interpolated\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Daily average temperature [ $^\\circ$C] at JFK\")\nplt.title(\"Daily average temperature for JFK during 2013\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe only noticeable visual difference between the value-omitted and linearly interpolated daily average temperatures is on December 31st, where there is no data in the value-omitted, but data for the linearly interpolated.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with pandas Data Frames (Heterogeneous Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task4/Task_4P.html#summary",
    "href": "portfolio/dw/Task4/Task_4P.html#summary",
    "title": "Working with pandas Data Frames (Heterogeneous Data)",
    "section": "Summary",
    "text": "Summary\nThis Jupyter Notebook demonstrates the use of pandas data frames to analyse time series meteorological data quantitatively using descriptive statistics and visually using line plots.\nPossible extensions to the data analysis include: - Calculating the daily average wind speed at EWR and JFK for comparison with LGA. - Calculate a moving average and plot it with the daily average.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with pandas Data Frames (Heterogeneous Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task2/Task_2P.html",
    "href": "portfolio/dw/Task2/Task_2P.html",
    "title": "Working with numpy vectors",
    "section": "",
    "text": "import calendar\nimport matplotlib.pyplot as plt\nimport numpy as np",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy vectors"
    ]
  },
  {
    "objectID": "portfolio/dw/Task2/Task_2P.html#introduction",
    "href": "portfolio/dw/Task2/Task_2P.html#introduction",
    "title": "Working with numpy vectors",
    "section": "Introduction",
    "text": "Introduction\nThis task requires working with numpy vectors and Python to conduct a data analysis exercise involving the price of Bitcoin in US dollars (BTC-USD) for 2023. The BTC-USD data is loaded into a numpy vector from a text file. Numpy vector operations are used to calculate some descriptive statistics for the third financial quarter. The Bitcoin price in US dollars is plotted using the Matplitlib library, with a function defined to allow any financial quarter or year to be plotted using the same lines of code. The daily Bitcoin price variation is investigated through a box plot and analysis of outliers.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy vectors"
    ]
  },
  {
    "objectID": "portfolio/dw/Task2/Task_2P.html#data-input",
    "href": "portfolio/dw/Task2/Task_2P.html#data-input",
    "title": "Working with numpy vectors",
    "section": "Data input",
    "text": "Data input\nThe historical data for BTC-USD is available on the Yahoo finance website: https://finance.yahoo.com/quote/BTC-USD&gt;. The data file contains the Bitcoin price data from 1st January 2023 to 8th March 2025 and is loaded using the loadtxt function from numpy.\n\nrates = np.loadtxt(\"btc-usd_data.txt\")\n\n\nYear for analysis\nThe loaded data starts on 1st January 2023 to 8th March 2025. We have data for the complete years 2023 and 2024. The variable below can be set to 2023 or 2024. The task requires data analysis of 2023, so the year variable is set to 2023.\n\nyear = 2023\n\n\n\nHelper function for handling different financial quarters and years in the data\nThe function get_year_info returns a dictionary containing information about the indices in the data for the start and end of the year and each of the financial quarters. This will be used to slice the data when doing the analysis below.\n\ndef get_year_info(chosen_year, data_start_year=2023):\n    \"\"\"\n    For a given chosen_year, determines, the indexes for the chosen_year start and end, the start\n    and end indices for each financial quarter and the number of days in the chosen_year.\n    The function handles leap years.\n\n    :param chosen_year: The year we need the information for.\n    :param data_start_year: The year the data starts from. Defaulted to 2023.\n    :return: A dictionary containing information for the indexes in the loaded data relating to the\n        start and end of the various financial quarters.\n    \"\"\"\n    # Initialise the result to an empty dictionary.\n    result = {}\n\n    # Ensure the input chosen_year is either 2023 or 2024\n    if chosen_year not in [data_start_year, data_start_year + 1]:\n        print(f\"Year must be {data_start_year} or {data_start_year + 1}\")\n        return\n\n    # Use the calendar module to check if the chosen_year is a leap year.\n    if calendar.isleap(chosen_year):\n        result[\"num_days_year\"] = 366\n        q1_num_days = 91\n    else:\n        result[\"num_days_year\"] = 365\n        q1_num_days = 90\n\n    # Number of days in quarters 2,3 are independent on the chosen_year being a leap year.\n    q2_num_days = 91\n    q3_num_days = 92\n\n    # Store the chosen_year's start and end indices\n    result[\"year_start_num\"] = 1\n    result[\"year_end_num\"] = result[\"num_days_year\"]\n\n    # For 2024 the data occurs after 2023 so update the start and end indices\n    if chosen_year == data_start_year + 1:\n        result[\"year_start_num\"] = result[\"num_days_year\"]\n        result[\"year_end_num\"] = result[\"year_start_num\"] + result[\"num_days_year\"] - 1\n\n        # Add data into dictionary\n    result[\"q1_start\"] = result[\"year_start_num\"]\n    result[\"q1_end\"] = result[\"q1_start\"] + q1_num_days - 1\n    result[\"q2_start\"] = result[\"q1_end\"] + 1\n    result[\"q2_end\"] = result[\"q2_start\"] + q2_num_days - 1\n    result[\"q3_start\"] = result[\"q2_end\"] + 1\n    result[\"q3_end\"] = result[\"q3_start\"] + q3_num_days - 1\n    result[\"q4_start\"] = result[\"q3_end\"] + 1\n    result[\"q4_end\"] = result[\"year_end_num\"]\n\n    return result\n\n\n\nGet information about the year for analysis\nCall get_year_info for the chosen year to obtain a dictionary that contains information relating to the indexes in the loaded data relating to the start and end of the various financial quarters.\n\nyear_info = get_year_info(year)",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy vectors"
    ]
  },
  {
    "objectID": "portfolio/dw/Task2/Task_2P.html#descriptive-statistics-for-q3-for-the-chosen-year",
    "href": "portfolio/dw/Task2/Task_2P.html#descriptive-statistics-for-q3-for-the-chosen-year",
    "title": "Working with numpy vectors",
    "section": "Descriptive Statistics for Q3 for the chosen year",
    "text": "Descriptive Statistics for Q3 for the chosen year\nThe following descriptive statistics are calculated using numpy functions and printed for Q3 of the chosen year:\n\nArithmetic mean\nMinimum\n1st quartile\nMedian\n3rd quartile\nMaximum\nStandard deviation\nInter-quartile range (IQR)\n\nThe above calculations are done for the chosen financial quarter in the selected year, which, in this case, is Q3 2023. The quarter can be changed by changing the slicing used to create the numpy array fin_quarter, and the year can be changed with the variable year found at the top of the notebook.\n\n# Create numpy array for the required quarter by slicing the full data\nfin_quarter = rates[year_info[\"q3_start\"]-1:year_info[\"q3_end\"] ]\n\n# Spacing to help with formatting of the printing\npadding = 30\n\n# Print the header\nprint(f\"\\033[1m  Descriptive Statistics for Q3 {year}\\033[0m\")\n\n# Arithmetic mean\nquarter_a_mean = np.mean(fin_quarter)\nprint(f\"##{'arithmetic mean:':&gt;{padding}}   {round(quarter_a_mean, 2):9.2f}\")\n\n# Minimum\nquarter_min = np.min(fin_quarter)\nprint(f\"##{'minimum:':&gt;{padding}}   {round(quarter_min, 2):9.2f}\")\n\n# Quartiles, Q1, Median, Q3\nquartiles = np.quantile(fin_quarter, [0, 0.25, 0.5, 0.75, 1])\n\n# Q1 - first quartile\nquarter_q1 = quartiles[1]\nprint(f\"##{'Q1:':&gt;{padding}}   {round(quarter_q1,2):9.2f}\")\n\n# Median\nquarter_median = quartiles[2]\nprint(f\"##{'median:':&gt;{padding}}   {round(quarter_median, 2):9.2f}\")\n\n# Q3 - third quartile\nquarter_q3 = quartiles[3]\nprint(f\"##{'Q3:':&gt;{padding}}   {round(quarter_q3, 2):9.2f}\")\n\n# Maximum\nquarter_max = np.max(fin_quarter)\nprint(f\"##{'maximum:':&gt;{padding}}   {round(quarter_max, 2):9.2f}\")\n\n# Standard deviation\nquarter_std = np.std(fin_quarter, ddof=0)\nprint(f\"##{'standard deviation:':&gt;{padding}}   {round(quarter_std, 3):9.2f}\")\n\n# Inter-quartile range (IQR)\nquarter_iqr = quarter_q3 - quarter_q1\nprint(f\"##{'IQR:':&gt;{padding}}   {round(quarter_iqr, 3):9.2f}\")\n\n\n  Descriptive Statistics for Q3 2023\n\n##              arithmetic mean:    28091.33\n\n##                      minimum:    25162.65\n\n##                           Q1:    26225.55\n\n##                       median:    28871.82\n\n##                           Q3:    29767.07\n\n##                      maximum:    31476.05\n\n##           standard deviation:     1827.04\n\n##                          IQR:     3541.51\n\n\n\n\nDuring the 3rd financial quarter of 2023, the Bitcoin price had a minimum of \\(\\$25,162.25\\) USD, a maximum of \\(\\$31,476.05\\) USD and a standard deviation of \\(\\$1,827.04\\) USD.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy vectors"
    ]
  },
  {
    "objectID": "portfolio/dw/Task2/Task_2P.html#plot-of-bitcoin-price-value-in-usd-for-q3-2023",
    "href": "portfolio/dw/Task2/Task_2P.html#plot-of-bitcoin-price-value-in-usd-for-q3-2023",
    "title": "Working with numpy vectors",
    "section": "Plot of Bitcoin price value in USD for Q3 2023",
    "text": "Plot of Bitcoin price value in USD for Q3 2023\n\nFunction for generating line plots of the data\nThe plot_quarter_year function takes the start, end and year start indices for slicing of the rates numpy array. Any financial quarter or year can be plotted by changing the slice start and end indices.\n\ndef plot_quarter_year(start, end, year_start, colour, label):\n    \"\"\"\n    Generate a plot of the Bitcoin price as a function of days for either a financial quarter or\n    calendar chosen_year.\n\n    :param start: The index where the plotting will start in the complete data.\n    :param end: The index in the complete data where the plotting will end.\n    :param year_start: The index in the complete data for the start of the chosen chosen_year.\n    :param colour: The colour for the line in the plot.\n    :param label: The label to give the line in the plot.\n    :return: A dictionary containing information for the indexes in the loaded data relating to the\n        start and end of the various financial quarters.\n    \"\"\"\n    # Create a numpy array that will be used for the x-axis in the plot\n    days = np.arange(start - year_start + 1, end - year_start + 2)\n    # Create the plot\n    plt.plot(days, rates[start:end + 1], color=colour, label=label)\n    plt.title(\"BTC to USD\")\n    plt.ylabel(\"Price of BTC in $US\")\n    plt.xlabel(f\"Day number in {year}\")\n    plt.legend()\n    plt.show()\n\n\n\nCall the function to generate the plot\n\n# Call the plot_quarter_year function using the Q3 start and end indices to choosing Q3 from rates\nplot_quarter_year(year_info[\"q3_start\"], year_info[\"q3_end\"], year_info[\"year_start_num\"], 'red',\n                  f'Q3 {year}')\n\n\n\n\n\n\n\n\nIn the third quarter of 2023, the Bitcoin price started nearing its maximum value for the quarter. There was a gradual decline in price until days 228, then a sudden drop to ~\\(\\$26,000\\) USD in a day. The price then gradually declined to the minimum for the quarter before rallying to finish the quarter at around \\(\\$28,000\\) USD.\n\n\nHighest and Lowest days\nThe numpy, armin, and argmax functions are used to calculate the index in the quarter’s numpy array where the minimum and maximum occur. This index is then adjusted to represent the day number in the year.\n\nmin_index = np.argmin(fin_quarter)\n# Change the index to the day within the whole chosen_year rather than just the quarter\nmin_index += (year_info['q3_start'] - year_info[\"year_start_num\"] + 1)\nprint(f\"{'## Lowest':&lt;10} price was on day {min_index} ({round(quarter_min, 2)}).\")\n\nmax_index = np.argmax(fin_quarter)\n# Change the index to the day within the whole chosen_year rather than just the quarter\nmax_index += (year_info['q3_start'] - year_info[\"year_start_num\"] + 1)\nprint(f\"## Highest price was on day {max_index} ({round(quarter_max, 2)}).\")\n\n## Lowest  price was on day 254 (25162.65).\n## Highest price was on day 194 (31476.05).\n\n\nFor the third quarter of 2023, the highest price occurred on day 194 (July 13th) and the lowest on day 254 (September 11th).",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy vectors"
    ]
  },
  {
    "objectID": "portfolio/dw/Task2/Task_2P.html#plots-for-q1-q2-and-q4-of-2023",
    "href": "portfolio/dw/Task2/Task_2P.html#plots-for-q1-q2-and-q4-of-2023",
    "title": "Working with numpy vectors",
    "section": "Plots for Q1, Q2 and Q4 of 2023",
    "text": "Plots for Q1, Q2 and Q4 of 2023\n\n# Call the plot_quarter_year function using the Q1 start and end indices to choosing Q1 from rates \nplot_quarter_year(year_info[\"q1_start\"], year_info[\"q1_end\"], year_info[\"year_start_num\"], 'green',\n                  f'Q1 {year}')\n\n# Call the plot_quarter_year function using the Q2 start and end indices to choosing Q3 from rates \nplot_quarter_year(year_info[\"q2_start\"], year_info[\"q2_end\"], year_info[\"year_start_num\"], 'orange',\n                  f'Q2 {year}')\n\n# Call the plot_quarter_year function using the Q3 start and end indices to choosing Q3 from rates\nplot_quarter_year(year_info[\"q3_start\"], year_info[\"q3_end\"], year_info[\"year_start_num\"], 'red',\n                  f'Q3 {year}')\n\n# Call the plot_quarter_year function using the Q4 start and end indices to choosing Q3 from rates \nplot_quarter_year(year_info[\"q4_start\"], year_info[\"q4_end\"], year_info[\"year_start_num\"], 'blue',\n                  f'Q4 {year}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bitcoin price started 2023 around \\(\\$16,500\\) USD and showed bullish behaviour to finish the quarter near \\(\\$28,000\\) USD. Performance in the 2nd quarter shows high variability, with the overall trend decreasing to the lowest value of ~ \\(\\$25,000\\) USD on June 15th before a dramatic rise to close the quarter around \\(\\$30,500\\) USD. The bullish behaviour seen at the end of the 3rd quarter carried into the 4th quarter. A jump in price from \\(\\$30,000\\) USD to ~ \\(\\$34,500\\) USD occurred between days 296 and 300 (October 23rd to 26th), with another strong rally from \\(\\$37,800\\) USD to \\(\\$44,000\\) USD between days 335 and 340 ( December 1st to 6th). The price stabilises during December to finish the quart and year at \\(\\$42,152.10\\) USD.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy vectors"
    ]
  },
  {
    "objectID": "portfolio/dw/Task2/Task_2P.html#create-the-plot",
    "href": "portfolio/dw/Task2/Task_2P.html#create-the-plot",
    "title": "Working with numpy vectors",
    "section": "Create the plot",
    "text": "Create the plot\nUse the Matplotlib boxplot function to create the plot. The figure size is adjusted to reduce the vertical white space in the plot and to increase its width. The arithmetic mean of the quarter daily price difference is calculated using numpy.mean and added to the plot with the Matplotlib plot function.\n\nplt.figure(figsize=(12, 3), )\nplt.boxplot(quarter_price_difference, vert=False)\nplt.plot(np.mean(quarter_price_difference), 1, color='green', marker='x', linewidth=2, markersize=10)\nplt.title(f\"Distribution of BTC-to-USD daily price increases in Q3 {year}\")\nplt.yticks([], [])\nplt.show()\n\n\n\n\n\n\n\n\nThe box plot shows a graphical representation of some of the descriptive statistics. The left side of the box is the first quartile, the right side is the third quartile, and the orange line inside the box is the median. The box length is the difference between the third and first quartiles and represents the inter-quartile range (IQR) and represents 50% of the data. The whiskers are placed at 1.5*IQR above and below the third and first quartiles. Values outside the whiskers are termed outliers.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy vectors"
    ]
  },
  {
    "objectID": "portfolio/dw/Task2/Task_2P.html#counting-outliers",
    "href": "portfolio/dw/Task2/Task_2P.html#counting-outliers",
    "title": "Working with numpy vectors",
    "section": "Counting outliers",
    "text": "Counting outliers\nOutliers are values that are outside the whiskers. Low outliers are below Q1-1.5IQR, and high outliers are above Q3+1.5IQR. The number of outliers can be counted by finding the values outside the whiskers using slicing. The number is simply the length of the resulting arrays.\n\n# Count outliers\n# Quartiles, Q1, Median, Q3\ndifference_quartiles = np.quantile(quarter_price_difference, [0.25, 0.75])\n\n# Q1\nquarter_difference_q1 = difference_quartiles[0]\n\n# Q3\nquarter_difference_q3 = difference_quartiles[1]\n\n# Inter quartile range (IQR)\nquarter_difference_iqr = quarter_difference_q3 - quarter_difference_q1\n\n# Calculate the values for the whiskers\nlower_whisker = quarter_difference_q1 - 1.5 * quarter_difference_iqr\nupper_whisker = quarter_difference_q3 + 1.5 * quarter_difference_iqr\n\n# Create arrays only containing values outside the whiskers\noutliers_above = quarter_price_difference[quarter_price_difference &gt; upper_whisker]\noutliers_below = quarter_price_difference[quarter_price_difference &lt; lower_whisker]\n\n# The length of the arrays represents the number of outliers.\nprint(f\"## There are {len(outliers_above) + len(outliers_below)} outliers.\",\n      f\"{len(outliers_above)} above the right whisker and\",\n      f\"{len(outliers_below)} below the left whisker.\")\n\n## There are 16 outliers. 7 above the right whisker and 9 below the left whisker.\n\n\nBitcoin’s price can have extreme price fluctuations in short periods of time, and these are likely to appear as outliers.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy vectors"
    ]
  },
  {
    "objectID": "portfolio/dw/Task2/Task_2P.html#summary",
    "href": "portfolio/dw/Task2/Task_2P.html#summary",
    "title": "Working with numpy vectors",
    "section": "Summary",
    "text": "Summary\nThis Jupyter Notebook demonstrates the use of numpy vectors and functions to analyze Bitcoin price data quantitatively using descriptive statistics and visually using line and box plots.\nPossible extensions to the data analysis include: - Create a box plot for each financial quarter to visually compare the daily price difference between quarters. - Plot the price difference data as a histogram and compare to a normal model. - Extend the analysis to look at other years.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy vectors"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Darrin William Stephens",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Darrin William Stephens",
    "section": "Education",
    "text": "Education\n\n\nGraduate Certificate of Cyber Security, 2023\nCharles Sturt University, Albury, New South Wales, Australia\nGraduate Certificate of Business Law, 2019\nSouthern Cross University, Lismore, New South Wales, Australia\nPh.D. in Mechanical Engineering, 2001\nJames Cook University, Townsville, Queensland, Australia\nB.E. in Mechanical Engineering (1st Class Honours), 1996\nJames Cook University, Townsville, Queensland, Australia"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Darrin William Stephens",
    "section": "Interests",
    "text": "Interests\n\nCFD\nMachine Learning\nData Science\nKnowledge Sharing\nLearning New Skills\nJigsaw Puzzles\nStar Wars"
  },
  {
    "objectID": "data_wrangling.html",
    "href": "data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "These are the works produced for SIT731 Data Wrangling unit.\nThe aim of this unit was to learn various data wrangling methodologies and programming techniques to perform them. This included programming in Python for performing various data wrangling tasks, learning data extraction methods from different sources, working with different types of data, storing and retrieving them, applying sampling techniques and inspecting them, cleaning them by identifying outliers/anomalies, handling missing data, transforming, selecting and extracting features, performing exploratory analysis, visualisation using various tools, summarising data appropriately, performing basic statistical analysis and modelling using basic machine learning. Further, techniques for maintaining data privacy and exercising ethics in data manipulation were be covered in the unit.\n\n\n\n Back to top",
    "crumbs": [
      "Portfolio",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "portfolio/dw/Task1/Task_1P.html",
    "href": "portfolio/dw/Task1/Task_1P.html",
    "title": "Introduction to Python and Jupyter Notebooks",
    "section": "",
    "text": "This task introduces the use of Juptyer notebooks and Python through a data analysis exercise involving the calculation and plotting of a group of people’s body mass index (BMI). Using Jupyter required creating and saving a notebook, creating and editing markdown and code cells, executing code cells, and switching between command and editing modes. Python was used for the data analysis, including creating and manipulating lists, creating functions, code documentation, exception handling, printing with formatting and plotting using the Matplotlib library.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Introduction to Python and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "portfolio/dw/Task1/Task_1P.html#introduction",
    "href": "portfolio/dw/Task1/Task_1P.html#introduction",
    "title": "Introduction to Python and Jupyter Notebooks",
    "section": "",
    "text": "This task introduces the use of Juptyer notebooks and Python through a data analysis exercise involving the calculation and plotting of a group of people’s body mass index (BMI). Using Jupyter required creating and saving a notebook, creating and editing markdown and code cells, executing code cells, and switching between command and editing modes. Python was used for the data analysis, including creating and manipulating lists, creating functions, code documentation, exception handling, printing with formatting and plotting using the Matplotlib library.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Introduction to Python and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "portfolio/dw/Task1/Task_1P.html#data-input",
    "href": "portfolio/dw/Task1/Task_1P.html#data-input",
    "title": "Introduction to Python and Jupyter Notebooks",
    "section": "Data input",
    "text": "Data input\nCreate three lists of identical lengths containing the data required for the analysis from my friends. The data required are: - names: A list containing the names of the people (friends) as strings. - heights: A list containing the heights of each person in centimetres. - weights: A list containing the weight of each person in kilograms.\n\nnames   = [\"George\", \"Mary\", \"Frank\", \"Goliath\", \"Susanna\"]\nheights = [172, 163, 185, 199, 178] # in centimetres\nweights = [139, 57, 52, 100, 83] # in kilograms\n\nThe data is now contained in three lists of identical lengths. The next step is to use this data to calculate the BMI for each person in the list.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Introduction to Python and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "portfolio/dw/Task1/Task_1P.html#calculation-of-the-body-mass-index-bmi",
    "href": "portfolio/dw/Task1/Task_1P.html#calculation-of-the-body-mass-index-bmi",
    "title": "Introduction to Python and Jupyter Notebooks",
    "section": "Calculation of the Body Mass Index (BMI)",
    "text": "Calculation of the Body Mass Index (BMI)\nThe traditional body mass index (BMI) formula is weight / height^2, where the weight is in kilograms and the height is in meters. The New BMI formula (Wikipedia, 2025) is 1.3 x weight / height^2.5, where the weight is in kilograms and the height is in meters. Given the weight and height, a function to calculate the BMI (calculate_bmi) is required. This function calculates the BMI using both the traditional and New methods. Before calculating the BMI values, the function conducts some data validation to ensure the passed arguments are convertible to floating point numbers and their value is greater than zero. The calculated BMI values are returned as a tuple.\n\ndef calculate_bmi(weight, height):\n    \"\"\"\n    Calculates the BMI for a person given the weight (kg) and height (m) using the traditional and New methods.\n\n    The formulae are:\n        - Traditional BMI = weight/height^2\n        - New BMI = 1.3 x weight/height^2.25\n\n    :param weight: The weight of the person in kilograms.\n    :param height: The height of the person in meters.\n    :return: A tuple containing the traditional BMI and New BMI.\n    \"\"\"\n    # Handle any input type errors\n    # Check weight can be converted to a float\n    try:\n        weight_num = float(weight)\n    except ValueError:\n        print(f\"Weight is not a number, the type provided is {type(weight)}\")\n        return None\n\n    # Check height can be converted to a float    \n    try:\n        height_num = float(height)\n    except ValueError:\n        print(f\"Height is not a number, the type provided is {type(height)}\")\n        return None\n        \n    # At this point both input variables are known to be numbers. Now check they have the appropriate range.\n    if weight_num &lt;= 0:\n        raise Exception(f\"The persons weight must be greater than zero, the value provided was {weight}\")\n    if height_num &lt;= 0:\n        raise Exception(f\"The persons height must be greater than zero, the value provided was {height}\")\n        \n    # Calculate the BMI\n    persons_bmi = weight_num/height_num**2\n    persons_new_bmi = 1.3*weight_num/height_num**2.5\n    \n    return persons_bmi, persons_new_bmi\n\nIterate over the list of people and calculate their BMI using the standard and New BMI methods by calling the function calculate_bmi. The tuple returned from calculate_bmi is unpacked into the variables trad_bmi and new_bmi which are appended to either the bmis ornew_bmis lists.\n\nbmis = []\nnew_bmis = []\nfor idx, name in enumerate(names):\n    trad_bmi, new_bmi = calculate_bmi(weights[idx], heights[idx]/100)  # Note: the height is converted from centimetres to meters\n    bmis.append(trad_bmi)\n    new_bmis.append(new_bmi)\n\nWe now have two lists, bmis and new_bmis, containing the calculated BMI values for each person for each calculation method.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Introduction to Python and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "portfolio/dw/Task1/Task_1P.html#categorisation-of-bmis",
    "href": "portfolio/dw/Task1/Task_1P.html#categorisation-of-bmis",
    "title": "Introduction to Python and Jupyter Notebooks",
    "section": "Categorisation of BMIs",
    "text": "Categorisation of BMIs\nEach person’s BMI can be placed into one of four categories: underweight, normal, overweight, or obese (Wikipedia, 2025). This is achieved with a helper function categorise_bmi. The function also returns a list of colours that can be used when plotting the data.\n\ndef categorise_bmi(persons_bmi):\n    \"\"\"\n    For a given persons BMI value, categorise it based on the World Health Organisation (WHO) categories and assign a colour for plotting.\n    \n    The categories and colours are:\n        - \"underweight\", BMI &lt; 18.5, colour=\"tab:blue\"\n        - \"normal\", 18.5 &lt;= BMI &lt; 25.0, colour=\"tab:green\"\n        - \"overweight\", 25.0 &lt;= BMI &lt; 30.0, colour=\"tab:orange\"\n        - \"obese\", BMI &gt;= 30.0, colour=\"tab:red\"\n\n    :param persons_bmi:\n    :return: tuple containing strings for the WHO category and the plot colour.\n    \"\"\"\n    if persons_bmi &lt; 18.5:\n        return \"underweight\", \"tab:blue\"\n    elif persons_bmi &lt; 25.0:\n        return \"normal\", \"tab:green\"\n    elif persons_bmi &lt; 30.0:\n        return \"overweight\", \"tab:orange\"\n    else:\n        return \"obese\", \"tab:red\"",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Introduction to Python and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "portfolio/dw/Task1/Task_1P.html#results",
    "href": "portfolio/dw/Task1/Task_1P.html#results",
    "title": "Introduction to Python and Jupyter Notebooks",
    "section": "Results",
    "text": "Results\nThe BMIs for each person in the input data can be tabulated by iterating over the calculated BMI data using a for loop to print one line for each person, as shown in the below output.\n\n# Find the longest name so the spacing can be adjusted when printing\nname_padding = max(map(len, names))\n\nbmi_colours = []\nfor idx, bmi in enumerate(bmis):\n    # Get BMI category and plotting colour\n    bmi_category, bmi_colour = categorise_bmi(bmi)\n    bmi_colours.append(bmi_colour)    \n    print(f\"## {names[idx]:{name_padding}} has BMI of {bmi:4.2f} which is {bmi_category}. The New BMI index is {new_bmis[idx]:4.2f}\")\n\n## George  has BMI of 46.98 which is obese. The New BMI index is 46.57\n## Mary    has BMI of 21.45 which is normal. The New BMI index is 21.84\n## Frank   has BMI of 15.19 which is underweight. The New BMI index is 14.52\n## Goliath has BMI of 25.25 which is overweight. The New BMI index is 23.27\n## Susanna has BMI of 26.20 which is overweight. The New BMI index is 25.53\n\n\nThis output shows that the New BMI is higher for Mary (categorised as normal) and lower for each other person, not classified as normal, than the traditional formula’s value. The New BMI formula uses an exponent for the height of 2.5 to better represent the scaling of mass with volume, which has an exponent of 3. This is an advantage of the New BMI formula as it better represents the scaling od mass with height. The factor of 1.3 is used to align the New BMI result with that of the traditional formula for people of average height. Therefore, the New BMI value will be lower for people with above-average height and higher for people with below-average height compared to the traditional formula’s value. The New BMI formula has similar limitations to the traditional BMI, which is discussed below.\nFinally, we can visualise the data using a bar plot. In the plot, each bar represents one person; the height of the bar signifies their BMI value, and the colour of the bar represents the WHO categorisation of the person’s BMI.\n\nimport matplotlib.pyplot as plt\nplt.bar(names, bmis, color=bmi_colours)\nplt.title(\"My Friends' BMIs\")\nplt.ylabel(r'BMI $\\left(\\frac{kg}{m^2}\\right)$')\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart shows my friend’s BMI values range from approximately 17 to 47. Frank has the lowest BMI and is categorised as underweight, while George is classified as obese, with his BMI being greater than 30. Mary is the only person that has a normal BMI.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Introduction to Python and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "portfolio/dw/Task1/Task_1P.html#limitations-and-benefits-of-bmi",
    "href": "portfolio/dw/Task1/Task_1P.html#limitations-and-benefits-of-bmi",
    "title": "Introduction to Python and Jupyter Notebooks",
    "section": "Limitations and Benefits of BMI",
    "text": "Limitations and Benefits of BMI\nBMI suffers from several limitations from both the medical and societal perspectives.\nMedical limitations of BMI are numerous: - The calculation does not take into account variations between genders or races, as it was initially developed for European men (Wikipedia, 2025). It assumes that the relationship between weight and height is fixed irrespective of race or gender. - It does not account for body composition (fat, muscle, bone and water). - It does not distinguish between muscle and fat mass. Two people with the same height and mass but different muscle and fat percentages will have the same BMI. However, the individual with the higher muscle percentage is likely to have lower health risks but can still be identified as at risk based on their BMI.\n- It does not account for changes due to aging, such as loss of height (Wikipedia, 2025), changes in bone density, or body composition. It assumes a simple scaling between weight and height with disregard for other physical characteristics such as waist, wrist, and neck circumferences.\nIn a societal context, the BMI can result in stigmatisation and labelling of individuals as “obese” or “overweight”, impacting their self-esteem and overall health.\nTwo of the most common misuses of BMI are: - An overemphasis on weight loss targeting the “normal” category rather than promoting overall health. - It can become the sole criterion for diagnosis rather than considering other factors such as an individual’s body composition, muscle mass and physical characteristics.\nDespite its limitations, BMI provides medical practitioners with a convenient and easy-to-calculate measure from readily available measurements (a person’s weight and height) that can be used to identify people who may be at risk of developing obesity-related conditions. Additionally, it can inform public health decisions.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Introduction to Python and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "portfolio/dw/Task1/Task_1P.html#summary",
    "href": "portfolio/dw/Task1/Task_1P.html#summary",
    "title": "Introduction to Python and Jupyter Notebooks",
    "section": "Summary",
    "text": "Summary\nThis Jupyter Notebook demonstrates how to use Jupyter Notebooks and Python to conduct a data analysis exercise involving calculating and plotting a group of people’s body mass index (BMI) using the traditional and New BMI formulae. The limitations and benefits of BMI were discussed.\nPossible extensions to the data analysis include: - De-identify the people by allocating a unique ID. - Further data should be added to allow segregation based on gender. - Consider alternate forms of BMI measure to overcome some of the limitations discussed above, such as the inclusion of waist measurement.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Introduction to Python and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "portfolio/dw/Task3/Task_3P.html",
    "href": "portfolio/dw/Task3/Task_3P.html",
    "title": "Working with numpy Matrices (Multidimensional Data)",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy Matrices (Multidimensional Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task3/Task_3P.html#introduction",
    "href": "portfolio/dw/Task3/Task_3P.html#introduction",
    "title": "Working with numpy Matrices (Multidimensional Data)",
    "section": "Introduction",
    "text": "Introduction\nThis task requires working with numpy matrices and Python to conduct a data analysis exercise involving body measurement data for males and females. The multidimensional body measurement data is loaded into a numpy matrix from a text file. Numpy vector operations are used to calculate the body mass index, append it to the matrix, and standardise the male data. Histogram and box plots were created using the Matplitlib library, and some descriptive statistics were calculated. A scatterplot matrix and matrices for Pearson’s correlation and Spearman’s rho were produced to allow visual and quantitative inspection of the associations between each variable.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy Matrices (Multidimensional Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task3/Task_3P.html#data-input",
    "href": "portfolio/dw/Task3/Task_3P.html#data-input",
    "title": "Working with numpy Matrices (Multidimensional Data)",
    "section": "Data input",
    "text": "Data input\nExcerpts giving the body measurements of adult males and females from the National Health and Nutrition Examination Survey (NHANES dataset) have been used for this analysis. The data was downloaded from a GitHub repository as two CSV files:\n\nnhanes_adult_male_bmx_2020.csv\nnhanes_adult_female_bmx_2020.csv\n\nThese files are loaded into two numpy 2-dimensional arrays using the numpy.loadtxt function. The first 19 lines of the files contain header information and were ignored during the loading.\n\n# Load the male data\nmale = np.loadtxt(\"nhanes_adult_male_bmx_2020.csv\", skiprows=19, delimiter=\",\")\n\n# load the female data\nfemale = np.loadtxt(\"nhanes_adult_female_bmx_2020.csv\", skiprows=19, delimiter=\",\")\n\n# Names for columns\ndata_column_labels = np.array([\"weight\", \"height\", \"upper arm len.\", \"upper leg len.\",\n                               \"arm circ.\", \"hip circ.\", \"waist circ.\", r\"BMI\", ])\n\n\nHelper function to calculate body mass index\nNow that we have the body measurement data loaded from the CSV files, we can calculate the BMI for each observation in the datasets. This is done in the helper function calculate_bmi. Using slicing, we select the columns representing the weight and height measurements and pass them to the function. Take care that the units of the data match those required in the function. The function uses numpy vector operations to calculate the BMI and return it as a numpy vector. This is conducted for both the male and female data arrays.\n\ndef calculate_bmi(weight, height):\n    \"\"\"\n    Calculates the BMI for a person given the weight (kg) and height (m) using the traditional\n    method.\n\n    The formula is BMI = weight/height^2\n\n    :param weight: A numpy vector of the weight of the person in kilograms.\n    :param height: A numpy vector of the height of the person in meters.\n    :return: A numpy vector containing the traditional BMI.\n    \"\"\"\n    # Handle any input type errors\n    # Check if weight is an integer or float\n\n    # We only need to check one value in vector as numpy requires all data to be the same type\n    if not (np.issubdtype(weight[0].dtype, np.floating) or np.issubdtype(weight[0].dtype, np.integer)):\n        raise Exception(f\"Weight is not a number, the type provided is {type(weight[0])}\")\n\n    # Check height can be converted to a float    \n    if not (np.issubdtype(height[0].dtype, np.floating) or np.issubdtype(height[0].dtype, np.integer)):\n        raise Exception(f\"Height is not a number, the type provided is {type(height)}\")\n\n    # At this point, both input variables are known to be numbers. Now check they have the\n    # appropriate range.\n    if np.min(weight) &lt;= 0:\n        raise Exception(f\"The persons weight must be greater than zero. \"\n                        f\"At least one zero or lower value detected.\")\n    if np.min(height) &lt;= 0:\n        raise Exception(f\"The persons height must be greater than zero.\"\n                        f\" At least one zero or lower value detected.\")\n\n    # Calculate the BMI\n    persons_bmi = weight / height ** 2\n\n    return persons_bmi\n\n\n# Calculate the BMI using the calculate_bmi function\n# Column 0 is the weight, and column 1 is the height, making sure we convert height from cm to m\nbmi_male = calculate_bmi(male[:, 0], male[:, 1] / 100)\nbmi_female = calculate_bmi(female[:, 0], female[:, 1] / 100)\n\nNow, we have numpy vectors containing the BMI values for the males and females. We can add these as a column to the original dataset using the numpy.column_stack function. In this case, I have stored the result in a new array for each gender. However, you could choose to overwrite the original array if you wish.\n\n# Add the BMI as a column to the raw data and store the result in a new 2-dimensional array.\nmale_data = np.column_stack((male, bmi_male))\nfemale_data = np.column_stack((female, bmi_female))",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy Matrices (Multidimensional Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task3/Task_3P.html#histogram-plots",
    "href": "portfolio/dw/Task3/Task_3P.html#histogram-plots",
    "title": "Working with numpy Matrices (Multidimensional Data)",
    "section": "Histogram Plots",
    "text": "Histogram Plots\nHistogram plots for the female and male BMIs are produced using Matpliblib.pyplot.hist with 20 bins. The plots are created as subplots in a single figure with the same x-axis limits to allow visual inspection and comparison of the distribution of each dataset (see below). The plots have been coloured to help distinguish between them.\n\nnum_bins = 20\n\n# Create figure with specified size\nhist_fig = plt.figure(figsize=(11, 7))\n\n# Plot female \nax1 = plt.subplot(2, 1, 1)  # 2 rows, 1 column, 1st subplot\nax1.hist(female_data[:, 7], num_bins, edgecolor=\"hotpink\", color=\"deeppink\")\nax1.set_title(f\"Female BMIs (n={female_data.shape[0]})\")\n\n# Plot male\nax2 = plt.subplot(212, sharex=ax1)  # 2 rows, 1 column, 2nd subplot\nax2.hist(male_data[:, 7], num_bins, edgecolor=\"lightskyblue\", color=\"dodgerblue\")\nax2.set_title(f\"Male BMIs (n={male_data.shape[0]})\")\n\n# Set the x limit for both plots\nplt.xlim(10, 70)\nplt.show()",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy Matrices (Multidimensional Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task3/Task_3P.html#box-plots",
    "href": "portfolio/dw/Task3/Task_3P.html#box-plots",
    "title": "Working with numpy Matrices (Multidimensional Data)",
    "section": "Box Plots",
    "text": "Box Plots\nBox plots for each dataset have been produced to compare distributions from a descriptive statics viewpoint visually. The left side of the box is the first quartile, the right side is the third quartile, and the white line inside the box is the median. The box length represents the inter-quartile range (IQR) and represents 50% of the data. The whiskers are placed at 1.5*IQR above and below the third and first quartiles. Values outside the whiskers are termed outliers.\nThe Matplotlib.pyplot.boxplot function was used to create the plot. The figure size was adjusted to reduce the vertical white space and increase its width. Again, the box plots have been coloured to help distinguish each dataset.\n\nplt.figure(figsize=(11, 4))\nlabels = [\"Male\", \"Female\"]\ncolours = [\"dodgerblue\", \"deeppink\"]\nbplot = plt.boxplot([male_data[:, 7], female_data[:, 7]], vert=False, patch_artist=True,\n                    tick_labels=labels, medianprops=dict(color=\"white\", linewidth=1.5))\n\n# Fill with colors\nfor patch, colour in zip(bplot[\"boxes\"], colours):\n    patch.set_facecolor(colour)\n\n# Set the x-axis limits\nplt.xlim(10, 70)\nplt.show()",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy Matrices (Multidimensional Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task3/Task_3P.html#descriptive-statistics-for-the-bmi-data",
    "href": "portfolio/dw/Task3/Task_3P.html#descriptive-statistics-for-the-bmi-data",
    "title": "Working with numpy Matrices (Multidimensional Data)",
    "section": "Descriptive Statistics for the BMI data",
    "text": "Descriptive Statistics for the BMI data\nThe following descriptive statistics are calculated using the helper function stats_describe function that returns a dictionary:\n\nArithmetic mean\nMedian\nMinimum\nMaximum\nStandard deviation\nInter-quartile range (IQR)\nSkewness\nNumber of observations in the data\n\n\ndef stats_describe(a):\n    \"\"\"\n    Compute several descriptive statistics of the passed array. This function borrows ideas from\n    the scipy.stats.describe function.\n\n    :param a: Input 1-d data array.\n    \n    :return: A dictionary containing the descriptive statistics for the input data,\n        a_mean: Arithmetic mean of `a`.\n        a_median: Arithmetic mean of `a`.\n        a_min: Minimum value of `a`.\n        a_max: Maximum value of `a`.\n        a_std: Standard deviation of `a` using ddof=1\n        a_iqr: Inter-quartile range value of `a`.\n        a_skew: Skewness value of `a`.\n        n: Number of observations in the data\n    \"\"\"\n\n    if a.size == 0:\n        raise ValueError(\"The input must not be empty.\")\n\n    n = a.shape[0]\n    a_min = np.min(a, axis=0)\n    a_max = np.max(a, axis=0)\n    a_mean = np.mean(a, axis=0)\n    a_median = np.median(a, axis=0)\n    a_std = np.std(a, axis=0, ddof=1)\n    a_iqr = scipy.stats.iqr(a, axis=0)\n    a_skew = scipy.stats.skew(a, axis=0, bias=True)\n\n    return {\"mean\": a_mean, \"median\": a_median, \"min\": a_min, \"max\": a_max, \"std\": a_std,\n            \"IQR\": a_iqr, \"skew\": a_skew, \"num. obs.\": n}\n\nCalculate the descriptive statistics for both male and female body mass indices.\n\nmale_stats = stats_describe(male_data[:, 7])\nfemale_stats = stats_describe(female_data[:, 7])\n\nPrint the descriptive statics in a table-like format.\n\n# Spacing to help with formatting of the printing\npadding = 10\n\n# Print the header\nprint(\"Descriptive Statistics for BMI data\")\nprint(f\"##                female      male\")\n\n# Iterate over dictionary keys, printing the information for each gender for each key. \nfor key in female_stats.keys():\n    # \n    if key == \"mean\":\n        print(f\"## BMI {key:&lt;{padding}}{round(female_stats[key], 2):7.2f}   \"\n              f\"{round(male_stats[key], 2):7.2f}\")\n    elif key == \"num. obs.\":\n        print(f\"##     {key:&lt;{padding}}{round(female_stats[key], 0):7g}   \"\n              f\"{round(male_stats[key], 0):7g}\")\n    else:\n        print(f\"##     {key:&lt;{padding}}{round(female_stats[key], 2):7.2f}   \"\n              f\"{round(male_stats[key], 2):7.2f}\")\n\nDescriptive Statistics for BMI data\n##                female      male\n## BMI mean        30.10     29.14\n##     median      28.89     28.27\n##     min         14.20     14.91\n##     max         67.04     66.50\n##     std          7.76      6.31\n##     IQR         10.01      7.73\n##     skew         0.92      0.97\n##     num. obs.    4221      4081\n\n\nFrom the histogram plot, the distribution of BMI for females is unimodal and skewed to the right (higher BMI values). For males, the distribution is unimodal and skewed to the right. The box plots show the median BMI (white line in the boxes) for females is higher than for males. Comparing the box length shows that the female BMI distribution has a larger spread than the males. This is also observed visually in the histograms but is easier to identify in the box plots. Both distributions show many outliers on the right side of the distribution and none on the left. The conclusions from the visual inspection of the plots can be verified by inspection of the descriptive statistics.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy Matrices (Multidimensional Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task3/Task_3P.html#standardising-the-data",
    "href": "portfolio/dw/Task3/Task_3P.html#standardising-the-data",
    "title": "Working with numpy Matrices (Multidimensional Data)",
    "section": "Standardising the data",
    "text": "Standardising the data\nWe can standardise the data by subtracting the mean from each element and dividing it by the standard deviation. Here we use the vectorised functionality of numpy to do the standardisation for each measurement (column) in a single command. We constrain the operations to the columns by setting axis=0 in the mean and std operations.\nHere we are standardising only the male data.\n\nzmale = (male_data - np.mean(male_data, axis=0)) / np.std(male_data, axis=0)\n\nThe pair_plot function plots a matrix of scatter plots of several variables against each other. These are useful for visually identifying associations between variables.\n\ndef pair_plot(a, data_labels, ax_labels, alpha=0.3):\n    \"\"\"\n    Draws a scatter plot matrix, given a data matrix, column data_labels, and column units.\n\n    This function is modified from section 7.4.3 of Gagolewski M. (2025).\n\n    Reference:\n    Gagolewski M. (2025), Minimalist Data Wrangling with Python, Melbourne,\n        DOI: 10.5281/zenodo.6451068, ISBN: 978-0-6455719-1-2,\n        URL: https://datawranglingpy.gagolewski.com/.\n\n    :param a: data matrix,\n    :param data_labels: list of column names\n    :param ax_labels: list of data_labels for axes\n    :param alpha: transparency\n    \"\"\"\n    # Get the number of columns\n    k = a.shape[1]\n\n    # The number of columns in X must match the number of data_labels provided\n    assert k == len(data_labels)\n\n    # Create the figure and axes for subplots\n    fig, axes = plt.subplots(nrows=k, ncols=k, sharex=\"col\", sharey=\"row\", figsize=(10, 10))\n    for i in range(k):\n        for j in range(k):\n            ax = axes[i, j]\n            if i == j:  # diagonal\n                ax.text(0.5, 0.5, data_labels[i], transform=ax.transAxes,\n                        ha=\"center\", va=\"center\", size=\"medium\")\n            else:\n                ax.scatter(a[:, j], a[:, i], s=10, color=\"dodgerblue\", edgecolor=\"lightskyblue\",\n                           alpha=alpha, zorder=2)\n                ax.grid(zorder=1, alpha=0.5)\n\n            # Print the column axes labels only on the left side and bottom of the pair-plot figure\n            if j == 0:\n                ax.set_ylabel(ax_labels[i], fontsize=10)\n            if i == len(data_labels) - 1:\n                ax.set_xlabel(ax_labels[j], fontsize=10)\n    fig.tight_layout()\n\nCreate the pair plot for the standardised male data.\n\n# Plot the height, weight, waist circumference, hip circumference, and BMI\ncolumns_to_plot = [1, 0, 6, 5, 7]\n\n# Labels for axes\naxes_labels = np.array([\"z\"] * 8)\n\n# Create the plot using the `pair_plot` function\npair_plot(zmale[:, columns_to_plot], data_column_labels[columns_to_plot],\n          axes_labels[columns_to_plot], alpha=0.8)\nplt.show()",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy Matrices (Multidimensional Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task3/Task_3P.html#pearson-correlation-and-spearmans-rho",
    "href": "portfolio/dw/Task3/Task_3P.html#pearson-correlation-and-spearmans-rho",
    "title": "Working with numpy Matrices (Multidimensional Data)",
    "section": "Pearson Correlation and Spearman’s Rho",
    "text": "Pearson Correlation and Spearman’s Rho\nThe Pearson correlation measures the strength of the linear association between two quantitative variables (De Veaux et al., 2020 p. 200). The correlation values will be between -1 and 1. Positive values indicate a positive association, e.g., when one variable increases, the other also increases. Negative values indicate a negative association, e.g., when one variable increases, the other decreases. Spearman’s Rho correlation uses the Pearson correlation on the rank of the values of two variables (De Veaux et al., 2020 p. 206). It also has values between -1 and 1. Spearman’s rho uses the rank of the variables to measure the consistency of the trend (monotonic relationship) between the two variables. Pearson’s correlation is sensitive to strong outliers, whereas Spearman’s rho is not.\n\ndef corrheatmap(a, data_labels, lbl_set_dict):\n    \"\"\"\n    Draws a correlation heat map, given a matrix of numbers and a list of column names.\n\n    This function is modified from section 9.1.2 of Gagolewski M. (2025).\n\n    Reference:\n    Gagolewski M. (2025), Minimalist Data Wrangling with Python, Melbourne,\n        DOI: 10.5281/zenodo.6451068, ISBN: 978-0-6455719-1-2,\n        URL: https://datawranglingpy.gagolewski.com/.\n\n    :param a: - matrix of numbers for all variable pairs.\n    :param data_labels: list of column names\n    :param lbl_set_dict: a dictionary of settings indicating whether data_labels should be turned\n    on or off for different aspects of the plots\n    \"\"\"\n    # Get number of rows in the input array \n    k = a.shape[0]\n\n    # Number of rows must equal the number of columns and the number of data_labels provided.\n    assert a.shape[0] == a.shape[1] and a.shape[0] == len(data_labels)\n\n    # plot the heat map using a custom colour palette\n    # (correlations are in [-1, 1])\n    plt.imshow(a, cmap=plt.colormaps.get_cmap(\"RdBu\"), vmin=-1, vmax=1)\n\n    # Add text data_labels\n    for i in range(k):\n        for j in range(k):\n            plt.text(i, j, f\"{a[i, j]:.2f}\", ha=\"center\", va=\"center\",\n                     color=\"black\" if np.abs(a[i, j]) &lt; 0.5 else \"white\", size=\"medium\", weight=550)\n\n    plt.xticks(np.arange(k), labels=data_labels, rotation=30)\n    plt.tick_params(axis=\"x\", which=\"both\",\n                    labelbottom=lbl_set_dict[\"labelbottom\"], labeltop=lbl_set_dict[\"labeltop\"],\n                    bottom=lbl_set_dict[\"bottom\"], top=lbl_set_dict[\"top\"])\n\n    plt.yticks(np.arange(k), labels=data_labels)\n    plt.tick_params(axis=\"y\", which=\"both\",\n                    labelleft=lbl_set_dict[\"labelleft\"], labelright=lbl_set_dict[\"labelright\"],\n                    left=lbl_set_dict[\"left\"], right=lbl_set_dict[\"right\"])\n\n    # Turn off grid lines\n    plt.grid(False)\n\nGenerate a correlation heat map for Pearson’s correlation and Spearman’s rho for the standardised male data.\n\nplt.figure(figsize=(11, 11))\nR = np.corrcoef(zmale, rowvar=False)\n\nax1 = plt.subplot(1, 2, 1)  # 1 rows, 2 column, 1st subplot\nlabel_settings = {\"labelbottom\": True, \"labeltop\": False, \"bottom\": True, \"top\": False,\n                  \"labelleft\": True, \"labelright\": False, \"left\": True, \"right\": False\n}\ncorrheatmap(R[np.ix_(columns_to_plot, columns_to_plot)], data_column_labels[columns_to_plot],\n            label_settings)\nax1.set_title(\"Pearsons's r\")\n\nax2 = plt.subplot(1, 2, 2)  # 1 rows, 2 column, 2nd subplot\n\nlabel_settings = {\"labelbottom\": True, \"labeltop\": False, \"bottom\": True, \"top\": False,\n                  \"labelleft\": False, \"labelright\": True, \"left\": False, \"right\": True\n}\n\nrho = scipy.stats.spearmanr(zmale)[0]\ncorrheatmap(rho[np.ix_(columns_to_plot, columns_to_plot)], data_column_labels[columns_to_plot],\n            label_settings)\nax2.set_title(r\"Spearman's $\\rho$\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFrom the pair plot and the heat maps above, the following can be said regarding the associations between the height, weight, waist circumference, hip circumference and BMI variables.\n\nBMI has a strong positive linear association (r &gt; 0.8) with weight, waist circumference and hip circumference and a very weak (0 &lt; r &lt; 0.1) positive linear association with height.\nHip circumference has a strong positive linear association with weight and waist circumference and a weak ( 0.1 &lt; r &lt; 0.5) positive linear association with height.\nWaist circumference has a strong positive linear association with weight, hip circumference, and BMI and a weak positive linear association with height.\nWeight has a weak positive linear association with height.\nThere are a couple of reasons why the weight, waist circumference and hip circumference have a strong association with BMI, whereas height has a weak association.\n\nThe formula for BMI is weight divided by height squared. This creates a stronger relationship with weight over height.\nFat distribution is more likely associated with waist and hip circumferences than height. Since weight increases with fat percentage, there is going to be a strong association between weight, waist and hip circumference. This is shown with the correlation coefficients between weight and these variables being greater than 0.9. Since there is a strong association between BMI and weight, any other quantity that has a strong association with weight will also have a strong association with BMI.\n\nThe Spearman’s rho values are similar to the Pearson correlation values for both the strong and weak associations.\n\nFor the strong associations this suggests the association between the variables is primarily linear.\nFor the weak and below associations, this suggests that neither a linear nor monotonic association is present between theariables.s.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy Matrices (Multidimensional Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task3/Task_3P.html#summary",
    "href": "portfolio/dw/Task3/Task_3P.html#summary",
    "title": "Working with numpy Matrices (Multidimensional Data)",
    "section": "Summary",
    "text": "Summary\nThis Jupyter Notebook demonstrates the use of numpy matrices, vectors, and functions to analyse body measurement data quantitatively using descriptive statistics and visually using histograms, box plots, and scatter plot matrices.\nPossible extensions to the data analysis include: - Add a standard model curve to the histogram plots to visually compare with the standard model. - Consider doing linear regression on the data to see if a predictive model for BMI can be produced from the body measurements.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy Matrices (Multidimensional Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task3/Task_3P.html#references",
    "href": "portfolio/dw/Task3/Task_3P.html#references",
    "title": "Working with numpy Matrices (Multidimensional Data)",
    "section": "References",
    "text": "References\n[1] R. D. De Veaux, P. F. Velleman, and D. E. Bock, Stats: Data and Models. Hoboken, NJ: Pearson, 2020. ‌",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Working with numpy Matrices (Multidimensional Data)"
    ]
  },
  {
    "objectID": "portfolio/dw/Task6/Task_6D.html",
    "href": "portfolio/dw/Task6/Task_6D.html",
    "title": "Pandas/Polars versus SQL",
    "section": "",
    "text": "import os\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport polars as pl\nimport tempfile\nimport timeit",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Pandas/Polars versus SQL"
    ]
  },
  {
    "objectID": "portfolio/dw/Task6/Task_6D.html#introduction",
    "href": "portfolio/dw/Task6/Task_6D.html#introduction",
    "title": "Pandas/Polars versus SQL",
    "section": "Introduction",
    "text": "Introduction\nThis task requires working with pandas to write code that produces equivalent output to various SQL queries. The nycflights13 dataset is used for this exercise. The dataset gives information about all 336,776 flights that departed from three New York airports in 2013 to destinations in the United States, Puerto Rico, and the American Virgin Islands. Additionally, equivalent queries were also written using the polars library, and the execution time for the pandas and polars libraries was calculated and compared.\n\nDatabase connection\nAn SQLite3 database file is created on the local disk for use during these tasks.\n\n#\n# Create a database file in a temporary location\ndbfile = os.path.join(tempfile.mkdtemp(), \"nycflights.db\")\nprint(dbfile)\n\n# Connect to database\nconn = sqlite3.connect(dbfile)\n\nC:\\Users\\darrin\\AppData\\Local\\Temp\\tmpll_hbexh\\nycflights.db\n\n\n\n\nLoad data files and export to database\nCompressed csv files containing data about New York City flights, airlines, airports, planes and weather are loaded from the local disk into pandas dataframes. The pandas dataframes are exported as tables to the SQLite3 database file.\nPolars dataframes (having _pl suffix on the name) are also created from the pandas dataframes. These will be used for testing the polars library in the tasks below.\n\nflights = pd.read_csv(\"nycflights13_flights.csv.gz\", comment=\"#\")\nflights_pl = pl.from_pandas(flights)\n\n\nairlines = pd.read_csv(\"nycflights13_airlines.csv.gz\", comment=\"#\")\nairlines_pl = pl.from_pandas(airlines)\n\n\nairports = pd.read_csv(\"nycflights13_airports.csv.gz\", comment=\"#\")\nairports_pl = pl.from_pandas(airports)\n\n\nplanes = pd.read_csv(\"nycflights13_planes.csv.gz\", comment=\"#\")\nplanes_pl = pl.from_pandas(planes)\n\n\nweather = pd.read_csv(\"nycflights13_weather.csv.gz\", comment=\"#\")\nweather_pl = pl.from_pandas(weather)\n\n\n# Export dataframes to the database\nflights.to_sql(\"flights\", conn, index=False)\nairlines.to_sql(\"airlines\", conn, index=False)\nairports.to_sql(\"Badges\", conn, index=False)\nplanes.to_sql(\"planes\", conn, index=False)\nweather.to_sql(\"weather\", conn, index=False)\n\n26130",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Pandas/Polars versus SQL"
    ]
  },
  {
    "objectID": "portfolio/dw/Task6/Task_6D.html#sql-queries-and-equivalent-pandas-and-polars-queries",
    "href": "portfolio/dw/Task6/Task_6D.html#sql-queries-and-equivalent-pandas-and-polars-queries",
    "title": "Pandas/Polars versus SQL",
    "section": "SQL queries and equivalent Pandas and Polars queries",
    "text": "SQL queries and equivalent Pandas and Polars queries\nThis section contains 17 different SQL queries and the pandas and polars code to give equivalent output.\n\n# Helper function to testing equality between dataframes\ndef test_dataframe_equality(df1, df2, name=\"SQL\"):\n    \"\"\"\n    Function to test if two dataframes are equal.\n\n    :param df1: First pandas dataframe.\n    :param df2: Second pandas dataframe.\n    :param name: Name of the method used to generate the first pandas dataframe.\n    :return: None.\n    \"\"\"\n    try:\n        pd.testing.assert_frame_equal(df1, df2)\n        print(f\"Outcome of comparison: {name} and Pandas results are equal\")\n    except AssertionError as e:\n        print(f\"Outcome of comparison: {name} and Pandas results are not equal: {e}\")\n\n\n# Helper function to print execution times\ndef print_execution_data(pd_dur, pl_dur):\n    \"\"\"\n    Function to print a table comparing Pandas to Polars execution time.\n    \n    :param pd_dur: pandas execution duration.\n    :param pl_dur: polars execution duration.\n    :return: None.\n    \"\"\"\n    print()\n    print(\"Metric                Pandas    Polars\")\n    print(f\"Execution time [s]:  {pd_dur:.4g}  {pl_dur:.4g}\")\n    print(f\"Relative to Pandas:  {1}           {pl_dur/pd_dur:.3g}\")\n\n\nTask 1\nThis SQL query selects the unique engines in the engines column from the planes table.\nMy pandas command: - In the planes dataframe, drop the duplicate rows by considering the engines column. Then select only the engines column and reset the index.\n\n# SQL result\ntask1_sql = pd.read_sql_query(\"\"\"SELECT DISTINCT engine FROM planes\"\"\", conn)\n\n# Pandas result with timing\nt1_st = timeit.default_timer()\n\ntask1_my = planes.drop_duplicates(subset=\"engine\")[[\"engine\"]].reset_index(drop=True)\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task1_my.head(), \"\\n\")\n\nt1_et = timeit.default_timer()\nt1_dur = t1_et - t1_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task1_sql, task1_my)\n\nHead of the Pandas data frame resulting from the query:\n          engine\n0      Turbo-fan\n1      Turbo-jet\n2  Reciprocating\n3        4 Cycle\n4    Turbo-shaft \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt1_st_pl = timeit.default_timer()\n\ntask1_pl = planes_pl.unique(subset=[\"engine\"]).select([\"engine\"])\n\nt1_et_pl = timeit.default_timer()\nt1_dur_pl = t1_et_pl - t1_st_pl\n\n# Checking the Pandas result == Polars result. We need to covert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task1_my.sort_values(by=[\"engine\"]).reset_index(drop=True),\n                        task1_pl.to_pandas().sort_values(by=[\"engine\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t1_dur, t1_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.002907  0.003202\nRelative to Pandas:  1           1.1\n\n\n\n\nTask 2\nThis SQL query selects rows with the unique combination of type and engines from the planes table.\nMy pandas command: - In the planes dataframe, drop the duplicate rows by considering the type and engine columns. Then select only the type and engine columns and reset the index.\n\n# SQL result\ntask2_sql = pd.read_sql_query(\"\"\"SELECT DISTINCT type, engine FROM planes\"\"\", conn)\n\n# Pandas result with timing\nt2_st = timeit.default_timer()\n\ntask2_my = planes.drop_duplicates(subset=[\"type\", \"engine\"])[[\"type\", \"engine\"]].reset_index(drop=True)\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task2_my.head(), \"\\n\")\n\nt2_et = timeit.default_timer()\nt2_dur = t2_et - t2_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task2_sql, task2_my)\n\nHead of the Pandas data frame resulting from the query:\n                       type         engine\n0   Fixed wing multi engine      Turbo-fan\n1   Fixed wing multi engine      Turbo-jet\n2  Fixed wing single engine  Reciprocating\n3   Fixed wing multi engine  Reciprocating\n4  Fixed wing single engine        4 Cycle \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt2_st_pl = timeit.default_timer()\n\ntask2_pl = planes_pl.unique(subset=[\"type\", \"engine\"]).select([\"type\", \"engine\"])\n\nt2_et_pl = timeit.default_timer()\nt2_dur_pl = t2_et_pl - t2_st_pl\n\n# Checking the Pandas result == Polars result. We need to covert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task2_my.sort_values(by=[\"type\", \"engine\"]).reset_index(drop=True),\n                        task2_pl.to_pandas().sort_values(by=[\"type\", \"engine\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t2_dur, t2_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.001832  0.001769\nRelative to Pandas:  1           0.965\n\n\n\n\nTask 3\nThis SQL query counts how many planes there are in the planes table for each engine type and returns the count along with the engine type\nMy pandas command: - In the planes dataframe, group the rows by the engine column and get the size of each group. Then reset the index and rename its column to COUNT(*). Select the columns in order to match the SQL query result.\n\n# SQL result\ntask3_sql = pd.read_sql_query(\"\"\"SELECT COUNT(*), engine FROM planes GROUP BY engine\"\"\", conn)\n\n# Pandas result with timing\nt3_st = timeit.default_timer()\n\ntask3_my = planes.groupby(\"engine\").size().reset_index(name=\"COUNT(*)\")[[\"COUNT(*)\", \"engine\"]]\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task3_my.head(), \"\\n\")\n\nt3_et = timeit.default_timer()\nt3_dur = t3_et - t3_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task3_sql, task3_my)\n\nHead of the Pandas data frame resulting from the query:\n   COUNT(*)         engine\n0         2        4 Cycle\n1        28  Reciprocating\n2      2750      Turbo-fan\n3       535      Turbo-jet\n4         2     Turbo-prop \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt3_st_pl = timeit.default_timer()\n\ntask3_pl = planes_pl.group_by(\"engine\").agg(pl.len().alias(\"COUNT(*)\")).select([\"COUNT(*)\", \"engine\"])\n\nt3_et_pl = timeit.default_timer()\nt3_dur_pl = t3_et_pl - t3_st_pl\n\n# Polars returns uint32 for len(), so we need to cast to allow comparison with pandas.\ntask3_pl = task3_pl.with_columns(pl.col(\"COUNT(*)\").cast(pl.Int64))\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task3_my.sort_values(by=[\"engine\"]).reset_index(drop=True),\n                        task3_pl.to_pandas().sort_values(by=[\"engine\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t3_dur, t3_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.001778  0.00439\nRelative to Pandas:  1           2.47\n\n\n\n\nTask 4\nThis SQL query counts the number of planes in the planes table for each engine and plane type group and returns the count along with the engine type and plane type.\nMy pandas command - In the planes dataframe, group the rows by the engine and type columns and get the size of each group. Then reset the index and rename its column to COUNT(*). Then select the columns in order to match the SQL query result.\n\n# SQL result\ntask4_sql = pd.read_sql_query(\"\"\"SELECT COUNT(*), engine, type FROM planes GROUP BY engine, type\"\"\", conn)\n\n# Pandas result with timing\nt4_st = timeit.default_timer()\n\ntask4_my = planes.groupby([\"engine\", \"type\"]).size().reset_index(name=\"COUNT(*)\")[[\"COUNT(*)\", \"engine\", \"type\"]]\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task4_my.head(), \"\\n\")\n\nt4_et = timeit.default_timer()\nt4_dur = t4_et - t4_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task4_sql, task4_my)\n\nHead of the Pandas data frame resulting from the query:\n   COUNT(*)         engine                      type\n0         2        4 Cycle  Fixed wing single engine\n1         5  Reciprocating   Fixed wing multi engine\n2        23  Reciprocating  Fixed wing single engine\n3      2750      Turbo-fan   Fixed wing multi engine\n4       535      Turbo-jet   Fixed wing multi engine \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt4_st_pl = timeit.default_timer()\n\ntask4_pl = planes_pl.group_by([\"engine\", \"type\"]).agg(pl.len().alias(\"COUNT(*)\")).select([\"COUNT(*)\", \"engine\", \"type\"])\n\nt4_et_pl = timeit.default_timer()\nt4_dur_pl = t4_et_pl - t4_st_pl\n\n# Polars returns uint32 for len(), so we need to cast to allow comparison with pandas.\ntask4_pl = task4_pl.with_columns(pl.col(\"COUNT(*)\").cast(pl.Int64))\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task4_my.sort_values(by=[\"engine\", \"type\"]).reset_index(drop=True),\n                        task4_pl.to_pandas().sort_values(by=[\"engine\", \"type\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t4_dur, t4_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.002324  0.002474\nRelative to Pandas:  1           1.06\n\n\n\n\nTask 5\nThis SQL query gives the minimum, average and maximum years for each unique combination of engine and manufacturer.\nMy pandas command: - For the planes dataframe, group the rows by engine and manufacturer. Select the year column and perform aggregation for the minimum, mean and maximum values. Then reset the index and select the min, mean, max, engine and manufacture columns to return. Rename the aggregation columns to match the SQL result.\n\n# SQL result\ntask5_sql = pd.read_sql_query(\"\"\"SELECT MIN(year), AVG(year), MAX(year), engine, manufacturer\n    FROM planes GROUP BY engine, manufacturer\"\"\", conn)\n\n# Pandas result with timing\nt5_st = timeit.default_timer()\n\ntask5_my = planes.groupby([\"engine\", \"manufacturer\"]).year.agg(\n    [\"min\", \"mean\", \"max\"]).reset_index()[[\"min\", \"mean\", \"max\", \"engine\", \"manufacturer\"]].rename(\n    columns={\"min\": \"MIN(year)\", \"mean\": \"AVG(year)\", \"max\": \"MAX(year)\"})\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task5_my.head(), \"\\n\")\n\nt5_et = timeit.default_timer()\nt5_dur = t5_et - t5_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task5_sql, task5_my)\n\nHead of the Pandas data frame resulting from the query:\n   MIN(year)  AVG(year)  MAX(year)         engine           manufacturer\n0     1975.0     1975.0     1975.0        4 Cycle                 CESSNA\n1        NaN        NaN        NaN        4 Cycle            JOHN G HESS\n2        NaN        NaN        NaN  Reciprocating  AMERICAN AIRCRAFT INC\n3     2007.0     2007.0     2007.0  Reciprocating     AVIAT AIRCRAFT INC\n4        NaN        NaN        NaN  Reciprocating          BARKER JACK L \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt5_st_pl = timeit.default_timer()\n\ntask5_pl = planes_pl.group_by([\"engine\", \"manufacturer\"]).agg(\n    [pl.col(\"year\").min().alias(\"MIN(year)\"), pl.col(\"year\").mean().alias(\"AVG(year)\"),\n    pl.col(\"year\").max().alias(\"MAX(year)\")]).select([\"MIN(year)\", \"AVG(year)\", \"MAX(year)\", \"engine\", \"manufacturer\"])\n\nt5_et_pl = timeit.default_timer()\nt5_dur_pl = t5_et_pl - t5_st_pl\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task5_my.sort_values(by=[\"engine\", \"manufacturer\"]).reset_index(drop=True),\n                        task5_pl.to_pandas().sort_values(by=[\"engine\", \"manufacturer\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t5_dur, t5_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.003621  0.006494\nRelative to Pandas:  1           1.79\n\n\n\n\nTask 6\nThis SQL query selects rows from the planes table where the speed column does not have a null value.\nMy pandas command: - Creates a logical vector for slicing by checking if the speed column in the planes dataframe is not Null. This slice is used with the planes dataframe to select those rows where the speed was not Null. Finally, the index is reset.\n\n# SQL result\ntask6_sql = pd.read_sql_query(\"\"\"SELECT * FROM planes WHERE speed IS NOT NULL\"\"\", conn)\n\n# Pandas result with timing\nt6_st = timeit.default_timer()\n\ntask6_my = planes[planes[\"speed\"].notnull()].reset_index(drop=True)\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task6_my.head(), \"\\n\")\n\nt6_et = timeit.default_timer()\nt6_dur = t6_et - t6_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task6_sql, task6_my)\n\nHead of the Pandas data frame resulting from the query:\n  tailnum    year                      type manufacturer      model  engines  \\\n0  N201AA  1959.0  Fixed wing single engine       CESSNA        150        1   \n1  N202AA  1980.0   Fixed wing multi engine       CESSNA       421C        2   \n2  N350AA  1980.0   Fixed wing multi engine        PIPER  PA-31-350        2   \n3  N364AA  1973.0   Fixed wing multi engine       CESSNA       310Q        2   \n4  N378AA  1963.0  Fixed wing single engine       CESSNA       172E        1   \n\n   seats  speed         engine  \n0      2   90.0  Reciprocating  \n1      8   90.0  Reciprocating  \n2      8  162.0  Reciprocating  \n3      6  167.0  Reciprocating  \n4      4  105.0  Reciprocating   \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt6_st_pl = timeit.default_timer()\n\ntask6_pl = planes_pl.filter(pl.col(\"speed\").is_not_null()).sort(by=[])\n\nt6_et_pl = timeit.default_timer()\nt6_dur_pl = t6_et_pl - t6_st_pl\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task6_my.sort_values(by=[\"speed\"]).reset_index(drop=True),\n                        task6_pl.to_pandas().sort_values(by=[\"speed\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t6_dur, t6_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.001803  0.0008909\nRelative to Pandas:  1           0.494\n\n\n\n\nTask 7\nThis SQL query selects rows from the tailnum column in the planes table with a number of seats between 150 and 210 and a year greater than or equal to 2011.\nMy pandas command - Create a logical vector for slicing by checking rows in the seats column in the planes dataframe where the value is between 150 and 210, and checking rows where the year column in the planes dataframe has a value equal to or greater than 2011. This logical vector is then used to slice the planes data frame. Then only the tailnum column is returned, and the index is reset.\n\n# SQL result\ntask7_sql = pd.read_sql_query(\"\"\"SELECT tailnum FROM planes WHERE seats BETWEEN 150 AND 210 AND year &gt;= 2011\"\"\", conn)\n\n# Pandas result with timing\nt7_st = timeit.default_timer()\n\ntask7_my = planes[(planes[\"seats\"].between(150,210) & (planes[\"year\"] &gt;= 2011))][[\"tailnum\"]].reset_index(drop=True)\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task7_my.head(), \"\\n\")\n\nt7_et = timeit.default_timer()\nt7_dur = t7_et - t7_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task7_sql, task7_my)\n\nHead of the Pandas data frame resulting from the query:\n  tailnum\n0  N150UW\n1  N151UW\n2  N152UW\n3  N153UW\n4  N154UW \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt7_st_pl = timeit.default_timer()\n\ntask7_pl = planes_pl.filter((pl.col(\"seats\").is_between(150,210)) &\n                            (pl.col(\"year\") &gt;= 2011)).select([\"tailnum\"]).sort(by=[])\n\nt7_et_pl = timeit.default_timer()\nt7_dur_pl = t7_et_pl - t7_st_pl\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task7_my.sort_values(by=[\"tailnum\"]).reset_index(drop=True),\n                        task7_pl.to_pandas().sort_values(by=[\"tailnum\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t7_dur, t7_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.001393  0.001259\nRelative to Pandas:  1           0.904\n\n\n\n\nTask 8\nThis SQL query selects rows from the tailnum, manufacturer, and seats columns in the planes table where the manufacturer is one of ‘BOEING’, ‘AIRBUS’, or ‘EMBRAER’ and the number of seats is greater than 390.\nMy pandas command: - Create a logical vector for slicing by checking rows in the manufacturer column in the planes dataframe where the value is one of ‘BOEING’, ‘AIRBUS’, ‘EMBRAER’ and checking rows in the seats column in the planes dataframe where the capacity is greater than 390. This logical vector is then used to slice the planes data frame. Then the tailnum, manufacturer and seats columns are returned, and the index is reset.\n\n# SQL result\ntask8_sql = pd.read_sql_query(\"\"\"SELECT tailnum, manufacturer, seats FROM planes\n    WHERE manufacturer IN ('BOEING', 'AIRBUS', 'EMBRAER') AND seats&gt;390\"\"\", conn)\n\n# Pandas result with timing\nt8_st = timeit.default_timer()\n\ntask8_my = planes[(planes[\"manufacturer\"].isin([\"BOEING\", \"AIRBUS\", \"EMBRAER\"])) & \n           (planes[\"seats\"] &gt; 390)][[\"tailnum\", \"manufacturer\", \"seats\"]].reset_index(drop=True)\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task8_my.head(), \"\\n\")\n\nt8_et = timeit.default_timer()\nt8_dur = t8_et - t8_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task8_sql, task8_my)\n\nHead of the Pandas data frame resulting from the query:\n  tailnum manufacturer  seats\n0  N206UA       BOEING    400\n1  N228UA       BOEING    400\n2  N272AT       BOEING    400\n3  N57016       BOEING    400\n4  N670US       BOEING    450 \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt8_st_pl = timeit.default_timer()\n\ntask8_pl = planes_pl.filter((pl.col(\"manufacturer\").is_in([\"BOEING\", \"AIRBUS\", \"EMBRAER\"])) &\n                            (pl.col(\"seats\") &gt; 390)).select([\"tailnum\", \"manufacturer\", \"seats\"]).sort(by=[])\n\nt8_et_pl = timeit.default_timer()\nt8_dur_pl = t8_et_pl - t8_st_pl\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task8_my.reset_index(drop=True),\n                        task8_pl.to_pandas().reset_index(drop=True), name=\"Polars\")\n\nprint_execution_data(t8_dur, t8_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.001728  0.001018\nRelative to Pandas:  1           0.589\n\n\n\n\nTask 9\nThis SQL query selects unique rows from the year and seats columns in the planes table where the year is greater than or equal to 2012. The returned data is sorted in ascending order by year and descending order by seats.\nMy pandas command: - Create a logical vector for slicing by checking rows in the year column in the planes dataframe where the value is greater than or equal to 2012. This logical vector is then used to slice the planes data frame. Drop duplicates corresponding to the year and seat columns in the resulting data frame. Then select the year and seats columns. Sort the dataframe by year in ascending order, followed by seats in descending order, and reset the index.\n\n# SQL result\ntask9_sql = pd.read_sql_query(\"\"\"SELECT DISTINCT year, seats FROM planes \n    WHERE year &gt;= 2012 ORDER BY year ASC, seats DESC\"\"\", conn)\n\n# Pandas result with timing\nt9_st = timeit.default_timer()\n\ntask9_my = planes[(planes[\"year\"] &gt;= 2012)].drop_duplicates(\n    subset=[\"year\", \"seats\"])[[\"year\", \"seats\"]].sort_values(\n    by=[\"year\", \"seats\"], ascending=[True, False]).reset_index(drop=True)\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task9_my.head(), \"\\n\")\n\nt9_et = timeit.default_timer()\nt9_dur = t9_et - t9_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task9_sql, task9_my)\n\nHead of the Pandas data frame resulting from the query:\n     year  seats\n0  2012.0    379\n1  2012.0    377\n2  2012.0    260\n3  2012.0    222\n4  2012.0    200 \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt9_st_pl = timeit.default_timer()\n\ntask9_pl = planes_pl.filter(pl.col(\"year\") &gt;= 2012).unique(\n    subset=[\"year\", \"seats\"]).select([\"year\", \"seats\"]).sort(by=[\"year\",\"seats\"], descending=[False, True])\nt9_et_pl = timeit.default_timer()\n\nt9_dur_pl = t9_et_pl - t9_st_pl\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task9_my.reset_index(drop=True),\n                        task9_pl.to_pandas().reset_index(drop=True), name=\"Polars\")\n\nprint_execution_data(t9_dur, t9_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.002258  0.003938\nRelative to Pandas:  1           1.74\n\n\n\n\nTask 10\nThis SQL query selects unique rows from the year and seats columns in the planes table where the year is greater than or equal to 2012. The returned data is sorted in descending order by seats, then ascending order by year.\nMy pandas command: - Create a logical vector for slicing by checking rows in the year column in the planes dataframe where the value is greater than or equal to 2012. This logical vector is then used to slice the planes dataframe. In the resulting dataframe, drop duplicates corresponding to the year and seats columns. Then select the year and seats columns. Then sort the dataframe by seats in descending order, followed by year in ascending order, and reset the index.\n\n# SQL result\ntask10_sql = pd.read_sql_query(\"\"\"SELECT DISTINCT year, seats FROM planes WHERE year &gt;= 2012 ORDER BY seats DESC, year ASC\"\"\", conn)\n\n# Pandas result with timing\nt10_st = timeit.default_timer()\n\ntask10_my = planes[(planes['year'] &gt;= 2012)].drop_duplicates(\n    subset=[\"year\", \"seats\"])[[\"year\", \"seats\"]].sort_values(\n    by=[\"seats\", \"year\"], ascending= [False,True]).reset_index(drop=True)\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task10_my.head(), \"\\n\")\n\nt10_et = timeit.default_timer()\nt10_dur = t10_et - t10_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task10_sql, task10_my)\n\nHead of the Pandas data frame resulting from the query:\n     year  seats\n0  2012.0    379\n1  2013.0    379\n2  2012.0    377\n3  2013.0    377\n4  2012.0    260 \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt10_st_pl = timeit.default_timer()\n\ntask10_pl = planes_pl.filter(pl.col(\"year\") &gt;= 2012).unique(\n    subset=[\"year\", \"seats\"]).select([\"year\", \"seats\"]).sort(by=[\"seats\", \"year\"], descending=[True, False])\n\nt10_et_pl = timeit.default_timer()\nt10_dur_pl = t10_et_pl - t10_st_pl\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task10_my.reset_index(drop=True),\n                        task10_pl.to_pandas().reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t10_dur, t10_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.002203  0.003303\nRelative to Pandas:  1           1.5\n\n\n\n\nTask 11\nThis SQL query counts the number of planes with the number of seats above 200 and groups them by manufacturer. Showing how many planes each manufacturer has where the seats are greater than 200.\nMy pandas command - Create a logical vector for slicing by checking rows in the seats column in the planes dataframe where the value is greater than 200. This logical vector is then used to slice the planes dataframe. The resulting dataframe is then grouped by the manufacturer, and the size of each group is evaluated. I reset the index at the same time as renaming the count column to COUNT(*).\n\n# SQL result\ntask11_sql = pd.read_sql_query(\"\"\"SELECT manufacturer, COUNT(*) FROM planes \n    WHERE seats &gt; 200 GROUP BY manufacturer\"\"\", conn)\n\n# Pandas result with timing\nt11_st = timeit.default_timer()\n\ntask11_my = planes[(planes[\"seats\"] &gt; 200)].groupby(\"manufacturer\").size().reset_index(name=\"COUNT(*)\")\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task11_my.head(), \"\\n\")\n\nt11_et = timeit.default_timer()\nt11_dur = t11_et - t11_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task11_sql, task11_my)\n\nHead of the Pandas data frame resulting from the query:\n       manufacturer  COUNT(*)\n0            AIRBUS        66\n1  AIRBUS INDUSTRIE         4\n2            BOEING       225 \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt11_st_pl = timeit.default_timer()\n\ntask11_pl = planes_pl.filter(pl.col(\"seats\") &gt; 200).group_by(\"manufacturer\").agg(\n    pl.len().alias(\"COUNT(*)\"))\n\nt11_et_pl = timeit.default_timer()\nt11_dur_pl = t11_et_pl - t11_st_pl\n\n# Polars returns uint32 for len(), so we need to cast to allow comparison with pandas.\ntask11_pl = task11_pl.with_columns(pl.col(\"COUNT(*)\").cast(pl.Int64))\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task11_my.sort_values(by=[\"manufacturer\", \"COUNT(*)\"]).reset_index(drop=True),\n                        task11_pl.to_pandas().sort_values(by=[\"manufacturer\", \"COUNT(*)\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t11_dur, t11_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.001559  0.001686\nRelative to Pandas:  1           1.08\n\n\n\n\nTask 12\nThis SQL query counts the number of planes for each manufacturer and selects those manufacturers with more than 10 planes. The manufacture and count are returned.\nMy pandas command: - Group the rows of the planes dataframe by the manufacturer and retrieve the size of each group. Reset the index and rename the index column to count. Use the query function to return only those rows where the count is greater than 10. Rename the count column to COUNT(*) and reset the index.\n\n# SQL result\ntask12_sql = pd.read_sql_query(\"\"\"SELECT manufacturer, COUNT(*) FROM planes \nGROUP BY manufacturer HAVING COUNT(*) &gt; 10\"\"\", conn)\n\n# Pandas result with timing\nt12_st = timeit.default_timer()\n\ntask12_my = planes.groupby(\"manufacturer\").size().reset_index(name=\"count\").query(\n    \"count &gt; 10\").rename(columns={\"count\": \"COUNT(*)\"}).reset_index(drop=True)\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task12_my.head(), \"\\n\")\n\nt12_et = timeit.default_timer()\nt12_dur = t12_et - t12_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task12_sql, task12_my)\n\nHead of the Pandas data frame resulting from the query:\n       manufacturer  COUNT(*)\n0            AIRBUS       336\n1  AIRBUS INDUSTRIE       400\n2            BOEING      1630\n3    BOMBARDIER INC       368\n4           EMBRAER       299 \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt12_st_pl = timeit.default_timer()\n\ntask12_pl = planes_pl.group_by(\"manufacturer\").agg(pl.len().alias(\"COUNT(*)\")).filter(\n    pl.col(\"COUNT(*)\") &gt; 10)\n\nt12_et_pl = timeit.default_timer()\nt12_dur_pl = t12_et_pl - t12_st_pl\n\n# Polars returns uint32 for len(), so we need to cast to allow comparison with pandas.\ntask12_pl = task12_pl.with_columns(pl.col(\"COUNT(*)\").cast(pl.Int64))\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task12_my.sort_values(by=[\"manufacturer\", \"COUNT(*)\"]).reset_index(drop=True),\n                        task12_pl.to_pandas().sort_values(by=[\"manufacturer\", \"COUNT(*)\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t12_dur, t12_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.002903  0.004606\nRelative to Pandas:  1           1.59\n\n\n\n\nTask 13\nThis SQL query will show the number of manufacturers that have more than 10 planes with a seat capacity greater than 200.\nMy pandas command: - Create a logical vector for slicing by checking rows in the seats column in the planes dataframe where the value is greater than 200. Slice the planes dataframe using this logical vector, then group the rows by the manufacturer column. Retrieve the size of each group. Reset the index and rename the index column to count. Use the query function to return only those rows where the count is greater than 10. Rename the count column to COUNT(*) and reset the index\n\n# SQL result\ntask13_sql = pd.read_sql_query(\"\"\"SELECT manufacturer, COUNT(*) FROM planes \n    WHERE seats &gt; 200 GROUP BY manufacturer HAVING COUNT(*) &gt; 10\"\"\", conn)\n\n# Pandas result with timing\nt13_st = timeit.default_timer()\n\ntask13_my = planes[planes[\"seats\"] &gt; 200].groupby(\"manufacturer\").size().reset_index(\n    name=\"count\").query(\"count &gt;10\").rename(columns={\"count\": \"COUNT(*)\"}).reset_index(drop=True)\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task13_my.head(), \"\\n\")\n\nt13_et = timeit.default_timer()\nt13_dur = t13_et - t13_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task13_sql, task13_my)\n\nHead of the Pandas data frame resulting from the query:\n  manufacturer  COUNT(*)\n0       AIRBUS        66\n1       BOEING       225 \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt13_st_pl = timeit.default_timer()\n\ntask13_pl = planes_pl.filter(pl.col(\"seats\") &gt; 200).group_by(\"manufacturer\").agg(pl.len().alias(\"COUNT(*)\")).filter(\n    pl.col(\"COUNT(*)\") &gt; 10)\n\nt13_et_pl = timeit.default_timer()\nt13_dur_pl = t13_et_pl - t13_st_pl\n\n# Polars returns uint32 for len(), so we need to cast to allow comparison with pandas.\ntask13_pl = task13_pl.with_columns(pl.col(\"COUNT(*)\").cast(pl.Int64))\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task13_my.sort_values(by=[\"manufacturer\", \"COUNT(*)\"]).reset_index(drop=True),\n                        task13_pl.to_pandas().sort_values(by=[\"manufacturer\", \"COUNT(*)\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t13_dur, t13_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.002486  0.00208\nRelative to Pandas:  1           0.837\n\n\n\n\nTask 14\nThis SQL query shows the top 10 manufacturers by the number of planes they have in the planes table.\nMy pandas command: - Group the rows in the planes dataframe by the manufacturer column. Retrieve the size of each group. Reset the index and rename the index column to howmany. Sort the dataframe by the howmany column in descending order. Reset the index and display the first 10 rows.\n\n# SQL result\ntask14_sql = pd.read_sql_query(\"\"\"SELECT manufacturer, COUNT(*) AS howmany FROM planes\n    GROUP BY manufacturer ORDER BY howmany DESC LIMIT 10\"\"\", conn)\n\n# Pandas result with timing\nt14_st = timeit.default_timer()\n\ntask14_my = planes.groupby(\"manufacturer\").size().reset_index(\n    name=\"howmany\").sort_values(by=\"howmany\", ascending=False).reset_index(drop=True).head(10)\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task14_my.head(), \"\\n\")\n\nt14_et = timeit.default_timer()\nt14_dur = t14_et - t14_st\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task14_sql, task14_my)\n\nHead of the Pandas data frame resulting from the query:\n       manufacturer  howmany\n0            BOEING     1630\n1  AIRBUS INDUSTRIE      400\n2    BOMBARDIER INC      368\n3            AIRBUS      336\n4           EMBRAER      299 \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt14_st_pl = timeit.default_timer()\n\ntask14_pl = planes_pl.group_by(\"manufacturer\").agg(pl.len().alias(\"howmany\")).sort(\n    \"howmany\", descending=True).limit(10)\n\nt14_et_pl = timeit.default_timer()\nt14_dur_pl = t14_et_pl - t14_st_pl\n\n# Polars returns uint32 for len(), so we need to cast to allow comparison with pandas.\ntask14_pl = task14_pl.with_columns(pl.col(\"howmany\").cast(pl.Int64))\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task14_my.sort_values(by=[\"manufacturer\", \"howmany\"]).reset_index(drop=True),\n                        task14_pl.to_pandas().sort_values(by=[\"manufacturer\", \"howmany\"]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t14_dur, t14_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.001631  0.006682\nRelative to Pandas:  1           4.1\n\n\n\n\nTask 15\nThis SQL query returns all rows and columns from the flights table and columns year, speed, and seats from the planes as plane_year, plane_speed, and plane_seat. Any row where the tailnum from flights matches the tailnum in planes the value from the year, speed and seats columns from planes will be added to those columns for that row in the output. If no matching tailnum is found in the planes table, then NULL is returned for the plane_year, plane_speed, and plane_seat for that row.\nMy pandas command: - Select the tailnum, year, speed, and seats columns from planes and assign them to temp15. - Perform a left merge of the flights and temp15 dataframes using tailnum column. Disable suffixes for flights and set to _p for the planes columns. This will prevent columns in both flights and planes from being renamed in flights with a suffix. The planes column will be added with the suffix _p. Finally, rename the year, speed, and seats columns with plane_ prefix.\n\n# SQL result\ntask15_sql = pd.read_sql_query(\"\"\"SELECT flights.*, planes.year AS plane_year, \n    planes.speed AS plane_speed, planes.seats AS plane_seats FROM flights\n    LEFT JOIN planes ON flights.tailnum=planes.tailnum\"\"\", conn)\n\n# Pandas result with timing\nt15_st = timeit.default_timer()\n\n# The following two lines could be done as one without using a temporary variable. However, it is easier to read this way.\ntemp15 = planes[[\"tailnum\", \"year\", \"speed\", \"seats\"]]\ntask15_my = pd.merge(flights, temp15,\n    on=\"tailnum\", how=\"left\", suffixes=(None, \"_p\")).rename(\n    columns={\"year_p\":\"plane_year\",\"speed\":\"plane_speed\", \"seats\":\"plane_seats\"})\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task15_my.head(), \"\\n\")\n\nt15_et = timeit.default_timer()\nt15_dur = t15_et - t15_st\n\n# Comparing the result from SQL and Pandas\n# This command will generate a future warning as seen below.\n# This can be resoled by ensuring all Null type (None, NA, NAN) are replaced with np.nan \nprint(\"This feature warning can be and has been disable in subsequent tasks by making the null like values consistent in the dataframes being compared.\")\ntest_dataframe_equality(task15_sql, task15_my)\n\n# Make null type values consistent\ntask15_sql.fillna(value=np.nan, inplace=True)\ntask15_my.fillna(value=np.nan, inplace=True)\n\n# Assert after making null type values consistent\npd.testing.assert_frame_equal(task15_sql, task15_my)\n\nHead of the Pandas data frame resulting from the query:\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1     517.0             515        2.0     830.0   \n1  2013      1    1     533.0             529        4.0     850.0   \n2  2013      1    1     542.0             540        2.0     923.0   \n3  2013      1    1     544.0             545       -1.0    1004.0   \n4  2013      1    1     554.0             600       -6.0     812.0   \n\n   sched_arr_time  arr_delay carrier  ...  origin dest air_time distance  \\\n0             819       11.0      UA  ...     EWR  IAH    227.0     1400   \n1             830       20.0      UA  ...     LGA  IAH    227.0     1416   \n2             850       33.0      AA  ...     JFK  MIA    160.0     1089   \n3            1022      -18.0      B6  ...     JFK  BQN    183.0     1576   \n4             837      -25.0      DL  ...     LGA  ATL    116.0      762   \n\n   hour  minute            time_hour  plane_year plane_speed  plane_seats  \n0     5      15  2013-01-01 05:00:00      1999.0         NaN        149.0  \n1     5      29  2013-01-01 05:00:00      1998.0         NaN        149.0  \n2     5      40  2013-01-01 05:00:00      1990.0         NaN        178.0  \n3     5      45  2013-01-01 05:00:00      2012.0         NaN        200.0  \n4     6       0  2013-01-01 06:00:00      1991.0         NaN        178.0  \n\n[5 rows x 22 columns] \n\nThis feature warning can be and has been disable in subsequent tasks by making the null like values consistent in the dataframes being compared.\n\n\nC:\\Users\\darrin\\AppData\\Local\\Temp\\ipykernel_21080\\4104427652.py:12: FutureWarning:\n\nMismatched null-like values None and nan found. In a future version, pandas equality-testing functions (e.g. assert_frame_equal) will consider these not-matching and raise.\n\n\n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt15_st_pl = timeit.default_timer()\n\n# The following two lines could be done as one without using a temporary variable. However, it is easier to read this way.\ntemp15_pl = planes_pl.select([\"tailnum\", \"year\", \"speed\", \"seats\"])\ntask15_pl = flights_pl.join(temp15_pl, on=[\"tailnum\"], how=\"left\").rename({\"year_right\":\"plane_year\",\"speed\":\"plane_speed\", \"seats\":\"plane_seats\"})\n\nt15_et_pl = timeit.default_timer()\nt15_dur_pl = t15_et_pl - t15_st_pl\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\n# Make null types consistent\ntask15_pl_pd = task15_pl.to_pandas()\ntask15_pl_pd.fillna(value=np.nan, inplace=True)\ntest_dataframe_equality(task15_my.reset_index(drop=True),\n                        task15_pl_pd.reset_index(drop=True), name=\"Polars\")\n\nprint_execution_data(t15_dur, t15_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.239  0.01319\nRelative to Pandas:  1           0.0552\n\n\n\n\nTask 16\nThis SQL query returns a dataframe that includes all the columns from both planes and airlines, with only the unique carrier and tailnum combinations that exist in the flight dataframe.\nMy pandas command: - From the flights dataframe select the carrier and tailnum columns and drop duplicate rows. Join the result of the previous step with the planes dataframe on the tailnum column using an inner join and assign to temp16. Then join temp16 with the airlines dataframe on the carrier column using an inner join.\n\n# SQL result\ntask16_sql = pd.read_sql_query(\"\"\"SELECT planes.*, airlines.* \nFROM (SELECT DISTINCT carrier, tailnum FROM flights) AS cartail\nINNER JOIN planes ON cartail.tailnum=planes.tailnum\nINNER JOIN airlines ON cartail.carrier=airlines.carrier\"\"\", conn)\n\n# Sort needed for comparison with pandas result\ntask16_sql = task16_sql.sort_values(by=[\"tailnum\", \"carrier\"], ascending=[True, True]).reset_index(drop=True)\n\n# Pandas result with timing\nt16_st = timeit.default_timer()\n\n# The following two lines could be done as one without using a temporary variable. However, it is easier to read this way.\ntemp16 = pd.merge(flights[[\"carrier\", \"tailnum\"]].drop_duplicates(), planes, on=\"tailnum\", how=\"inner\")\ntask16_my = pd.merge(temp16, airlines, on=\"carrier\", how=\"inner\")\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task16_my.head(), \"\\n\")\n\nt16_et = timeit.default_timer()\nt16_dur = t16_et - t16_st\n\n# Create a copy to use with polars comparison\ntask16_pd= task16_my.copy()\n\n# Move the carrier column and sort for comparison with SQL result \ntask16_my.insert(9, \"carrier\", task16_my.pop(\"carrier\"))\ntask16_my=task16_my.sort_values(by=[\"tailnum\",\"carrier\"], ascending=[True, True]).reset_index(drop=True)\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task16_sql, task16_my)\n\nHead of the Pandas data frame resulting from the query:\n  carrier tailnum    year                     type manufacturer     model  \\\n0      UA  N14228  1999.0  Fixed wing multi engine       BOEING   737-824   \n1      UA  N24211  1998.0  Fixed wing multi engine       BOEING   737-824   \n2      AA  N619AA  1990.0  Fixed wing multi engine       BOEING   757-223   \n3      B6  N804JB  2012.0  Fixed wing multi engine       AIRBUS  A320-232   \n4      DL  N668DN  1991.0  Fixed wing multi engine       BOEING   757-232   \n\n   engines  seats  speed     engine                    name  \n0        2    149    NaN  Turbo-fan   United Air Lines Inc.  \n1        2    149    NaN  Turbo-fan   United Air Lines Inc.  \n2        2    178    NaN  Turbo-fan  American Airlines Inc.  \n3        2    200    NaN  Turbo-fan         JetBlue Airways  \n4        2    178    NaN  Turbo-fan    Delta Air Lines Inc.   \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt16_st_pl = timeit.default_timer()\n\n# The following two lines could be done as one without using a temporary variable. However, it is easier to read this way.\ntemp16_pl = flights_pl.select([\"carrier\", \"tailnum\"]).unique().join(planes_pl, on=[\"tailnum\"], how=\"inner\")\ntask16_pl = temp16_pl.join(airlines_pl, on=[\"carrier\"], how=\"inner\")\n\nt16_et_pl = timeit.default_timer()\nt16_dur_pl = t16_et_pl - t16_st_pl\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task16_pd.sort_values(by=[\"tailnum\",\"carrier\"], ascending=[True, True]).reset_index(drop=True),\n                        task16_pl.to_pandas().sort_values(by=[\"tailnum\",\"carrier\"], ascending=[True, True]).reset_index(drop=True),\n                        name=\"Polars\")\n\nprint_execution_data(t16_dur, t16_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.1513  0.02181\nRelative to Pandas:  1           0.144\n\n\n\n\nTask 17\nThis SQL query retrieves flight data originating from EWR and joins it with averaged daily weather from the same location.\nMy pandas command: - From the flights dataframe only select rows where the origin column equals ‘EWR’ and assign the result to flights2 - From the weather dataframe, select only rows where the origin column equal ‘EWR’. Then group rows by the year, month, and day. Perform averaging on the temp and humidity columns. In the resulting dataframe, rename the temp and humidity columns to atemp and ahumid. Reset the dataframe index and assign the result to weather2. - Merge the weather2 into the flights2 dataframe from the left on the year, month and days columns. A left merge will result in all the rows of flights2 being copied into the result. If there is no matching atemp or ahumid for any rows in flights2, then Nan will be inserted into these columns for the rows.\n\n# SQL query\ntask17_sql = pd.read_sql_query(\"\"\"SELECT flights2.*, atemp, ahumid FROM ( SELECT * FROM flights WHERE origin='EWR') AS flights2\nLEFT JOIN (\nSELECT year, month, day, AVG(temp) AS atemp, AVG(humid) AS ahumid FROM weather WHERE origin='EWR' GROUP BY year, month, day\n) AS weather2\nON flights2.year=weather2.year\nAND flights2.month=weather2.month\nAND flights2.day=weather2.day\"\"\", conn)\n\n# Pandas result with timing\nt17_st = timeit.default_timer()\n# From the flights dataframe, select only rows where the origin column equals EWR and assign the result to flights2 \nflights2 = flights[(flights.origin=='EWR')]\n\n# From the weather dataframe, select only rows where the origin column equals EWR. Then group rows by year, month and day. Perform averaging on the temp and humidity columns.\n# Rename the temp and humidity columns in the resulting data frame as atemp and ahumid. Reset the dataframe index and assign the result to weather2.\nweather2 = weather[weather.origin==\"EWR\"].groupby([\"year\",\"month\",\"day\"])[[\"temp\",\"humid\"]].mean(numeric_only=True).rename(columns={'temp':'atemp','humid':'ahumid'}).reset_index()\n\n# Merge the weather2 dataframe into the flights2 dataframe from the left on the year, month and days columns. Left merge will result in all the rows of flights2 will be in the result.\n# If there is no matching atemp or ahumid for any rows in flights2, then a NaN will be inserted into the columns for those rows.\ntask17_my = pd.merge(flights2, weather2, on=['year', 'month', 'day'], how='left')\n\nprint(\"Head of the Pandas data frame resulting from the query:\")\nprint(task17_my.head(), \"\\n\")\n\nt17_et = timeit.default_timer()\nt17_dur = t17_et - t17_st\n\n# Make null type values consistent - see Task 15 for description\ntask17_sql.fillna(value=np.nan, inplace=True)\ntask17_my.fillna(value=np.nan, inplace=True)\n\n# Comparing the result from SQL and Pandas\ntest_dataframe_equality(task17_sql, task17_my)\n\nHead of the Pandas data frame resulting from the query:\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1     517.0             515        2.0     830.0   \n1  2013      1    1     554.0             558       -4.0     740.0   \n2  2013      1    1     555.0             600       -5.0     913.0   \n3  2013      1    1     558.0             600       -2.0     923.0   \n4  2013      1    1     559.0             600       -1.0     854.0   \n\n   sched_arr_time  arr_delay carrier  ...  tailnum origin dest air_time  \\\n0             819       11.0      UA  ...   N14228    EWR  IAH    227.0   \n1             728       12.0      UA  ...   N39463    EWR  ORD    150.0   \n2             854       19.0      B6  ...   N516JB    EWR  FLL    158.0   \n3             937      -14.0      UA  ...   N53441    EWR  SFO    361.0   \n4             902       -8.0      UA  ...   N76515    EWR  LAS    337.0   \n\n   distance  hour  minute            time_hour  atemp     ahumid  \n0      1400     5      15  2013-01-01 05:00:00  38.48  58.386087  \n1       719     5      58  2013-01-01 05:00:00  38.48  58.386087  \n2      1065     6       0  2013-01-01 06:00:00  38.48  58.386087  \n3      2565     6       0  2013-01-01 06:00:00  38.48  58.386087  \n4      2227     6       0  2013-01-01 06:00:00  38.48  58.386087  \n\n[5 rows x 21 columns] \n\nOutcome of comparison: SQL and Pandas results are equal\n\n\n\n# Polars result with timing\nt17_st_pl = timeit.default_timer()\n\nflights2_pl = flights_pl.filter(pl.col(\"origin\") == \"EWR\")\nweather2_pl = (weather_pl.filter(pl.col(\"origin\") == \"EWR\").group_by([\"year\", \"month\", \"day\"]).agg([pl.col(\"temp\").mean().alias(\"atemp\"),\n                pl.col(\"humid\").mean().alias(\"ahumid\")]))\ntask17_pl=flights2_pl.join(weather2_pl, on=[\"year\", \"month\", \"day\"], how=\"left\")\n\nt17_et_pl = timeit.default_timer()\nt17_dur_pl = t17_et_pl - t17_st_pl\n\n# Make null type values consistent - see Task 15 for description\ntask17_pl_pd = task17_pl.to_pandas()\ntask17_pl_pd.fillna(value=np.nan, inplace=True)\n\n# Checking the Pandas result == Polars result. We need to convert the polars dataframe to a pandas dataframe,\n# sort and reset the index for the comparison\ntest_dataframe_equality(task17_my, task17_pl_pd, name=\"Polars\")\n\nprint_execution_data(t17_dur, t17_dur_pl)\n\nOutcome of comparison: Polars and Pandas results are equal\n\nMetric                Pandas    Polars\nExecution time [s]:  0.1417  0.01958\nRelative to Pandas:  1           0.138\n\n\n\n# Close database connection\nconn.close()",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Pandas/Polars versus SQL"
    ]
  },
  {
    "objectID": "portfolio/dw/Task6/Task_6D.html#summary",
    "href": "portfolio/dw/Task6/Task_6D.html#summary",
    "title": "Pandas/Polars versus SQL",
    "section": "Summary",
    "text": "Summary\nThis Jupyter Notebook demonstrates the use of pandas and polars for performing equivalent SQL queries. For cases where the queries were simple, the pandas queries were faster (up to 2x) than polars. However, for cases where the queries were complex, such as the joins, polars was faster (up tp 11x) than pandas.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Pandas/Polars versus SQL"
    ]
  },
  {
    "objectID": "portfolio/dw/Task8/Task_8HD.html",
    "href": "portfolio/dw/Task8/Task_8HD.html",
    "title": "Data Cleaning and Text Analysis",
    "section": "",
    "text": "Code\n# Versions of the libraries used are noted beside the import lines\n# Python version: 3.11.7\nimport os\nimport time\nimport re\nimport pandas as pd                     # 2.1.4\nimport numpy as np                      # 1.26.4\nimport seaborn as sns                   # 0.12.2\n\nimport geopandas                        # 1.01\nfrom geodatasets import get_path        # 2024.8.0\nfrom geopy.geocoders import Nominatim   # 2.4.1\nimport matplotlib.pyplot as plt         # 3.8.0\n\nfrom stop_words import get_stop_words   # 2018.7.23\nimport inflect                          # 7.5.0\nfrom wordcloud import WordCloud         # 1.9.4\nfrom IPython.display import Markdown    # 8.20.0\nfrom collections import Counter \n\nimport plotly.express as px             # 5.9.0\nimport textstat                         # 0.7.7",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Cleaning and Text Analysis"
    ]
  },
  {
    "objectID": "portfolio/dw/Task8/Task_8HD.html#convert-data-files-from-xml-to-csv",
    "href": "portfolio/dw/Task8/Task_8HD.html#convert-data-files-from-xml-to-csv",
    "title": "Data Cleaning and Text Analysis",
    "section": "Convert data files from XML to CSV",
    "text": "Convert data files from XML to CSV\nThe site’s data files are provided in XML format. The first step is to load each file separately and convert them to CSV. This is performed with the xml_to_csv function shown below. This step is only performed if the CSV file doesn’t exists to prevent performing the conversion each time the code is executed.\n\n\nCode\n# Function to convert XML files to CSV\ndef xml_to_csv(file_name, csv_name=None, drop_cols=None):\n    \"\"\"\n    Converts an XML file to CSV using Pandas.\n\n    :param file_name: Name of the XML file to convert\n    :param csv_name: Optional name of exported CSV file. If not provided, \n        the CSV file name will be the same as the XML file name.\n    :param drop_cols: Optional list of columns to drop from dataframe\n    \"\"\"\n    # Read XML file in dataframe\n    df = pd.read_xml(file_name)\n\n    # Check if the user wants to leave any columns out of the conversion\n    if drop_cols is not None:\n        for col in drop_cols:\n            del df[col]\n    # Set CSV name if not provided\n    if csv_name is None:\n        csv_name = file_name.split(\".\")[0]+ \".csv\"\n    # Write CSV file\n    df.to_csv(csv_name, index=False)\n\n    print(f\"Converted {file_name} to {csv_name}\")\n\n# Convert the files if required\nfiles = [\"Badges\", \"Comments\", \"PostHistory\", \"PostLinks\", \"Posts\", \"Tags\",\n        \"Users\", \"Votes\"]\n\ncsv_files = []\nfor file in files:\n    csv_file = os.path.join(\"data\", file+\".csv\")\n    xml_file = os.path.join(\"data\", file+\".xml\")\n    csv_files.append(csv_file)\n    \n    if os.path.exists(csv_file):\n        print(f\"File '{csv_file}' exists.\")\n    else:\n        print(f\"File '{csv_file}' does not exist.\")\n        xml_to_csv(xml_file,drop_cols=drops[file])",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Cleaning and Text Analysis"
    ]
  },
  {
    "objectID": "portfolio/dw/Task8/Task_8HD.html#load-csv-data-files-into-pandas-data-frames",
    "href": "portfolio/dw/Task8/Task_8HD.html#load-csv-data-files-into-pandas-data-frames",
    "title": "Data Cleaning and Text Analysis",
    "section": "Load CSV data files into pandas data frames",
    "text": "Load CSV data files into pandas data frames\nEach CSV file is loaded into pandas data frames. For each data frame, the datatype (dtype) was checked (not shown in the code), and corrections were made if pandas incorrectly assigned the data type. Corrections were often needed for DateTime fields and some string fields.\n\n\nCode\n# Load the data from CSV file into pandas data frames\nbadges = pd.read_csv(csv_files[0], comment=\"#\")\n\n# Correct data types in data frame\nbadges[\"Date\"] = pd.to_datetime(badges[\"Date\"])\nbadges = badges.astype({\"Name\": \"string\"})\n\n\n\n\nCode\n# Load the data from CSV file into pandas data frames\ncomments = pd.read_csv(csv_files[1], comment=\"#\")\n\n# Correct data types in data frame\ncomments[\"CreationDate\"] = pd.to_datetime(comments[\"CreationDate\"])\ncomments = comments.astype({\"Text\": \"string\", \"UserDisplayName\":\"string\"})\n\n\n\n\nCode\n# Load the data from CSV file into pandas data frames\npost_history = pd.read_csv(csv_files[2], comment=\"#\")\n\n# Correct data types in data frame\npost_history = post_history.convert_dtypes()\npost_history[\"CreationDate\"] = pd.to_datetime(post_history[\"CreationDate\"])\n\n\n\n\nCode\n# Load the data from CSV file into pandas data frames\npost_links = pd.read_csv(csv_files[3], comment=\"#\")\n\n# Correct data types in data frame\npost_links[\"CreationDate\"] = pd.to_datetime(post_links[\"CreationDate\"])\n\n\n\n\nCode\n# Load the data from CSV file into pandas data frames\nposts = pd.read_csv(csv_files[4], comment=\"#\")\n\n# Correct data types in data frame\nposts = posts.convert_dtypes()\nposts[\"CreationDate\"] = pd.to_datetime(posts[\"CreationDate\"])\nposts[\"LastEditDate\"] = pd.to_datetime(posts[\"LastEditDate\"])\nposts[\"LastActivityDate\"] = pd.to_datetime(posts[\"LastActivityDate\"])\nposts[\"CommunityOwnedDate\"] = pd.to_datetime(posts[\"CommunityOwnedDate\"])\nposts[\"ClosedDate\"] = pd.to_datetime(posts[\"ClosedDate\"])\n\n\n\n\nCode\n# Load the data from CSV file into pandas data frames\ntags = pd.read_csv(csv_files[5], comment=\"#\")\n\n# Correct data types in dataframe\ntags = tags.convert_dtypes()\n\n\n\n\nCode\n# Load the data from CSV file into pandas data frames\nusers = pd.read_csv(csv_files[6], comment=\"#\")\nusers.head()\n\n# Correct data types in dataframe\nusers = users.convert_dtypes()\nusers[\"CreationDate\"] = pd.to_datetime(users[\"CreationDate\"])\n\n\n\n\nCode\n# Load the data from CSV file into pandas data frames\nvotes = pd.read_csv(csv_files[7], comment=\"#\")\nvotes.head()\n\n# Correct data types in dataframe\nvotes[\"CreationDate\"] = pd.to_datetime(votes[\"CreationDate\"])",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Cleaning and Text Analysis"
    ]
  },
  {
    "objectID": "portfolio/dw/Task8/Task_8HD.html#geographic-distribution-of-forum-users",
    "href": "portfolio/dw/Task8/Task_8HD.html#geographic-distribution-of-forum-users",
    "title": "Data Cleaning and Text Analysis",
    "section": "Geographic distribution of forum users",
    "text": "Geographic distribution of forum users\nThis section investigates the distribution of forum users worldwide by using the location information provided by users on their account profiles. The location information users enter is optional, and no form of validation is performed. This results in many empty entries and free-form input of the data. The free-form input presents challenges when working with the data. These challenges are identified and addressed in this section, while others remain for future improvements.\n\nData clean-up\nRemove any rows in the users data frame that do not contain location information.\n\n\nCode\nnum_users = users.shape[0]\n# Drop rows that have no location\nusers.dropna(subset=[\"Location\"], inplace=True)\nnum_users_loc = users.shape[0]\n\n\nIn May 2024, 50776 users of the Quant site were registered, and 14636 of these had provided an entry in the location field of their profile.\nThe next step is to determine how many of the provided locations contain valid information that can be used to locate the user on a map. This is an issue of privacy, so Stack Exchange does not require users to provide a location. Privacy could also be the reason the location provided is not validated.\nA function was created to validate the location. To be a valid location, the string must contain either of the following patterns: “city, state, country” or “city, country”. The functions start by checking if the string only contains numbers, URLs or non-ASCII characters. Then, it checks if any invalid character sequences have been found, such as double hyphens, double apostrophes, etc. A name pattern is used for each valid name component and accepts letters, spaces, hyphens, apostrophes and periods. This pattern is compiled into a full pattern that adds commas and spaces between each name.\nThe captured groups from the regex match are then checked to ensure that the city isn’t empty and the state isn’t empty if a country is provided. If only the city were state are provided, the state could also represent the country, e.g., Paris, France. If only a city and state are provided, this could also be for an American user as they often don’t enter their country. It has been observed that some invalid locations are still allowed by the function, and therefore, the function could be improved.\n\n\nCode\ndef is_valid_location(location_string):\n    \"\"\"\n    Uses regex to validate if a location string follows the pattern of:\n    - 'city, state, country' OR \n    - 'city, country' (valid for no USA countries or USA locations)\n    \n    Rejects strings with only numbers, URLs, non-ASCII characters.\n    Rejects invalid character sequences:\n        - Double hyphens\n        - Double apostrophes\n        - Hyphen followed by apostrophe\n        - Apostrophe followed by hyphen\n        - Double periods\n\n    A name pattern is used to identify valid name components \n    (city, state, country). This pattern now accepts letters, spaces,\n     hyphens, apostrophes, and periods.\n\n    The function requires at least two matches of the name pattern and marks\n    any entries were the state and 'usa'/'united states' are not separated\n     by a comma as invalid\n\n    This function allows some invalid locations, therefore could be improved.\n    \n    :param location_string: String containing the location information to check\n    :return: True or False depending on if the location_string is valid\n    \"\"\"\n    # Clean up input string\n    location_string = location_string.strip()\n    \n    # Reject if contains URLs, or non-ASCII characters\n    if re.search(r\"^[\\d\\s]+$|http|www|[^\\x00-\\x7F]\", location_string):\n        return False\n\n    # Check for invalid character patterns\n    invalid_patterns = {\n    \"double_hyphen\": re.compile(r\"--\"),      # Double hyphens\n    \"double_apostrophe\": re.compile(r\"''\"),  # Double apostrophes\n    \"hyphen_apostrophe\": re.compile(r\"-'\"),  # Hyphen followed by apostrophe\n    \"apostrophe_hyphen\": re.compile(r\"'-\"),  # Apostrophe followed by hyphen\n    \"double_period\": re.compile(r\"\\.\\.\")     # Double periods\n    }\n\n    # Check for invalid character sequences\n    for pattern_name, pattern in invalid_patterns.items():\n        if pattern.search(location_string):\n            return False\n\n   \n    # Define pattern for valid name components (city, state, country)\n    # This pattern now accepts letters, spaces, hyphens, apostrophes, and\n    #  periods.\n    name_pattern = r\"[A-Za-z\\s\\-'.]+\"\n\n    # Build the full pattern using the name_pattern\n    location_pattern = re.compile(\n        f\"^({name_pattern}),\\\\s*({name_pattern})(?:,\\\\s*({name_pattern}))?$\"\n    )\n\n    match = re.match(location_pattern, location_string)\n    if not match:\n        return False\n\n    # Extract the captured groups\n    groups = match.groups()\n    city = groups[0].strip()\n    state = groups[1].strip()\n    country = groups[2].strip() if groups[2] else None\n    \n    # Ensure city and state are not empty\n    if not city or not state:\n        return False\n        \n    # If we have a country part (3-part format), ensure it's not empty\n    if groups[2] is not None and not country:\n        return False\n\n    substrings = [\"usa\", \"us\", \"united states\"]\n    # Any entries that state == USA are dropped\n    if state.lower() in substrings:\n        return False\n\n    # Check the we don't have other strings with the substrings\n    for substring in substrings:\n        if substring in state.lower():\n            return False\n        \n    # Passed all checks\n    return True\n\n\nThe is_valid_location function is applied to the Location column of the users data frame, with the function output captured in a new column, ValidLocation. Users without a valid location are removed from the analysis as a latitude and longitude cannot be found for invalid locations.\n\n\nCode\n# Apply is_valid_location function to the dataSeries Location\nusers[\"ValidLocation\"] = users[\"Location\"].apply(is_valid_location)\n\n# Drop rows without a valid location\nusers = users[users[\"ValidLocation\"]]\nnum_users_valid_location = users.shape[0]\n\n\nThe number of users with valid locations is 7598.\nNow that valid locations have been found, the next step is to take the location information and separate it into city, state, and country for usage later. For this purpose, a function was created to extract the location parts from the location string. The function extract_location_parts takes a string and splits it into parts at commas. If the split produces three parts, they are assumed to represent the city, state, and country. If, after the split, only two parts are obtained, then it is assumed the first part represents the city. The second part is checked to see if it matches any American state or territory; if it does, then the second part is assigned to the state and the country is set to “USA”. If the second part doesn’t match any American state or territory, it is assumed to represent a country and the state is set to None. This function could be improved further by adding a check to the country part, making sure it is a valid country.\n\n\nCode\ndef extract_location_parts(location_string):\n    \"\"\"\n    Extract the parts of a location as city, state, country from a provided\n     string.\n\n    :param location_string: String to get location parts from\n    :return: pandas data series containing the parts of the location\n    \"\"\"\n    # Split the input string by commas\n    parts = [part.strip() for part in location_string.split(\",\")]\n\n    # Check if already in city, state, country format\n    if len(parts) == 3:\n        city, state, country = parts\n    \n    # Check if in city, country or city, state format\n    elif len(parts) == 2:\n        city, second_part = parts\n        \n        # Check if the second part might be a US state\n        #us_states = {\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n        #             \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n        #             \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n        #             \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n        #             \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\",\n        #             \"DC\"}  # Adding District of Columbia\n        us_states = {\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States#States.\n            \"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n            \"HI\", \"IA\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\", \"MD\",\n            \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\",\n            \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n            \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\", \"WY\",\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States#Federal_district.\n            \"DC\",\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States\n        # #Inhabited_territories.\n            \"AS\", \"GU\", \"MP\", \"PR\", \"VI\",\n        }\n        \n        # Also check for full state names\n        us_state_names = {\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States#States.\n            \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \n            \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \n            \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \n            \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \n            \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \n            \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \n            \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \n            \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \n            \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \n            \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\",\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States#Federal_district.        \n            \"District of Columbia\",\n        # https://en.wikipedia.org/wiki/\n        # List_of_states_and_territories_of_the_United_States\n        # #Inhabited_territories.\n            \"American Samoa\", \"Guam GU\", \"Northern Mariana Islands\",\n            \"Puerto Rico PR\", \"U.S. Virgin Islands\"\n        }\n\n        # If the second part looks like a US state, treat it as city, state and\n        #  add USA\n        if second_part.upper() in us_states or second_part in us_state_names:\n            state = second_part\n            country = \"USA\"\n        else:\n            # Otherwise, it's city, country format\n            country = second_part\n            state = None\n    \n    return pd.Series([city, state, country])\n\n\nThe extract_location_parts function is applied to the Location column of the user data frame and the result is returned as three new columns: city, state, and country. For ease of processing, a copy of the users data frame is produced that only contains unique entries for the city, state, and country.\n\n\nCode\nusers[[\"City\", \"State\", \"Country\"]] = users[\"Location\"].apply(\n    extract_location_parts)\n\nusers_geo = users.drop_duplicates(\n    subset=[\"City\",\"State\",\"Country\"])[[\"City\", \"State\", \"Country\"]].copy()\nnum_unique_loc = users_geo.shape[0]\n\n\nAfter cleaning the user’s location data 2000 unique locations required geo-locating.\n\n\nObtaining latitude and longitude\nThe geopy package is used to obtain the latitude and longitude of a location . Geopy provides a client interface for several popular geocoding web services. The Nominatim web service was utilised as it is free. Nominatim uses OpenStreetMap data to find locations on Earth by name and address. The only limitations found were that it required only one request per second and could time out. A time out value of 60 seconds was found to work.\n\n\nCode\ndef get_lat_lon(city, country, state=None):\n    \"\"\"\n    Function to return the latitude and longitude given a city, state,\n     country or city, country. If no latitude or longitude can be \n     found for the provided address then None is returned.\n\n    :param city: string containing the name of locations city\n    :param country: string containing the name of locations country\n    :param state: Option string containing the name of locations state\n    :return: Pandas data series containing the latitude and longitude\n    \"\"\"\n    geolocator = Nominatim(user_agent=\"geocoder\")\n\n    # Slow down the requests. The Nominatim usage policy says no more than one\n    #  request per second\n    time.sleep(1.1)\n    \n    if state:\n        location = geolocator.geocode(f\"{city}, {state}, {country}\", timeout=60)\n    else:\n        location = geolocator.geocode(f\"{city}, {country}\", timeout=60)\n        \n    if location:\n        return pd.Series([location.latitude, location.longitude])\n    else:\n        return pd.Series([None, None])\n\n\nSince the requesting latitude and longitude for each location was slow, it was only performed once, and the result was stored on disk. If the user_geolocation.csv file is found on disk, then the data is loaded into a data frame. Otherwise, it is calculated and stored on disk.\n\n\nCode\ngeo_location_file = os.path.join(\"data\", \"user_geolocation.csv\")\nif os.path.exists(geo_location_file):\n    print(f\"File '{geo_location_file}' exists.\")\n    print(f\"Reading geo locations.\")\n    users_geo = pd.read_csv(geo_location_file, comment=\"#\")\nelse:\n    print(f\"File '{geo_location_file}' does not exist.\")\n    print(f\"Creating geo locations.\")\n    users_geo[[\"Latitude\",\"Longitude\"]] = users_geo.apply(\n        lambda x: get_lat_lon(x[\"City\"], x[\"Country\"],x[\"State\"]), axis=1)\n\n    # Drop and location where a latitude or longitude was not found\n    users_geo = users_geo.dropna(subset=[\"Latitude\", \"Longitude\"])\n    users_geo.to_csv(geo_location_file, index=False)\n\nnum_unique_loc_val = users_geo.shape[0]\nnum_invalid_latlong = num_unique_loc - num_unique_loc_val\n\n\nDue to false positives in the cleaning function, there were still some (71) invalid locations were a latitude and longitude did not exist. Thus the final number of unique user locations was 1929.\n\n\nProducing a map of locations\nThe unique user locations were plotted onto a world map using Geopandas.\n\n\nCode\ngdf = geopandas.GeoDataFrame(\n    users_geo, geometry=geopandas.points_from_xy(\n        users_geo.Longitude, users_geo.Latitude), crs=\"EPSG:4326\"\n)\n\nworld = geopandas.read_file(get_path(\"naturalearth.land\"))\n\nfig = plt.figure(figsize=(12, 6))\nax = fig.add_subplot()\n\n# Remove Antarctica\nworld.clip([-180, -55, 180, 90]).plot(\n    ax=ax, color=\"lightgray\", edgecolor=\"black\")\n\n# We can now plot our GeoDataFrame.\ngdf.plot(ax=ax, color=\"red\", markersize=8)\nax.set_xticks([])\nax.set_yticks([])\nplt.title(\"Location of Quant Stack Exchange users\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Location of Quant Stack Exchange users\n\n\n\n\n\nFigure 1 shows the unique locations of the Quant Stack Exchange site users worldwide. Many users are clustered in Europe, the east and west coasts of the United States and India. Africa, South America and South-East Asia have fewer user sites. In Australia, the users are located around capital cities, with Perth being an exception, with no users.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Cleaning and Text Analysis"
    ]
  },
  {
    "objectID": "portfolio/dw/Task8/Task_8HD.html#the-top-100-words-used-in-the-titles-of-the-top-20-of-questions",
    "href": "portfolio/dw/Task8/Task_8HD.html#the-top-100-words-used-in-the-titles-of-the-top-20-of-questions",
    "title": "Data Cleaning and Text Analysis",
    "section": "The top 100 words used in the titles of the top 20% of questions",
    "text": "The top 100 words used in the titles of the top 20% of questions\nFor this analysis, the titles of questions are analysed for the top 20% of posts based on their score, and the top 100 words are extracted and visualised as a word cloud. The count of the top 20 words are shown in a table. A post’s score is the number of UpVotes minus the number of DownVotes.\n\nData preparation\nTo prepare the data for visualisation, a data frame consisting of only questions is created from the posts data frame using the PostTypeId column. Questions have a PostTypeId = 1. A function (text_clean) is created that will clean the title text for each post. The cleaning involves converting all text to lowercase, removing any numbers, removing common words (stop words), and converting plurals to the singular version. The text_clean function is applied to the Title column from the questions data frame, and the result is returned as a new column, CleanTitle. The questions data frame is then filtered only to have posts with a score higher than the top 80% of scores. Cleaned titles from the remaining posts are converted to a list.\n\n\nCode\n# Filter for questions only (PostTypeId = 1)\n# Make a copy as we modify this data frame later\nquestions = posts[posts[\"PostTypeId\"] == 1].copy()\n\n# Generic English stop words to ignore\nstop_words = get_stop_words(\"en\")\n\n# Define custom stop words for the finance domain\ncustom_stopwords = [\"can\", \"question\", \"using\", \"use\", \"value\", \"values\",\n        \"calculate\", \"formula\", \"formulas\", \"quant\", \"quantitative\",\n        \"finance\", \"financial\"]\n\n# Combine generic and custom stop words\nstop_words.extend(custom_stopwords)\n\n# Function to clean text\ndef clean_text(text, convert_plurals=False):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove numbers\n    text = re.sub(r\"\\d+\", \"\", text)\n    \n    # Remove stop words\n    words = [word for word in text.split() \n            if word not in stop_words and len(word) &gt; 2]\n\n    # Convert plurals of words to their singular form\n    if convert_plurals:\n        p = inflect.engine()\n        words = [p.singular_noun(word) \n                if p.singular_noun(word) else word  for word in words]\n    \n    return \" \".join(words)\n\n# Clean the titles\nquestions[\"CleanTitle\"] = questions[\"Title\"].apply(\n    clean_text,  convert_plurals=True)\n\n# Separate high posts\nhigh_posts = questions[questions[\"Score\"] &gt;= questions[\"Score\"].quantile(0.8)]\n\n# Combine all titles for each group\nhigh_posts_text = \" \".join(high_posts[\"CleanTitle\"].tolist())\n\n\n\n\nVisualisation and Top 20 words\nFigure 2 shows the words used in the titles of the 20% of the questions. Option, model and volatility are the words that stand out. It isn’t surprising that “option” was the most common occurring word in the questions, given it is the most used type of derivative by financial markets. Table 1 shows the top 20 words that appear in the high-scoring posts. It is observed that the words are those that you would expect from financial discussions. The words I would have expected to see but are missing are those relating to programming, programming languages and specific financial models such as the black-scholes.\n\n\nCode\n# Generate word cloud for the top 100 words\nwordcloud = WordCloud(width=1000, height=600, background_color=\"white\",\n        max_words=100, collocations=False, contour_width=3\n        ).generate(high_posts_text)\n    \nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.title(\"Words in titles of the top 20% of Questions\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Words in titles of the top 20% of Questions\n\n\n\n\n\n\n\nCode\n# Analyse word frequencies\nhigh_freq = Counter(high_posts_text.split())\n\n# Get the most common words in each category\ndf = pd.DataFrame(high_freq.most_common(20), columns=[\"Word\",\"Count\"])\nMarkdown(df.to_markdown(index = False))\n\n\n\n\nTable 1: Top 20 words in the title of the high-scoring questions\n\n\n\n\n\n\nWord\nCount\n\n\n\n\noption\n419\n\n\nvolatility\n383\n\n\nmodel\n364\n\n\nprice\n282\n\n\nportfolio\n234\n\n\nmarket\n223\n\n\nrate\n199\n\n\ndatum\n190\n\n\npricing\n183\n\n\nrisk\n174\n\n\nreturn\n171\n\n\nstock\n158\n\n\nimplied\n154\n\n\ntrading\n154\n\n\ntime\n135\n\n\nstochastic\n118\n\n\nfuture\n107\n\n\nbond\n100\n\n\ninterest\n100\n\n\nfactor\n91",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Cleaning and Text Analysis"
    ]
  },
  {
    "objectID": "portfolio/dw/Task8/Task_8HD.html#sec-tags",
    "href": "portfolio/dw/Task8/Task_8HD.html#sec-tags",
    "title": "Data Cleaning and Text Analysis",
    "section": "Evolution of tag popularity over time",
    "text": "Evolution of tag popularity over time\nThis visualisation tracks how the popularity of different tags changes over time. Tags are used to categorise posts, making it easier for people to search for posts related to a particular topic.\n\nData preparation\nThe first step is to drop any rows that contain a NaN in the Tags column of the posts data frame. The return from this operation is a view into the data frame, which will prevent the addition of any new columns. Therefore, a copy (posts_cleaned) of the data frame is made to allow for alteration. The tags for each post are stored as |tag1|tag2|tag3| etc. A function, separate_tags, was created to separate the tags from a string to a list of individual tags with this format. The separate_tags function is applied to the posts_cleaned data frame, and the results are stored in a new column called SepTag. Having the tags in a list isn’t helpful for visualisation, so the explode function creates duplicate rows for each tag in the list of a given row (tag_posts). The creation date for the posts is used to create a new column containing each post’s month-year. The data is then grouped by month and tag to allow for counting occurrences of each tag per month. The top ten tags by count are extracted into a list, which is then used to filter the monthly tag counts so only the top ten tags remain. The last step is to create a column in the posts_cleaned data frame for the tag type. If the SepTags column is empty, then the ‘No Tag’ label is applied; if the tag lists contain any of the top 10 tags, the label’ Popular Tag’ is applied; otherwise, the label ‘Unpopular Tag’ is applied. These labels will be used in later visualisations.\n\n\nCode\n# Drop rows with NA values in the Tags column and make a copy of the resulting\n#  data frame\nposts_cleaned = posts.dropna(subset=[\"Tags\"]).copy()\n\n# Tags in StackExchange are stored as |tag1|tag2|tag3|\ndef separate_tags(x):\n    \"\"\"\n    Function to separate tags and return as a list\n\n    :param x: string containing tags that need separating\n    :return: A list of tags \n    \"\"\"\n    return x[1:-1].split(\"|\")\n\n# Apply the separate_tags function to the \"Tags\" column and store the result\n#  in a new column\nposts_cleaned[\"SepTag\"] = posts_cleaned[\"Tags\"].apply(separate_tags)\n\n# Use explode to ensure that there is only one tag per row. This duplicates \n# rows for all other column values\ntag_posts = posts_cleaned.explode(\"SepTag\")\n\n# Group by month and tag to count occurrences\ntag_posts[\"Month\"] = tag_posts[\"CreationDate\"].dt.to_period(\"M\")\nmonthly_tag_counts = tag_posts.groupby(\n    [\"Month\", \"SepTag\"]).size().reset_index(name=\"Count\")\n\n# Convert Period to datetime for plotting\nmonthly_tag_counts[\"MonthDate\"] = monthly_tag_counts[\"Month\"].dt.to_timestamp()\n\n# Get the top 10 tags\ntop_tags = tag_posts[\"SepTag\"].value_counts().nlargest(10).index.tolist()\n\n# Filter for only the top tags\ntop_tag_counts = monthly_tag_counts[monthly_tag_counts[\"SepTag\"].isin(top_tags)]\n\n# Define conditions and choices\nconditions = [\n    # Empty list check\n    posts_cleaned[\"SepTag\"].str.len() == 0,  \n    # Intersection check\n    posts_cleaned[\"SepTag\"].apply(lambda x: bool(set(x) & set(top_tags)))  \n]\n\n# Create a TagType column\nchoices = [\"No Tag\", \"Popular Tag\"]\nposts_cleaned[\"TagType\"] = np.select(\n    conditions, choices, default=\"Unpopular Tag\")\n\n\n\n\nVisualisation of the top 10 tags\nFor the visualisation, a stacked area chart is chosen as it allows to see the cumulative total of the top 10 tags as well as their distribution. Figure 3 shows the evolution of the tag’s popularity over time.\n\n\nCode\n# Create a pivot table for the stacked area chart\npivot_data = top_tag_counts.pivot(\n    index=\"MonthDate\", columns=\"SepTag\", values=\"Count\")\n\n# Create the visualization\nplt.rcParams[\"figure.figsize\"] = (8,6)\n\n# Stacked area chart\nax = pivot_data.plot.area(alpha=0.7)\nplt.xlabel(\"Date\", fontsize=14)\nplt.ylabel(\"Number of Posts per Month\", fontsize=14)\n\n# Add a legend\nax.legend(loc=\"upper left\", fontsize=12)\nplt.tight_layout()\nplt.title(\"Evolution of tag popularity over time in Quant Stack Exchange site\")\nplt.grid(True, alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Evolution of tag popularity over time in Quant Stack Exchange site\n\n\n\n\n\nSome observations from this graph:\n\nFrom 2011 to 2021, the monthly posts increased, although there were some sudden drops between 2016 and 2021. This might be attributed to periods of global economic challenges, such as the financial challenges in 2018. It was a year marked by market volatility, economic concerns, and a slowdown in growth, making it a challenging period for investors and the global economy.\nThe number of monthly posts declined from 2021 to the end of 2022. This period was during the COVID-19 pandemic. Was this decline related to COVID-19 or some other reasons?\nMonthly posts have increased since 2023 but not back to the level of 2021.\nChatGPT launched at the end of 2022. Does this affect why the number of posts hasn’t recovered since 2023? Are people now asking Chatgpt for answers rather than posting on the Quant Stack Exchange site?\nFrom 2011 to 2024, the most common tags were options, options pricing, and black scholes, which are the mathematical equations used to price options.\nProgramming was not popular in the early years, but has gained popularity since, especially during the COVID-19 period.\n‘Volatility’ appears more often in the word cloud than in the tag usage. ‘Black-scholes’ appears in the top 10 tags but not in the top 20 title words. This implies users are using tag words in post titles but not tagging the posts with an appropriate tag, or they are tagging posts but not using the the tag words in the title. Therefore, if title words and tags are isolated, they are not good indicators of what topic the post is about.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Cleaning and Text Analysis"
    ]
  },
  {
    "objectID": "portfolio/dw/Task8/Task_8HD.html#cross-site-reference-analysis-for-the-top-25-domains",
    "href": "portfolio/dw/Task8/Task_8HD.html#cross-site-reference-analysis-for-the-top-25-domains",
    "title": "Data Cleaning and Text Analysis",
    "section": "Cross-site reference analysis for the top 25 domains",
    "text": "Cross-site reference analysis for the top 25 domains\n\nData preparation\nA regular expression pattern is used to find all occurrences of ‘http’ or ‘https’ followed by ‘://’, followed by words, hyphens and periods. The regular expression pattern is used with the pandas findall command on the Body column of the posts data frame. The results from the findall matching are returned as a list and stored in a new column called Domains. Similar to the tags processing, the data frame is exploded using the list entries in the Domains column, and the unique domain names are counted and returned as a data series in descending order. The domains are then classified as an internal domain, a link to another stack exchange site or stackoverflow, or an external domain. The top 25 domains and counts are then copied into another data frame for further processing. The total counts of each type are calculated, and then for each domain its percentage of type count and percentage of the overall count is evaluated.\n\n\nCode\n# Regular expression for finding URL's in posts r\"https?://([\\w\\-\\.]+)\"\n  # https? - matches http or https\n  # :// - matches the colon and two forward slashes\n  # ([\\w\\-\\.]+) - Group to match domain name.\n    # [] character class\n      # \\w match any word (alphanumeric + underscore)\n      # \\- match a hyphen\n      # \\. match a period/dot\n      # + - make the match greedy and allow one or match of the character class\npattern = r\"https?://([\\w\\-\\.]+)\"\n\n# Find all patterns and return as a list stored in new column Domains.\nposts[\"Domains\"] = posts[\"Body\"].str.findall(pattern)\n\n# Use explode to create new rows for each entry in the lists in the domains\n#  column\n# Select the domains column and count the unique entries. The data series will \n# be returned sorted in descending order\n# Reset the index\ndomain_counts = posts.explode(\n    \"Domains\")[\"Domains\"].value_counts().reset_index().rename(\n        columns={\"count\":\"Count\"})\n\n# Classify the domains as internal or external using a regular expression \n# search for domain names\n# Any stackexchange or stackoverflow domains are internal \n# (r\"stackexchange|stackoverflow\")\ndomain_counts[\"Type\"] = domain_counts[\"Domains\"].str.contains(\n    r\"stackexchange|stackoverflow\", case=False, regex=True).map(\n        {True: \"Internal\", False: \"External\"})\n\n# Get the top 25 domains\n# Make a copy so we can add extra columns later\ntop_25_domains = domain_counts.head(25).copy() \n\n# Calculate total counts by type\ndomain_type_totals = top_25_domains.groupby(\"Type\")[\"Count\"].transform(\"sum\")\n\n# Calculate the overall total\ntotal_links = top_25_domains[\"Count\"].sum()\n\n# Add percentage columns\ntop_25_domains[\"Percentage of Type\"]= (\n    top_25_domains[\"Count\"] / domain_type_totals * 100).round(2)\ntop_25_domains[\"Percentage Overall\"] = (\n    top_25_domains[\"Count\"] / total_links * 100).round(2)\n\n# Sort by type and count (descending)\ntop_25_domains = top_25_domains.sort_values(\n    [\"Type\", \"Count\"], ascending=[True, False])\n\n\n\n\nSummary table and tree map for the top 25 domains\n\n\nCode\nposts_total = posts[\"Body\"].shape[0]\nposts_total_links = domain_counts.shape[0]\navg_links_post = round(total_links/posts_total_links)\n\n\nSummary information for the cross-site data analysis:\n\nTotal number of posts (questions and answers): 48764\nTotal number of posts with website links: 4726\nTotal number of website links: 21210\nAverage links per post with any links: 4\n\nApproximately 10% of posts have website links. When a post has a link, the average number of links is 4.\n\n\nCode\ndef colour_rows_by_type(row):\n    \"\"\"\n    Function to set the background colour for a row based upon its domain type\n\n    :param row: Row to return background colour for\n    :return: List of background colours for each entry in the row\n    \"\"\"\n    if row[\"Type\"] == \"Internal\":\n        # Light blue for Internal\n        return [\"background-color: #E6F2FF\"] * len(row)  \n    else:\n        # Light red for External\n        return [\"background-color: #FFECE6\"] * len(row)  \n\n# Apply the styling and hide the index\nstyled_table = top_25_domains.style.apply(\n    colour_rows_by_type, axis=1).hide(axis=\"index\")\n\n# Basic styling\nstyled_table = styled_table.format({\"Count\": \"{:,d}\",\n    \"Percentage of Type\": \"{:.2f}%\",\n    \"Percentage Overall\": \"{:.2f}%\"\n})\n\n# Add a table style with borders\nstyled_table = styled_table.set_table_styles([\n    {\"selector\": \"th\", \"props\": [(\"background-color\", \"#f5f5f5\"), \n                                (\"color\", \"#333\"), \n                                (\"font-weight\", \"bold\"),\n                                (\"border\", \"1px solid #ddd\"),\n                                (\"padding\", \"8px\")]},\n    {\"selector\": \"td\", \"props\": [(\"border\", \"1px solid #ddd\"),\n                                (\"padding\", \"8px\")]},\n    {\"selector\": \"caption\", \"props\": [(\"caption-side\", \"top\"), \n                                     (\"font-size\", \"16px\"),\n                                     (\"font-weight\", \"bold\"),\n                                     (\"color\", \"#333\")]}\n])\n\nstyled_table\n\n\n\n\nTable 2: Domain Count, Type and Percentages coloured by type\n\n\n\n\n\n  \n    \n      Domains\n      Count\n      Type\n      Percentage of Type\n      Percentage Overall\n    \n  \n  \n    \n      i.stack.imgur.com\n      9,163\n      External\n      51.49%\n      43.20%\n    \n    \n      en.wikipedia.org\n      2,945\n      External\n      16.55%\n      13.88%\n    \n    \n      papers.ssrn.com\n      1,073\n      External\n      6.03%\n      5.06%\n    \n    \n      github.com\n      779\n      External\n      4.38%\n      3.67%\n    \n    \n      arxiv.org\n      589\n      External\n      3.31%\n      2.78%\n    \n    \n      www.cmegroup.com\n      294\n      External\n      1.65%\n      1.39%\n    \n    \n      www.investopedia.com\n      284\n      External\n      1.60%\n      1.34%\n    \n    \n      cran.r-project.org\n      281\n      External\n      1.58%\n      1.32%\n    \n    \n      ssrn.com\n      252\n      External\n      1.42%\n      1.19%\n    \n    \n      www.sciencedirect.com\n      245\n      External\n      1.38%\n      1.16%\n    \n    \n      www.sec.gov\n      241\n      External\n      1.35%\n      1.14%\n    \n    \n      finance.yahoo.com\n      220\n      External\n      1.24%\n      1.04%\n    \n    \n      onlinelibrary.wiley.com\n      210\n      External\n      1.18%\n      0.99%\n    \n    \n      doi.org\n      202\n      External\n      1.14%\n      0.95%\n    \n    \n      www.jstor.org\n      196\n      External\n      1.10%\n      0.92%\n    \n    \n      www.youtube.com\n      178\n      External\n      1.00%\n      0.84%\n    \n    \n      www.google.com\n      175\n      External\n      0.98%\n      0.83%\n    \n    \n      www.researchgate.net\n      169\n      External\n      0.95%\n      0.80%\n    \n    \n      www.bloomberg.com\n      151\n      External\n      0.85%\n      0.71%\n    \n    \n      www.quandl.com\n      149\n      External\n      0.84%\n      0.70%\n    \n    \n      quant.stackexchange.com\n      2,286\n      Internal\n      66.96%\n      10.78%\n    \n    \n      rads.stackoverflow.com\n      590\n      Internal\n      17.28%\n      2.78%\n    \n    \n      stats.stackexchange.com\n      210\n      Internal\n      6.15%\n      0.99%\n    \n    \n      math.stackexchange.com\n      191\n      Internal\n      5.59%\n      0.90%\n    \n    \n      stackoverflow.com\n      137\n      Internal\n      4.01%\n      0.65%\n    \n  \n\n\n\n\n\n\nTable 2 shows the domain counts, type and percentages. Observations of this data:\n\n16 % of the links in posts are to other Stack Exchange or Stack Overflow sites, i.e., they are internal.\nOf the internal links, 67 % are self-links to other posts within the Quant Stack Exchange site, https://quant.stackexchange.com.\nThe remaining 84% of links are to external websites, with a majority of these being sites that host academic publications such as https://papers.ssrn.com, https://arxiv.com, https://www.jstor and https://www.researchgate.net\nAll images in Stack Exchange posts are hosted from https://i.stack.imgur.com. That’s why this domain makes up 43.2% of the post links.\n\nApproximately 10% of the posts website links, and 43% are images.\nThe tabular data can also be graphically visualised using a tree map as shown in Figure 4. This makes it easier to see the split between internal and external domains and well as the relative proportions within them.\n\n\nCode\nfig = px.treemap(top_25_domains, path=[px.Constant(\"Links\"), \"Type\",\"Domains\"],\n    values=\"Count\", color=\"Count\",color_continuous_scale=\"rdbu_r\",\n    title=\"Tree map of the top 25 linked domains\")\nfig.update_traces(root_color=\"lightgrey\")\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 4: Tree map of the top 25 linked domains",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Cleaning and Text Analysis"
    ]
  },
  {
    "objectID": "portfolio/dw/Task8/Task_8HD.html#programming-language-usage-evolution",
    "href": "portfolio/dw/Task8/Task_8HD.html#programming-language-usage-evolution",
    "title": "Data Cleaning and Text Analysis",
    "section": "Programming language usage evolution",
    "text": "Programming language usage evolution\nThe evolution of tags, Section 4.3 showed programming as a popular tag. In this section, posts containing code blocks are analysed for the programming language used to investigate any trends in the language usage over time.\n\nData preparation\nThe procedure used for preparing that data is:\n\nExtract code blocks from the body of posts using a regex pattern. This step is performed with the function extract_code_block.\nUsing regex patterns, identify the programming languages used for each extracted code block. The function identify_programming_language performs this.\nCreate a new data frame containing data gathered about the code block, such as the post ID, post type, post score, post length, language, and code length.\n\nAlong with full code functions, the code blocks in the Quant Stack Exchange also contain fragments of code, markdown tables, program outputs, and other miscellaneous text. Some programming languages are difficult to distinguish when they are only code fragments.\n\n\nCode\n# Filter for questions and answers that likely contain code\ndef extract_code_blocks(body):\n    \"\"\"\n    Extract code blocks from post body using regex.\n\n    :param body: Body from posts as string\n    :return: List of strings containing the the text from each code block\n    \"\"\"\n    # If the body is empty, return an empty list\n    if pd.isna(body):\n        return []\n    \n    # Find code blocks (text between &lt;code&gt; tags)\n    # (.*?) \n        # () makes a capture group\n        # . matches any character except newlines\n        # * zero or more characters\n        # ? non-greedy, stops at first &lt;/code&gt; tag after &lt;code&gt;\n    code_pattern = re.compile(r\"&lt;code&gt;(.*?)&lt;/code&gt;\", re.DOTALL)\n    code_blocks = code_pattern.findall(body)\n    \n    return code_blocks\n\n\n\n\nCode\n# Function to identify programming languages in code blocks using regular\n#  expressions\ndef identify_programming_language(code_block):\n    \"\"\"\n    Identify the likely programming language of a code_block.\n    \n    This function uses regex patterns to try and identify the programming \n    language within the code block. However, code block in the Quant stack\n    exchange site can also contain markdown code for tables and other text\n    etc.\n    \n    :param code_block: String of the text between code tags in a posts\n    :return: String containing the language identified\n    \"\"\"\n    # Define regex patterns for different languages\n    patterns = {\n    #   Python patterns\n    #   1.  import\\s+ matches the word 'import' followed by one or more \n    #       whitespace characters\n    #   2.  def\\s+ matches the word 'def' followed by one or more \n    #       whitespace characters\n    #   3.  class\\s+ matches the word 'class' followed by one or more\n    #       white space characters\n    #   4.  \\s*for\\s+.*\\s+in\\s+ matches a Python for loop pattern,\n    #       \\s* zero or more white space characters, for the word 'for',\n    #       \\s+ one or more whitespace character after the for,\n    #       .* matches any characters, \\s+in\\s+ matches the word in with\n    #       whitespace on both sides\n    #   5.  numpy, pandas, matplotlib, scipy, sklearn, tensorflow, pytorch,\n    #       matches common python libraries\n    #   6.  np\\. matches np followed by a period\n    #   7.  pd.\\ matches pd followed by a period \n    #   8.  print\\( matches print(\n    #   9.  \\.plot\\( matches .plot(\n    #   10. datetime\\. matches, datetime followed by a period\n    #   11. \\[\\s*(\\d+)\\s*rows\\s*x\\s*(\\d+)\\s*columns\\s*\\]\\dt\\. matches the \n    #       dimension information about a data table \n    #       e.g., [ 12 rows x 12 columns]\n    #   12. \\dt. matches dt followed by a period\n        \"Python\": r\"\"\"import\\s+|def\\s+|class\\s+|\\s*for\\s+.*\\s+in\\s+|numpy|\n                    np\\.|pandas|pd\\.|matplotlib|scipy|sklearn|tensorflow|\n                    pytorch|print\\(f|\\.plot\\(|datetime\\.|\n                    \\[\\s*(\\d+)\\s*rows\\s*x\\s*(\\d+)\\s*columns\\s*\\]|\\dt\\.\"\"\",\n\n    # R patterns\n    #   1.  library\\( matches library\n    #   2.  &lt;- Matches &lt;-, which is used as an assignment operator in R\n    #   3.  (?&lt;=.)\\$(?=.) matches the dollar sign when it is between two other\n    #       characters\n    #   4.  ddply matches the ddply function name\n    #   5.  rnorm matches the rnorm function name\n    #   6.  ggplot matches the ggplot function name\n    #   7.  data\\.frame matches data.frame\n    #   8.  function\\s*\\(.*\\)\\s*\\{ matches R function definition pattern, \n    #       function keyword followed by optional whitespace, matching \n    #       parentheses with any character between them, optional white \n    #       space followed by opening curly brace\n    #   9.  rbind matches the rbind function name\n    #   10. require\\( matches the require function\n    #   11. tidyr matches the package tidyr\n    #   12. caret matches the package caret\n    #   13. xts matches the package xts\n    #   14. quantmode matches the package quantmode\n        \"R\": r\"\"\"library\\(|&lt;-|(?&lt;=.)\\$(?=.)|ddply|rnorm|ggplot|data\\.frame|\n                function\\s*\\(.*\\)\\s*\\{|rbind|require\\(|tidyr|caret|xts|\n                quantmode\"\"\",\n        \n        # SQL patterns\n        # Matches either of the keywords SELECT, FROM, WHERE, JOIN, GROUP BY,\n        #  ORDER BY, INSERT, UPDATE, DELETE\n        \"SQL\": r\"\"\"SELECT|FROM|WHERE|JOIN|GROUP BY|ORDER BY|INSERT|UPDATE|\n                DELETE\"\"\",\n\n    # MATLAB patterns\n    #   1.  function\\s+.*\\s*= matches the MATLAB/Octave function declarations,\n    #       function followed by whitespace, any character, whitespace followed\n    #       by equals sign\n    #   2.  matlab, octave match either matlab or octave keywords\n    #   3.  \\.\\* Matches element-wise multiplication.*\n    #   4.  \\.\\^ matches element-wise power operation .^\n    #   5.  zeros\\( matches the zeros function zeros(\n    #   6.  ones\\( matches the ones function ones(\n    #   7.  figure\\s*\\( matches the figure function with option whitespace\n    #       between figure and the bracket\n    #   8.  linspace matches the keyword linspace\n    #   9.  matrix matches the keyword matrix\n        \"MATLAB\": r\"\"\"function\\s+.*\\s*=|matlab|octave|\\.\\*|\\.\\^|zeros\\(|ones\\(|\n                    figure\\s*\\(|linspace|matrix\"\"\",\n\n    # C/C++ patterns\n    #   1.  \\#include match #include\n    #   2.  int\\s+main matches the start of a main function declaration\n    #   3.  void\\s+\\w+\\s*\\(  matches a void funtion declaration void followed \n    #       by one or more spaces, oen or more words followed by zero or more\n    #       spaces and an open bracket\n    #   4.  std:: matches standard librart namespace\n    #   5.  printf matches printf statement\n    #   6.  cout match the c++ output stream\n    #   7.  template matches the keyword template\n    #   8.  boost:: matches the boost namespace\n    #   9.  eigen matches the eigen function\n        \"C/C++\": r\"\"\"\\#include|int\\s+main|void\\s+\\w+\\s*\\(|std::|printf|cout|\n                    template|boost::|eigen\"\"\",\n        \n    #   1.  Javascript patterns\n    #   2.  function\\s+\\w+\\s*\\( matches a javascript function declaration. \n    #       function followed by one or more whitespaces, one or more word\n    #       matches, zero or more whitespaces followed by an opening braket\n    #   3.  var\\s+ mactches the use of the var keyword\n    #   4.  let\\s+ matches the use of the let keyword\n    #   5.  const\\s+ matches the use of the constant keyword\n    #   6.  document\\. matches the DOM document object\n    #   7.  window\\. matches the browser window object\n    #   8.  Math\\. matches the Math object\n        \"Javascript\": r\"\"\"function\\s+\\w+\\s*\\(|var\\s+|let\\s+|const\\s+|\n                        document\\.|window\\.|Math\\.\"\"\",\n\n    #   1. VBA patterns\n    #   2. Sub\\s+ matches subroutn declaration\n    #   3. Function\\s+ matches a function declaration\n    #   4. Dim\\s+ Matches variable declarations in VBA\n    #   5. Worksheets matches the keyword worksheets in Excel VBA\n    #   6. Range\\( matches usage of the Range object\n    #   7. Cells\\( matches usage of the Cells object\n        \"Excel/VBA\": r\"\"\"Sub\\s+|Function\\s+|Dim\\s+|Worksheets|Range\\(|Cells\\(\"\"\",\n\n    # Mathematica patterns\n    #   1.  Plot\\[ matches command Plot followed by [\n    #   2.  Integrate\\[ matches command Integrate followed by [\n    #   3.  Solve\\[ matches command Solve followed by [\n    #   4.  Module\\[ matches command Module followed by [\n    #   5.  \\\\\\[Sigma\\] matches command \\[Sigma] \n        \"Mathematica\": r\"\"\"Plot\\[|Integrate\\[|Solve\\[|Module\\[|\\\\\\[Sigma\\[\"\"\",\n\n    # Latex patterns\n    #   1.  \\\\begin match \\begin\n    #   2.  \\\\end match \\end\n    #   3.  \\\\frac match \\frac\n    #   4.  \\\\sum match \\sum\n    #   5.  \\\\int match \\int\n    #   6.  mathbb match \\mathbb\n        \"Latex\": r\"\"\"\\\\begin|\\\\end|\\\\frac|\\\\sum|\\\\int|\\\\mathbb\"\"\",\n\n    # Markdown patterns\n    #   1.  \\| Any pipe\n    #   2.  \\|[\\s\\-\\|:]+ Match a table delimiter row\n    #        | --- | --- | or | :---:| :---:|\n    #   3.  -{2,} two or more hypens\n    #   4.  ={3,} three or more equals\n    #   5.  &quo t\n    #   6. \\*\\s astrix foollowed by white space\n        \"Markdown/HTML\":r\"\"\"\\||\\|[\\s\\-\\|:]+|-{2,}|={3,}|\\&quot;|\\*\\s\"\"\"\n    }\n    \n    # Check for language indicators\n    for lang, pattern in patterns.items():\n        if re.search(pattern, code_block, re.IGNORECASE):\n            return lang\n    \n    # Check for specific math symbols common in quant posts\n    if re.search(r\"\\\\sigma|\\\\mu|\\\\alpha|\\\\beta|\\\\Delta\", code_block):\n        return \"Mathematical Notation\"\n    \n    # If code block is very short check for specific features\n    #   1.  Simple function calls e.g. print(), sum(), calulcate123()\n    #   2.  Functions with arguments e.g. add(1,2), get(first, last)\n    #   3.  Method calls e.g. object.method()\n    #   4.  Nested calls e.g. print(name())\n    if len(code_block) &lt; 50:\n        if re.search(r\"[a-zA-Z0-9]+\\([a-zA-Z0-9,\\s\\.]*\\)\", code_block):\n            return \"Formula\"\n\n    # No match then return Unknown\n    return \"Unknown\"\n\n\n\n\nCode\n# Extract code from posts\nposts[\"CodeBlocks\"] = posts[\"Body\"].apply(extract_code_blocks)\nposts[\"CodeCount\"] = posts[\"CodeBlocks\"].apply(len)\n\n# Filter posts with code\nposts_with_code = posts[posts[\"CodeCount\"] &gt; 0].copy()\nnum_post_code_blocks = len(posts_with_code)\n\n# Identify the language for each code block\nlanguage_data = []\nfor _, row in posts_with_code.iterrows():\n    post_id = row[\"Id\"]\n    post_type = \"Question\" if row[\"PostTypeId\"] == 1 else \"Answer\"\n    score = row[\"Score\"]\n    post_length = len(row[\"Body\"])\n    \n    for i, code_block in enumerate(row[\"CodeBlocks\"]):\n        language = identify_programming_language(code_block)\n        code_length = len(code_block)\n        \n        language_data.append({\n            \"PostId\": post_id,\n            \"PostType\": post_type,\n            \"Score\": score,\n            \"PostLength\": post_length,\n            \"Language\": language,\n            \"CodeLength\": code_length\n        })\n\ncode_df = pd.DataFrame(language_data)\nlanguage_counts = code_df[\"Language\"].value_counts()\n\n# Gather some statistics\nnum_posts = posts.shape[0]\nper_of_posts = round((num_post_code_blocks/num_posts)*100, 1)\nnum_code_blocks = sum(posts[\"CodeCount\"])\navg_blocks_post = round(num_code_blocks/num_post_code_blocks)\n\n\n4789 posts containing code blocks were found.\n\n\nProgramming language statistics and distribution\nAfter the data has been prepared, some statisitics about the code blocks found can be reported.\nCode block statistics:\n\nTotal posts analysed: 48764\nPosts containing code: 4789 (9.8)%\nTotal code blocks found: 14225\nAverage code blocks per post with code: 3\n\nTable 3 shows the distribution of programming languages detected in the code blocks found in posts. You will notice that the ‘Unknown’ is significant, this is because of the large number of program outputs and code fragments contained in the code blocks. Further refinement of the regex patterns is recommended to reduce the number of ‘unknown’ detections.\n\n\nCode\ndf_lang_dist = language_counts.to_frame(name=\"Count\")\n\ndf_lang_dist[\"Percent of code\"] = language_counts.apply(\n    lambda x: f\"{x*100/num_code_blocks:.2f}\")\nMarkdown(df_lang_dist.to_markdown(index = True))\n\n\n\n\nTable 3: Distribution of programming languages detected\n\n\n\n\n\n\nLanguage\nCount\nPercent of code\n\n\n\n\nUnknown\n9503\n66.8\n\n\nPython\n1634\n11.49\n\n\nR\n806\n5.67\n\n\nMarkdown/HTML\n802\n5.64\n\n\nFormula\n668\n4.7\n\n\nSQL\n252\n1.77\n\n\nMATLAB\n237\n1.67\n\n\nJavascript\n151\n1.06\n\n\nC/C++\n104\n0.73\n\n\nExcel/VBA\n50\n0.35\n\n\nMathematica\n12\n0.08\n\n\nLatex\n3\n0.02\n\n\nMathematical Notation\n3\n0.02\n\n\n\n\n\n\n\n\nFigure 5 shows a bar chart of the distribution of programming languages in posts on the Quant Stack Exchange site. For plot, the ‘Unknow’, ‘Markdown/HTML’, ‘Formula’, ‘Latex’, and ‘Mathemtical Notation’ categories were removed to focus on actual languages used in software programming. From the figure, it is observed that Python is twice as popular as R, eight times more popular than SQL or MATLAB, and approximately sixteen times more popular than Javascript or C/C++.\n\n\nCode\n# Programming Language Distribution\nplt.figure(figsize=(8, 6))\n\n# Filter for languages with &gt;10 occurrences and not the unknown, markdown,\n#  formula, notation or latex categories\nlanguage_counts = language_counts.loc[(~language_counts.index.str.contains(\n    \"Unk|Mark|Form|not|latex\")) & (language_counts &gt; 10)] \n\nsns.barplot(x=language_counts.index, y=language_counts.values,\n palette=\"viridis\")\nplt.xlabel(\"Language\", fontsize=14)\nplt.ylabel(\"Number of Code Blocks\", fontsize=14)\nplt.xticks(rotation=45, ha=\"right\")\nplt.grid(axis=\"y\", alpha=0.3)\nplt.title(\"Programming Languages Used in Quant Stack Exchange Posts\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Programming Languages Used in Quant Stack Exchange Posts\n\n\n\n\n\n\n\nLanguage Usage Over Time\nBased on all the posts, Python may be the most popular programming language, but has it always been the most popular? A heatmap of language usage over time was created to answer this question. The creation data of posts was used to create a column of the year of the posts in the posts_with_code data frame. The language data was joined with the post dates using an inner join on ‘PostId’ from the language data frame and the ‘Id’ from the post dates data frame. The languages were then filtered to remove the ‘Unknow’, ‘Markdown/HTML’, ‘Formula’, ‘Latex’, and ‘Mathemtical Notation’ categories. A cross-table was created using the ‘Year’ and ‘Language’, and the number of code blocks per year was calculated by performing a sum on the table columns. The data in cross-table were then normalised using the count per year. Leaving the data as a percentage of code blocks for that year. This was done because the number of code blocks per year increased with time, and we are only interested in the change in the percentage of a language used for a given year.\n\n\nCode\n# Add year column using creationDate\nposts_with_code[\"Year\"] = posts_with_code[\"CreationDate\"].dt.year\n    \n# Join language data with post dates\nlanguage_dates = pd.merge(code_df, posts_with_code[[\"Id\", \"Year\", \"CodeCount\"]],\n  left_on=\"PostId\", right_on=\"Id\")\nlanguage_dates = language_dates[~language_dates[\"Language\"].str.contains(\n    \"Unk|Mark|Form|Not|Latex\")]\n\n# Count languages by year\nlanguage_by_year = pd.crosstab(\n    language_dates[\"Year\"], language_dates[\"Language\"])\n\n# Count the number of code blocks per year\ncode_blocks_by_year = language_by_year.sum(axis=1)\n\nlanguage_by_year_norm = language_by_year.div(code_blocks_by_year, axis=\"index\")\n\n\nFigure 6 shows the evolution of the usage of each programming language between 2011 and 2024. Most notable in the figure is the rise of Python and the demise of R. R was the most popular language from 2011 until 2015. In 2015 Python and R had similar popularity; however, in 2020, Python became the dominant language used in the code blocks of the site, wih approximately 80% of the detected languages usage being Python. Another interesting feature is that SQL usage has declined since 2014. It is unlikely that people have stopped using databases, but more likely that people are now using Python to interact with databases rather than SQL. All other programming languages follow a similar trend to SQL; their usage was highest around 2011 but has dropped since 2017.\n\n\nCode\nplt.figure(figsize=(8, 8))\n\nax = sns.heatmap(language_by_year_norm, vmin=0, vmax=1, annot=False,\n linewidth=0.5, cmap=\"viridis\", fmt=\".2f\")\nplt.ylabel(\"Year\", fontsize=14)\nplt.xlabel(\"Language\", fontsize=14)\nax.invert_yaxis()\nplt.tight_layout()\nplt.title(\"Programming language usage trends over time (% of code blocks)\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Programming language usage trends over time (% of code blocks)",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Cleaning and Text Analysis"
    ]
  },
  {
    "objectID": "portfolio/dw/Task8/Task_8HD.html#time-taken-for-a-question-to-be-answered",
    "href": "portfolio/dw/Task8/Task_8HD.html#time-taken-for-a-question-to-be-answered",
    "title": "Data Cleaning and Text Analysis",
    "section": "Time taken for a question to be answered",
    "text": "Time taken for a question to be answered\nIf I were to post a question today, how long would I need to wait until someone answered it? This question is the focus of this section.\n\nData preparation\nThe response time of answering a post needs to be calculated; this is done by creating two data frames from the post data frame, one containing the questions (PostTypeId=1) and the other containing the answers (PostTypeId=2). These two data frames are then merged using an inner joint on ‘Id’ from the questions and ‘ParentId’ from the answers, with the result being stored in a data frame called answered_df. Columns with the same name in each data frame are kept using the suffixes _question and _answer. The time in minutes to answer a question is calculated from the difference in creation dates. Answers that are faster than one minute are not likely to be correct and, therefore, are excluded from the data. Since there can be multiple answers to a question, we take the fastest answer response using group by and the min() function.\n\n\nCode\n# Response time calculation\nquestions_df = posts[posts[\"PostTypeId\"]==1].copy()\nanswers_df = posts[posts[\"PostTypeId\"]==2].copy()\n\n# Merge questions with their answers\naswered_df = pd.merge(\n    questions_df[[\"Id\", \"CreationDate\"]],\n    answers_df[[\"ParentId\", \"CreationDate\"]],\n    left_on=\"Id\",\n    right_on=\"ParentId\",\n    suffixes=(\"_question\", \"_answer\")\n)\n    \n# Calculate time difference in minutes\naswered_df[\"TimeInterval\"] = (\n    (aswered_df[\"CreationDate_answer\"] - aswered_df[\"CreationDate_question\"])\n    .dt.total_seconds() / 60\n)\n\n# Drop times less than 1 minute\naswered_df = aswered_df[~(aswered_df[\"TimeInterval\"] &lt; 1) ]\n\n# Group by question Id and find the minimum time interval\nresponse_df = aswered_df.groupby(\"Id\")[\"TimeInterval\"].min().reset_index()\n    \n# Sort by question Id\nresponse_df = response_df.sort_values(\"Id\")\n\n\n\n\nDistribution of answer response time\nFigure 7 shows the distribution of the log of the time taken in minutes to answer a question. Several vertical lines have been added to the figure to show the first quartile, the median, the third quartile and a 30-day response time. The distribution is bimodal, with the peak of the primary mode around 6.8 hours and the secondary peak around 307 days. What factors would influence the second mode in the distribution?\nThe Quant Stack Exchange site is a community-driven Q&A website governed by a reputation system. It rewards the users by giving repuation points and badges for the usefulness of their posts. The response time to answer a question depends on a number of factors, such as the question’s quality and complexity, the availability of experts and the experts’ interest in the question topic. For this study, only the relationship between the complexity of the question and the response time to answer is considered.\n\n\nCode\n# Create histogram\nplt.hist(np.log(response_df[\"TimeInterval\"]), bins=50, color=\"skyblue\",\n edgecolor=\"black\")\n\n# Add a vertical line at Q1\nplt.axvline(x=4.6, color=\"tab:orange\", linestyle=\"--\", label=\"Q1 = 1.7 hours\")\n\n# Add a vertical line at 1 days\nplt.axvline(x=6, color=\"tab:green\", linestyle=\"--\", label=\"Median = 6.8 hours\")\n\n# Add a vertical line at Q1\nplt.axvline(x=7.51, color=\"tab:purple\", linestyle=\"--\", label=\"Q3 = 30.7 hours\")\n\n# Add a vertical line at 30 days\nplt.axvline(x=10.67, color=\"tab:red\", linestyle=\"--\", label=\"30 days\")\n\nplt.xlabel(\"Log of Time to Answer a question (minutes)\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.title(\"Distribution of Log(Response time (min))\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Distribution of the log of the time in minutes to answer a question\n\n\n\n\n\n\n\nRelationship between response time and question complexity\nTo investigate the association between question complexity and the time taken to answer the question, the following parameters were studied:\n\nThe density of the mathematics in the question\nThe length of the question\nRepuation of the asker\nReadability of the question\n\nI define the density of mathematics in a question as the number of Latex equation patterns per word in the question after code blocks have been removed. A function was created to calculate the density of the mathematics used in a question. The function takes the body of a post, finds code blocks using a regex, and replaces them with an empty string; then, Latex math patterns are found using regex, and the number is counted. The post’s word count is calculated by finding all the words in the posts using a regular expression. The density is the number of Latex math expressions divided by the word count.\n\n\nCode\n# Function to calculate math density using regex\ndef calculate_math_density(body):\n    \"\"\"\n    Calculate math density from post body using various regex patterns.\n\n    :param body: string of the body text\n    :return: Float containg the density of math in the post body\n    \"\"\"\n    if pd.isna(body):\n        return []\n    \n    body = body.lower()\n    \n    # Remove code blocks\n    code_pattern = re.compile(r\"&lt;code&gt;(.*?)&lt;/code&gt;\", re.DOTALL)\n    body = re.sub(code_pattern, \"\", body)\n\n    math = []\n    \n    # Find inline LaTeX math (between $ signs)\n    inline_pattern = re.compile(r\"\\$([^$]+)\\$\")\n    inline_math = inline_pattern.findall(body)\n    math.extend(inline_math)\n    \n    # Find display LaTeX formulas (between $$ signs)\n    display_pattern = re.compile(r\"\\$\\$([^$]+)\\$\\$\")\n    display_math = display_pattern.findall(body)\n    math.extend(display_math)\n    \n    # Count words (simple split)\n    num_words = len(re.findall(r\"\\w+\", body))\n\n    # Avoid division by zero\n    if num_words == 0:\n        return 0\n\n    # Density = math matches per word\n    return len(math) / num_words\n\n\nAfter the mathematics density is calculated for each question, a left join is performed between the response time and the question data frames to get the response time for each question with an answer. This is stored in a new questions_response_df data frame. The cleaned posts data frame created in Section 4.3 is also merged with the question data to leave only rows in the clean posts data frame that are in the questions data. The tag type information in the cleaned posts data frame will be used to colour the points in the following scatter plots.\n\n\nCode\n# Apply math density calculation to posts\nquestions_df[\"MathDensity\"] = questions_df[\"Body\"].apply(calculate_math_density)\n\n# Merge response_df with questions to get response time for questions with\n#  an answer\nquestion_response_df = pd.merge(\n    response_df,\n    questions_df,\n    on=\"Id\",\n    how=\"left\"\n)\n\n# Merge to ensure the rows are the same. \nposts_cleaned_filtered = posts_cleaned.merge(\n    question_response_df[[\"Id\"]], on=\"Id\", how=\"inner\")\n\n\n\n\nCode\n# Plot Response time versus mathematical density\npalette = [\"tab:red\", \"tab:green\"]\n\nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(x=question_response_df[\"MathDensity\"],\n    y=np.log(question_response_df[\"TimeInterval\"]),\n    alpha=0.6,hue=posts_cleaned_filtered[\"TagType\"], palette=palette)\n\nplt.xlabel(\"Mathematical density\")\nplt.ylabel(\"Log(Response time (min))\")\nplt.title(\"Scatter Plot of Log(Response time (min)) vs. Mathematical density\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8: Scatter plot of Log(Response time (min)) vs. Mathematical density\n\n\n\n\n\n\n\nCode\nsp_correlation_md = round(question_response_df[\"MathDensity\"].corr(\n    np.log(question_response_df[\"TimeInterval\"]), method=\"spearman\"),3)\nprint(f\"\"\"The Spearman correlation between Mathematical density and \"\"\"\n\"\"\"Log(Response time) is {sp_correlation_md:.3f}\"\"\")\n\n\nThe Spearman correlation between Mathematical density and Log(Response time) is {sp_correlation_md:.3f}\n\n\nFigure 8 shows a scatter plot between Log(Response time) and Mathematical density with the points coloured by tag type. The first notable feature of this plot is that there are no tags of type ‘No Tag’. This shows that all answered questions had at least one tag set. The distribution of ‘Unpopular’ and ‘Popular’ tags is random and scattered throughout the plots with no visible clustering. The second notable feature is that there are a lot of answered questions that have no mathematics in them, i.e., a math density of zero. Lastly, the high response time is visible for all mathematical density values. The Spearman correlation coefficient (-0.004) shows no consistent monotonic relationship between the Log(Response time) and the Mathematical density.\n\n\nCode\n# Plot Log response versus Log post length\nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(x=np.log(question_response_df[\"Body\"].apply(len)),\n    y= np.log(question_response_df[\"TimeInterval\"]),\n    alpha=0.6,hue=posts_cleaned_filtered[\"TagType\"], palette=palette)\n\nplt.xlabel(\"Log(Post length (characters))\")\nplt.ylabel(\"Log(Response time (min))\")\nplt.title(\n\"Scatter plot of Log(Response time (min)) vs. Log(Post length (characters))\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Scatter plot of Log(Response time (min)) vs. Log(Post length (characters))\n\n\n\n\n\n\n\nCode\nsp_correlation_pl = round(np.log(question_response_df['Body'].apply(len)).corr(\n    np.log(question_response_df['TimeInterval']), method='spearman'), 3)\nprint(f\"\"\"The Spearman correlation between Log(Post length) and \"\"\"\n\"\"\"Log(Response time) is {sp_correlation_pl:.3f}\"\"\")\n\n\nThe Spearman correlation between Log(Post length) and Log(Response time) is {sp_correlation_pl:.3f}\n\n\nFigure 9 shows a scatter plot between Log(Response time) and Log(Post length). The majority of answered questions had a length between 150 and 3000 characters. Again, the distribution of the tag type is random and spreads throughout the entire domain of the plot. Two clusters are visible, one with a high response time and the other with a lower response time. The Spearman correlation coefficient (0.143) shows no consistent monotonic relationship between the Log(Response time) and Log(Post length).\n\n\nCode\nquestions_reputation_df = pd.merge(question_response_df, \n    users[['Id', 'Reputation']], left_on='OwnerUserId', right_on='Id',\n    how='left')\nquestions_reputation_df['Reputation'].fillna(1, inplace=True)\n\nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(\n    x=np.log(questions_reputation_df['Reputation']),\n    y=np.log(questions_reputation_df['TimeInterval']),\n    alpha=0.6,hue=posts_cleaned_filtered['TagType'], palette=palette)\n\nplt.xlabel('Log(Asker reputation)')\nplt.ylabel('Log(Response time)')\nplt.title('Scatter plot of Log(Response time (min)) vs. Log(Asker reputation)')\nplt.show()\n\n\nC:\\Users\\darrin\\AppData\\Local\\Temp\\ipykernel_21820\\578517144.py:4: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Scatter plot of Log(Response time (min)) vs. Log(Asker reputation)\n\n\n\n\n\n\n\nCode\nsp_correlation_ar = round(np.log(questions_reputation_df['Reputation']).corr(\n    np.log(question_response_df['TimeInterval']), method='spearman'), 3)\nprint(f\"\"\"The Spearman correlation between Log(Askers reputation) and \"\"\"\n\"\"\"Log(Response time) is {sp_correlation_ar:.3f}\"\"\")\n\n\nThe Spearman correlation between Log(Askers reputation) and Log(Response time) is {sp_correlation_ar:.3f}\n\n\nFigure 10 shows a scatter plot between Log(Response time) and Log(Asker reputation). Many users have a low reputation of one, and a scattering of users with a high reputation ask questions. The tag type again shows a random distribution throughout the plot. Questions from users with high reputations can also take a long time to answer. Again, the Spearman correlation coefficient (0.143) shows no consistent monotonic relationship between the Log(Response time) and Log(Askers reputation).\n\n\nCode\n# Function to clean text\ndef clean_body(text):\n    \"\"\"\n    Removes number, html blocks and latex code from a given string.\n\n    :param text: String to clean\n    :return: clened text\n    \"\"\"\n    if pd.isna(text):\n        return \"\"\n    \n    # Remove numbers\n    text = re.sub(r\"\\d+\", \"\", text)\n\n    # Remove inline math $...$\n    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n    \n    # Remove display math $$...$$\n    text = re.sub(r\"\\$\\$.*?\\$\\$\", \"\", text)\n\n    # Remove &lt;code&gt; &lt;/code&gt; blocks\n    code_pattern = re.compile(r\"&lt;code&gt;(.*?)&lt;/code&gt;\", re.DOTALL)\n    text = re.sub(code_pattern, \"\", text)\n    \n    return text\n\n\n\n\nCode\n# Clean html and Latex from body\nquestion_response_df[\"CleanedBody\"] =  question_response_df[\"Body\"].apply(\n    clean_body)\n\n#Calculating readability score using Flesch Kincaid Grade\nquestion_response_df[\"FleschKincaidGrade\"] =  question_response_df[\"CleanedBody\"\n    ].apply(textstat.flesch_kincaid_grade)\n\nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(\n    x=question_response_df[\"FleschKincaidGrade\"],\n    y=np.log(question_response_df[\"TimeInterval\"]),\n    alpha=0.6,hue=posts_cleaned_filtered[\"TagType\"], palette=palette)\n\nplt.xlabel(\"Readability Score\")\nplt.ylabel(\"Log(Response time)\")\nplt.title(\"Scatter plot of Log(Response time (min)) vs. Readability Score\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11: Scatter plot of Log(Response time (min)) vs. Log(Asker reputation)\n\n\n\n\n\n\n\nCode\nsp_correlation_read = round(question_response_df[\"FleschKincaidGrade\"].corr(\n    question_response_df[\"TimeInterval\"], method=\"spearman\"), 3)\nprint(f\"\"\"The Spearman correlation between asker's reputation and response \"\"\"\n\"\"\"time is {sp_correlation_read:.3f}\"\"\")\n\n\nThe Spearman correlation between asker's reputation and response time is {sp_correlation_read:.3f}\n\n\nThe readability score of a question was calculated using the Flesch Kincaid Grade. Higher numbers indicate harder-to-read posts. The readability score was calculated on the posts’s body text after the mathmatical formula and code blocks were removed. Figure 11 shows a scatter plot between the Log(Response time) and Readability score. Most questions have a readability score of around 13, with some scoring above 20. There is no visible trend between response time and readability score. The Spearman correlation coefficient (0.074) shows no consistent monotonic relationship between the Log(Response time) and Readability score.\nIt has been seen that there is no consistent monotonic relationship between Log(Response time) and either of Mathematical density, Log(Post length), Log(Asker reputation) or Readability score. The non-existence of a relationship between the Log(Response time) and the other variables does not mean we can rule out any relationship between these variables as there could still be:\n\nMultivariate relationships where variables might only show associations in combination\nInteraction effects among the independent variables\n\nFurther analysis that is beyond the scope of this work would be needed to draw a conclusion about the impact of question complexity on the time it takes to be answered.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Cleaning and Text Analysis"
    ]
  },
  {
    "objectID": "portfolio/dw/Task8/Task_8HD.html#summary",
    "href": "portfolio/dw/Task8/Task_8HD.html#summary",
    "title": "Data Cleaning and Text Analysis",
    "section": "Summary",
    "text": "Summary\nAnalysis of the Quant Stack Exchange site has been performed for the geographic distribution of the site’s users, the popular words used in posts, the popularity of tags used in posts, the type of programming language discussed, and how this has changed over the site’s life, and the response time to answering questions, and the relationship to question complexity.\n\nUser Geographic Distribution\n\nUsers are heavily concentrated in Europe, US coasts, and India\nLower representation in Africa, South America, and Southeast Asia\nAustralian users mainly in capital cities (except Perth)\n\n\n\nPopular Content Analysis\n\nThe most common words in the top questions: “option,” “volatility,” and “model”.\nFinancial terminology dominates high-scoring questions\nTop tags evolved from 2011 to 2024: options, options pricing, and Black-Scholes consistently popular\nProgramming tags gained popularity, especially during COVID-19\n\n\n\nCross-Site References\n\n~10% of posts contain website links (average four links per post)\n84% of links are to external sites, with academic resources being common\n16% link to other Stack Exchange/Overflow sites\nImages account for 43% of all post links\n\n\n\nProgramming Language Trends\n\nPython overtook R as the dominant language (80% of code blocks by 2020)\nR was most popular from 2011-2015\nSQL and other languages have declined since 2017\nAbout 10% of posts contain code blocks\n\n\n\nQuestion Response Time Analysis\n\nDistribution is bimodal: peaks at ~6.8 hours and ~307 days\nNo strong correlation was found between response time and:\n\nMathematical content density\nPost length\nAsker reputation\nText readability\n\n\nThe analysis suggests that while the site has evolved, particularly in programming language preferences, the factors that determine how quickly questions receive answers remain complex and possibly involve multivariate relationships beyond the scope of this analysis.",
    "crumbs": [
      "Portfolio",
      "Data Wrangling",
      "Data Cleaning and Text Analysis"
    ]
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "This section highlights some of the projects completed during my Masters of Data Science at Deakin University.\n\n\n\n Back to top",
    "crumbs": [
      "Portfolio",
      "Introduction"
    ]
  }
]